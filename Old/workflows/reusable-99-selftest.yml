name: Reusable 99 Selftest

on:
  workflow_call:
    inputs:
      python-versions:
        description: >-
          JSON array of Python versions forwarded to `reusable-90-ci-python.yml`. Leave empty to
          exercise the default 3.11 matrix used by Maint 90 Selftest.
        required: false
        type: string
        default: '["3.11"]'
  workflow_dispatch:
    inputs:
      python-versions:
        description: >-
          JSON array of Python versions forwarded to `reusable-90-ci-python.yml`. Leave empty to
          exercise the default 3.11 matrix used by Maint 90 Selftest.
        required: false
        default: '["3.11"]'

jobs:
  # Matrix of feature scenarios exercising the reusable workflow with different flag combinations.
  scenario:
    name: Scenario - ${{ matrix.name }}
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        include:
          - name: minimal
            enable-metrics: false
            enable-history: false
            enable-classification: false
            enable-coverage-delta: false
            enable-soft-gate: false
          - name: metrics_only
            enable-metrics: true
            enable-history: false
            enable-classification: false
            enable-coverage-delta: false
            enable-soft-gate: false
          - name: metrics_history
            enable-metrics: true
            enable-history: true
            enable-classification: false
            enable-coverage-delta: false
            enable-soft-gate: false
          - name: classification_only
            enable-metrics: false
            enable-history: false
            enable-classification: true
            enable-coverage-delta: false
            enable-soft-gate: false
          - name: coverage_delta
            enable-metrics: false
            enable-history: false
            enable-classification: false
            enable-coverage-delta: true
            enable-soft-gate: false
            baseline-coverage: '65'
            coverage-alert-drop: '2'
          - name: full_soft_gate
            enable-metrics: true
            enable-history: true
            enable-classification: true
            enable-coverage-delta: true
            enable-soft-gate: true
            baseline-coverage: '65'
            coverage-alert-drop: '2'
    steps:
      - name: Invoke reusable CI (matrix scenario)
        uses: stranske/Trend_Model_Project/.github/workflows/reusable-90-ci-python.yml@phase-2-dev
        with:
          python-versions: ${{ inputs.python-versions }}
          artifact-prefix: "sf-${{ matrix.name }}-"
          enable-metrics: ${{ matrix.enable-metrics }}
          enable-history: ${{ matrix.enable-history }}
          enable-classification: ${{ matrix.enable-classification }}
          enable-coverage-delta: ${{ matrix.enable-coverage-delta }}
          enable-soft-gate: ${{ matrix.enable-soft-gate }}
          baseline-coverage: ${{ matrix.baseline-coverage || '0' }}
          coverage-alert-drop: ${{ matrix.coverage-alert-drop || '1' }}

  aggregate:
    name: Aggregate Verification
    runs-on: ubuntu-latest
    needs: [scenario]
    outputs:
      verification_table: ${{ steps.verify.outputs.table }}
      failures: ${{ steps.verify.outputs.failures }}
    steps:
      - name: Summarize run
        run: |
          {
            echo "## Self-Test Reusable CI Summary"
            echo "Scenarios executed: minimal, metrics_only, metrics_history, classification_only, coverage_delta, full_soft_gate"
            echo "Each scenario's internal ci_feature_assert.py step already enforced expectations."
            echo "Download the \`selftest-report\` artifact for a JSON summary of expected vs. actual uploads."
            echo "Reproduce the October 2025 lockfile failure locally with:"
            echo "\`pytest tests/test_lockfile_consistency.py -k \"up_to_date\" -q\`"
            echo "This aggregate job can be extended to download & re-assert artifacts if needed."
          } >> "$GITHUB_STEP_SUMMARY"
      - name: Verify artifact inventory
        id: verify
        uses: actions/github-script@v7
        with:
          script: |
            const scenarios = [
              { name: 'minimal', metrics:false, history:false, classification:false, covDelta:false, soft:false },
              { name: 'metrics_only', metrics:true, history:false, classification:false, covDelta:false, soft:false },
              { name: 'metrics_history', metrics:true, history:true, classification:false, covDelta:false, soft:false },
              { name: 'classification_only', metrics:false, history:false, classification:true, covDelta:false, soft:false },
              { name: 'coverage_delta', metrics:false, history:false, classification:false, covDelta:true, soft:false },
              { name: 'full_soft_gate', metrics:true, history:true, classification:true, covDelta:true, soft:true },
            ];
            const run_id = context.runId;
            const {owner, repo} = context.repo;
            // Collect all artifacts (paginate if needed)
            let page=1, artifacts=[]; 
            while(true){
              const resp = await github.rest.actions.listWorkflowRunArtifacts({owner, repo, run_id, per_page:100, page});
              artifacts = artifacts.concat(resp.data.artifacts || []);
              if(!resp.data.artifacts || resp.data.artifacts.length < 100) break;
              page+=1;
            }
            const names = new Set(artifacts.map(a=>a.name));
            function expectedFor(s){
              const p = (suffix)=> `sf-${s.name}-${suffix}`;
              const out = [p('coverage-3.11')]; // coverage always
              if(s.metrics) out.push(p('ci-metrics'));
              if(s.history) out.push(p('metrics-history'));
              if(s.classification) out.push(p('classification'));
              if(s.covDelta) out.push(p('coverage-delta'));
              if(s.soft){
                out.push(p('coverage-summary'));
                out.push(p('coverage-trend'));
                out.push(p('coverage-trend-history'));
              }
              return out;
            }
            const rows = ['| Scenario | Expected Artifacts | Missing | Unexpected Present | Status |','|---|---|---|---|---|'];
            const report = [];
            let failures = 0;
            // Build reverse expectation set so we can find unexpected items
            const expectedAll = new Set();
            scenarios.forEach(s => expectedFor(s).forEach(a=>expectedAll.add(a)));
            for(const s of scenarios){
              const exp = expectedFor(s);
              const missing = exp.filter(e=>!names.has(e));
              // Collect unexpected artifacts that match this scenario prefix but are NOT expected
              const prefix = `sf-${s.name}-`;
              const scenarioActual = [...names].filter(n=>n.startsWith(prefix));
              const unexpected = scenarioActual.filter(n=>!exp.includes(n));
              const ok = missing.length===0 && unexpected.length===0;
              if(!ok) failures++;
              rows.push(`| ${s.name} | ${exp.join('<br>')} | ${missing.join('<br>') || '—'} | ${unexpected.join('<br>') || '—'} | ${ok? '✅':'❌'} |`);
              report.push({scenario: s.name, expected: exp, missing, unexpected, ok});
            }
            // Detect stray artifacts from scenarios not defined (defensive)
            // Defensive catch-all: if a reusable scenario starts emitting new artifact names that
            // are not declared in the matrix above, flag them so the self-test can evolve in lockstep.
            const stray = [...names].filter(n => n.startsWith('sf-') && !expectedAll.has(n));
            if(stray.length){
              rows.push(`| (stray) | (n/a) | (n/a) | ${stray.join('<br>')} | ❌ |`);
              report.push({scenario:'_stray_', expected:[], missing:[], unexpected:stray, ok:false});
              failures++;
            }
            const table = rows.join('\n');
            core.setOutput('table', table);
            core.setOutput('failures', String(failures));
            const summary = { run_id, artifact_count: artifacts.length, failures, scenarios: report };
            require('fs').writeFileSync('selftest-report.json', JSON.stringify(summary, null, 2));
      - name: Append verification table
        run: |
          {
            echo "### Artifact Verification"
            echo "${{ steps.verify.outputs.table }}"
          } >> "$GITHUB_STEP_SUMMARY"
      - name: Upload self-test report artifact
        uses: actions/upload-artifact@v4
        with:
          name: selftest-report
          path: selftest-report.json
      - name: Fail if mismatches
        if: ${{ steps.verify.outputs.failures != '0' }}
        run: |
          echo "Artifact expectation mismatches detected." >&2
          exit 1

