Issue 1: Explain Results metric catalog should include turnover + cost diagnostics
## Why

The Explain Results Q&A flow treats metrics like "turnover" as a known request keyword, but the metric catalog currently does not include turnover/cost diagnostics. This causes unnecessary "requested data is unavailable" refusals for questions about turnover or transaction costs, even when the underlying analysis output contains those values.

## Scope

Extend the metric catalog extraction used by Explain Results to include scalar turnover and transaction cost metrics from `risk_diagnostics` (and multi-period turnover series when present). Ensure these metrics are available for citation and "missing metric" detection.

## Non-Goals

Do not change how turnover or transaction costs are computed in the pipeline.
Do not add new analytics or risk models.
Do not loosen citation requirements for numeric claims.

## Tasks

- [x] Add extraction of `risk_diagnostics` scalar fields in `src/trend_analysis/llm/result_metrics.py` (support dicts and objects with attributes).
- [x] Include turnover fields (e.g., `turnover`, `turnover_value`) in the metric catalog with source `[from risk_diagnostics]`.
- [x] Include transaction cost fields (e.g., `transaction_cost`, `cost`, `per_trade_bps`, `half_spread_bps`) in the metric catalog when present.
- [x] When `details["turnover"]` is present as a `pd.Series` (multi-period), add scalar summaries under deterministic paths (e.g., `turnover.latest`, `turnover.mean`) with an explicit source label.
- [x] Update metric synonyms / keyword mapping so "turnover" and "transaction cost" are treated as available when entries exist.
- [x] Update `tests/test_result_metric_extraction.py` to validate new entries are extracted and formatted with correct `[from ...]` sources.
- [x] Update `tests/test_result_validation.py` to ensure `detect_unavailable_metric_requests()` does not flag turnover when turnover entries exist.

## Acceptance Criteria

- [x] `extract_metric_catalog()` returns at least one turnover entry when turnover is present in `risk_diagnostics` or `details["turnover"]`.
- [x] `extract_metric_catalog()` returns at least one transaction cost entry when cost fields exist in `risk_diagnostics`.
- [x] `format_metric_catalog()` renders these entries with explicit `[from <source>]` annotations.
- [x] `detect_unavailable_metric_requests("Report turnover", entries)` returns no missing turnover keyword when turnover entries exist.
- [x] All unit tests pass.

## Implementation Notes

Primary files:
- `src/trend_analysis/llm/result_metrics.py`
- `tests/test_result_metric_extraction.py`
- `tests/test_result_validation.py`

Suggested labels: `agent:codex`, `status:ready`.

Issue 2: Cap Explain Results prompt size with a deterministic compact metric catalog
## Why

The current Explain Results prompt can become extremely large because it includes per-fund metrics across multiple sections (and weights) without any cap. With larger universes, this risks token-limit failures, slow responses, and reduced answer quality.

## Scope

Introduce a deterministic, configurable compaction step for the metric catalog used by Explain Results (Streamlit + CLI). Keep the output useful by prioritizing portfolio-level stats and top holdings, while keeping catalog size bounded.

## Non-Goals

Do not change the underlying analysis results payload.
Do not remove the ability to cite metrics.
Do not introduce non-deterministic sampling in catalog selection.

## Tasks

- [x] Add `compact_metric_catalog(...)` helper in `src/trend_analysis/llm/result_metrics.py` that selects a bounded subset of `MetricEntry` objects.
- [x] Implement deterministic selection rules:
- [x] Always include `_SINGLE_STATS_SECTIONS` entries (e.g., `out_user_stats`, `out_ew_stats`, etc.).
- [x] Always include benchmark IR entries (if present).
- [x] Include top-N weights from `fund_weights` (and `ew_weights` if `fund_weights` absent), sorted by absolute weight.
- [x] Include per-fund stats only for funds in the selected top-N weights set (and optionally funds explicitly mentioned in the user questions).
- [x] Add configuration knobs (env vars) for caps, e.g. `TREND_EXPLAIN_MAX_FUNDS`, `TREND_EXPLAIN_MAX_WEIGHTS`, `TREND_EXPLAIN_MAX_ENTRIES`.
- [x] Update `streamlit_app/components/explain_results.py` to use the compacted catalog before calling `format_metric_catalog(...)`.
- [x] Update `src/trend/cli.py` Explain Results flow to use the same compaction logic (shared helper).
- [x] Add tests that create a synthetic result with many funds and assert the formatted catalog line count stays within bounds and includes top-weight funds.

## Acceptance Criteria

- [x] Explain Results metric catalog size is bounded by `TREND_EXPLAIN_MAX_ENTRIES` (or default cap) for large universes.
- [x] The compact catalog still contains portfolio-level stats and top holdings weights.
- [x] Citations remain valid and discrepancy logging still works.
- [x] All unit tests pass.

## Implementation Notes

Primary files:
- `src/trend_analysis/llm/result_metrics.py`
- `streamlit_app/components/explain_results.py`
- `src/trend/cli.py`
- Add or extend tests under `tests/` (new test file acceptable).

Suggested labels: `agent:codex`, `status:ready`.

Issue 3: Reduce discrepancy-log noise by ignoring date-like numbers in result validation
## Why

The discrepancy logger currently flags all uncited numbers, including dates/years (e.g., "2023-01 to 2024-12"). This creates excessive false positives in the discrepancy log, making the log harder to trust and less actionable.

## Scope

Update result validation so date-like numbers (years and YYYY-MM / YYYY-MM-DD components) are not treated as uncited metric claims. Preserve strict validation for actual numeric performance claims.

## Non-Goals

Do not weaken validation for real metric claims (returns, Sharpe, drawdown, etc.).
Do not remove the requirement that at least one cited metric value is present.

## Tasks

- [x] Add a helper in `src/trend_analysis/llm/result_validation.py` to detect date-like numeric spans (e.g., numbers that appear within YYYY-MM or YYYY-MM-DD patterns).
- [x] Update `validate_result_claims()` so date-like numbers do not generate `uncited_value` issues.
- [x] Add unit tests in `tests/test_result_validation.py` to confirm that a date range (e.g., "2023-01 to 2024-12") does not produce `uncited_value` issues.
- [x] Add unit tests to confirm that non-date uncited metric values (e.g., "CAGR was 8%.") are still flagged when missing citations.

## Acceptance Criteria

- [x] Date-like numbers inside common date formats do not produce `uncited_value` issues.
- [x] Actual metric numbers still produce `uncited_value` issues when uncited.
- [x] Existing hallucination detection behavior remains intact.
- [x] All unit tests pass.

## Implementation Notes

Primary files:
- `src/trend_analysis/llm/result_validation.py`
- `tests/test_result_validation.py`

Suggested labels: `agent:codex`, `status:ready`.

Issue 4: Wire Explain Results into Streamlit Results page
## Why

The Explain Results component exists (`streamlit_app/components/explain_results.py`) and is imported by the Results page, but it is not actually rendered from `streamlit_app/pages/3_Results.py`. Users therefore cannot access Explain Results from the UI.

## Scope

Render the Explain Results component on the Streamlit Results page for completed runs, using a stable `run_key` for caching.

## Non-Goals

Do not redesign the Results page layout.
Do not change the Explain Results chain behavior beyond UI integration.

## Tasks

- [ ] Identify the section in `streamlit_app/pages/3_Results.py` where a completed analysis result is rendered.
- [ ] Compute a stable `run_key` for the current run (reuse existing helpers such as `_current_run_key(...)` where appropriate).
- [ ] Call `streamlit_app.components.explain_results.render_explain_results(result, run_key=run_key)` for completed runs.
- [ ] Ensure the UI path gracefully handles missing `details` payloads (component already supports this).
- [ ] Add a lightweight test that asserts the Results page contains a call site for `render_explain_results` (e.g., file-content assertion) to prevent regressions.

## Acceptance Criteria

- [ ] The Streamlit Results page displays an "Explain Results" section for completed runs.
- [ ] Clicking the Explain Results button generates output or a clear error message when LLM configuration is missing.
- [ ] Cached results are reused for the same `run_key`.
- [ ] All unit tests pass.

## Implementation Notes

Primary files:
- `streamlit_app/pages/3_Results.py`
- `streamlit_app/components/explain_results.py`

Suggested labels: `agent:codex`, `status:ready`.

Issue 5: Export Explain Results output as reproducibility artifacts (CLI + Streamlit download)
## Why

Explain Results outputs are currently ephemeral (printed/rendered only). Persisting explanation artifacts enables reproducibility, sharing, and auditability (especially since the system emphasizes grounding, discrepancy logs, and disclaimers).

## Scope

Add artifact writing support for Explain Results in the CLI and add download buttons in Streamlit once an explanation is generated. Artifacts should include both the final text and a structured JSON payload (claim issues, trace URL, metric count).

## Non-Goals

Do not change existing analysis artifact naming for metrics/details.
Do not require LLM availability in CI.

## Tasks

- [ ] Add `--output` (directory or file prefix) option to the `trend explain` subcommand in `src/trend/cli.py`.
- [ ] Implement writing:
- [ ] `explanation_<run-id>.txt` containing the final rendered text (including discrepancy log + disclaimer).
- [ ] `explanation_<run-id>.json` containing `{ run_id, created_at, text, metric_count, trace_url, claim_issues }`.
- [ ] Add a helper to serialize `ResultClaimIssue` objects into JSON-friendly dicts.
- [ ] Update `streamlit_app/components/explain_results.py` to show download buttons for:
- [ ] Explanation text (TXT)
- [ ] Structured JSON (claim issues, trace URL, metric count)
- [ ] Add unit tests for the serializer helper and artifact path generation (no live LLM required).

## Acceptance Criteria

- [ ] `trend explain --details <path> --output <dir>` writes both TXT and JSON explanation artifacts.
- [ ] Streamlit Explain Results section provides download buttons after an explanation is generated.
- [ ] JSON artifact includes claim issues and trace URL when available.
- [ ] All unit tests pass.

## Implementation Notes

Primary files:
- `src/trend/cli.py`
- `streamlit_app/components/explain_results.py`
- Add a new test module under `tests/` for artifact serialization if needed.

Suggested labels: `agent:codex`, `status:ready`.
