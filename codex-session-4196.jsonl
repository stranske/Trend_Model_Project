{"type":"thread.started","thread_id":"019b87cf-1bf0-7b22-b2fe-34ba40a25f4a"}
{"type":"turn.started"}
{"type":"item.completed","item":{"id":"item_0","type":"reasoning","text":"**Preparing to inspect repository**"}}
{"type":"item.started","item":{"id":"item_1","type":"command_execution","command":"/bin/bash -lc ls","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_1","type":"command_execution","command":"/bin/bash -lc ls","aggregated_output":"Agents.md\nCHANGELOG.md\nCLAUDE.md\nCODE_OF_CONDUCT.md\nCONTRIBUTING.md\nDEPENDENCY_QUICKSTART.md\nDOCKER_QUICKSTART.md\nDockerfile\nIssues.txt\nLICENSE\nMANIFEST.in\nMakefile\nREADME.md\nREADME_APP.md\nREADME_DATA.md\nSECURITY.md\nTrend Universe Data.csv\nagents\nanalysis\narchives\nassets\ncliff.toml\ncodex-prompt-4196.md\ncodex-session-4136.jsonl\ncodex-session-4138.jsonl\ncodex-session-4148.jsonl\ncodex-session-4150.jsonl\ncodex-session-4151.jsonl\ncodex-session-4154.jsonl\ncodex-session-4176.jsonl\ncodex-session-4196.jsonl\nconfig\ncoverage-summary.md\ndata\ndemo\ndocker-compose.yml\ndocs\nexamples\ngate-summary.md\nissues\nkeepalive_status.md\nman\nnotebooks\npackage-lock.json\npackage.json\nperf\npr_body.md\npyproject.toml\npytest.ini\nreports\nrequirements.lock\nretired\nscripts\nsrc\nstreamlit_app\ntests\ntools\ntrend_model.egg-info\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_2","type":"reasoning","text":"**Checking for AGENTS instructions**"}}
{"type":"item.started","item":{"id":"item_3","type":"command_execution","command":"/bin/bash -lc 'cat AGENTS.md'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_3","type":"command_execution","command":"/bin/bash -lc 'cat AGENTS.md'","aggregated_output":"cat: AGENTS.md: No such file or directory\n","exit_code":1,"status":"failed"}}
{"type":"item.completed","item":{"id":"item_4","type":"reasoning","text":"**Checking for Agents.md file**"}}
{"type":"item.started","item":{"id":"item_5","type":"command_execution","command":"/bin/bash -lc 'cat Agents.md'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_5","type":"command_execution","command":"/bin/bash -lc 'cat Agents.md'","aggregated_output":"# Agents.md\n\n> **Canonical implementation roadmap**: See [docs/phase-2/Agents.md](docs/phase-2/Agents.md) for the complete Phase-2 implementation spec and step-by-step guidance.\n\n## Mission\n\nConverge the scattered modules into one fully test-covered, vectorised pipeline that can be invoked from a single CLI entry-point. Never touch notebooks living under any directory whose name ends in `old/`.\n\n---\n\n## Quick Reference\n\n### Demo Pipeline\n\n```bash\n# 1. Bootstrap environment\n./scripts/setup_env.sh\n\n# 2. Generate demo dataset\npython scripts/generate_demo.py\n\n# 3. Run full demo pipeline\npython scripts/run_multi_demo.py\n\n# 4. Run test suite\n./scripts/run_tests.sh\n```\n\nSee [docs/DemoMaintenance.md](docs/DemoMaintenance.md) for the full checklist.\n\n### Key Entry Points\n\n| Purpose | Command |\n|---------|---------|\n| CLI analysis | `PYTHONPATH=\"./src\" python -m trend_analysis.run_analysis -c config/demo.yml` |\n| Streamlit app | `./scripts/run_streamlit.sh` |\n| Tests | `./scripts/run_tests.sh` |\n| Fast validation | `./scripts/dev_check.sh --fix` |\n\n---\n\n## Automation Entry Points\n\n### Agents 70 Orchestrator\n- **File**: `.github/workflows/agents-70-orchestrator.yml`\n- **Role**: Single automation front door for all agent operations\n- **Triggers**: 20-minute schedule sweep plus manual `workflow_dispatch`\n\n### Agents 63 Issue Intake\n- **File**: `.github/workflows/agents-63-issue-intake.yml`\n- **Role**: Bootstrap Codex PRs on `agent:codex` labeled issues\n- **Triggers**: Issue events (`opened`, `labeled`, `reopened`) plus manual dispatch\n\n### Agents 64 Verify Agent Assignment\n- **File**: `.github/workflows/agents-64-verify-agent-assignment.yml`\n- **Role**: Validates agent assignment before orchestrator proceeds\n\nLegacy consumer wrappers were retired. See [docs/archive/ARCHIVE_WORKFLOWS.md](docs/archive/ARCHIVE_WORKFLOWS.md).\n\n---\n\n## Canonical Module Locations\n\n| Layer / Concern | Canonical Location | Deprecated |\n|-----------------|-------------------|------------|\n| Data ingest & cleaning | `trend_analysis/data.py` | `data_utils.py`, notebook helpers |\n| Portfolio logic & metrics | `trend_analysis/metrics.py` | loops in `run_analysis.py` |\n| Export / I/O | `trend_analysis/export/` | root-level `exports.py` |\n| Domain kernels | `trend_analysis/core/` | standalone `core/` directory |\n| Pipeline orchestration | `trend_analysis/pipeline.py` | duplicated control flow |\n| CLI entry-point | `trend_analysis/cli.py` | bespoke `scripts/*.py` |\n| Config | `config/defaults.yml` via `trend_analysis.config.load()` | hard-coded constants |\n| Tests | `tests/` (pytest with coverage gate) | â€” |\n\n**Rule**: One concern â†’ one module. Replacements must delete or comment-out whatever they obsolete in the same PR.\n\n---\n\n## Guard-Rails\n\n### Communication\n- When a request contains multiple tasks, explicitly recap which items are complete and which remain before handing control back.\n- Call out any suggestion that is only a partial fix, and list the follow-up steps required for it to succeed.\n- Highlight assumptions about workflow or automation status and offer to pull the live data when needed.\n\n### Code Quality\n- **Vectorise first**: Falling back to for-loops requires a comment justifying why vectorisation is impossible or harmful.\n- **Public API**: Uses US-English snake_case; private helpers prefixed with `_`.\n- **No circular imports**: `pipeline.py` orchestrates; nothing imports it.\n- **Test fixtures**: Must be text-serialised (CSV/JSON); no binary formats in PRs.\n\n### Git Workflow\nBefore pushing, verify the target PR is still open:\n```bash\ngh pr view --json state,mergedAt,closed\n```\n- âŒ Never push to merged/closed PRs\n- âœ… Create a new branch and PR if the original is closed\n\n---\n\n## Export Guard-Rails\n\n> ğŸ›¡ï¸ Codex removed the pretty reporting layer once; it shall not happen again.\n\n1. **Call the canonical exporters**: After `pipeline.run()` completes, pipe results into exactly one of `export_to_excel`, `export_to_csv`, or `export_to_json`.\n\n2. **Excel format contract**:\n   - Bold title row\n   - `0.00%` for CAGR & Vol, `0.00` for Sharpe & Sortino\n   - Red numerals for MaxDD\n   - Freeze panes on header, auto-filter\n   - Column width = `max(len(header)) + 2`\n\n3. **Column order is law**: Tests must fail if column order mutates.\n\n4. **Back-compat**: Silent config = drop fully formatted Excel workbook into `outputs/` exactly as v1.0 did.\n\n---\n\n## Multi-Period Export (Phase-2)\n\nâœ… **Implemented**: Multi-period runs now emit Phase-1 style exports via:\n- `export_phase1_workbook()` - Excel with one sheet per period + summary tab\n- `export_phase1_multi_metrics()` - CSV/JSON with `*_periods.*` and `*_summary.*` files\n- `export_multi_period_metrics()` - General multi-period export helper\n\nHelper functions:\n- `workbook_frames_from_results()` - Builds sheet mapping\n- `period_frames_from_results()` - Converts result sequence to export format\n- `combined_summary_result()` - Aggregates portfolio returns across periods\n\n---\n\n## Feature Status\n\n### Implemented âœ…\n- Rank-based manager selection mode (`mode: rank`)\n- Blended scoring with z-score normalization\n- Scalar metric memoization (opt-in via `performance.cache.metrics: true`)\n- PR draft toggle for Codex bootstrap (`codex_pr_draft` input)\n- Multi-period Phase-1 style exports\n- Selector and weighting plugin classes\n\n### Backlog ğŸ“‹\n- Preview score frame in UI\n- Weight heatmap visualization\n- Expected shortfall metric\n- Diversification value metric\n- Export commit hash in outputs\n\n---\n\n## Debugging\n\n### Fund Selection Issues\n\nUse the debug script in `examples/`:\n```bash\npython examples/debug_fund_selection.py\n```\n\nThis reveals:\n- Which managers get filtered due to missing data\n- Available manager pool for selection\n- Actual ranking results\n\n### Common Pitfalls\n- Don't assume ranking is wrong without checking data completeness first\n- Both in-sample AND out-of-sample periods must have complete data\n- Verify configuration parameters match intended behavior\n\n---\n\n## Related Documentation\n\n- [docs/phase-2/Agents.md](docs/phase-2/Agents.md) - Complete implementation spec\n- [docs/DemoMaintenance.md](docs/DemoMaintenance.md) - Demo pipeline checklist\n- [docs/archive/ARCHIVE_WORKFLOWS.md](docs/archive/ARCHIVE_WORKFLOWS.md) - Retired workflows\n- [docs/metric_cache.md](docs/metric_cache.md) - Metric memoization details\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_6","type":"reasoning","text":"**Starting implementation by defining RunPayload**"}}
{"type":"item.started","item":{"id":"item_7","type":"command_execution","command":"/bin/bash -lc \"rg -n \\\"DiagnosticResult|PipelineResult|AnalysisResult|_coerce_analysis_result|run_full|run\\\\(\\\" src/trend src/trend_analysis streamlit_app tests\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_7","type":"command_execution","command":"/bin/bash -lc \"rg -n \\\"DiagnosticResult|PipelineResult|AnalysisResult|_coerce_analysis_result|run_full|run\\\\(\\\" src/trend src/trend_analysis streamlit_app tests\"","aggregated_output":"src/trend/diagnostics.py:35:class DiagnosticResult(Generic[T]):\nsrc/trend/diagnostics.py:56:    def success(cls, value: T) -> \"DiagnosticResult[T]\":\nsrc/trend/diagnostics.py:66:    ) -> \"DiagnosticResult[T]\":\nsrc/trend/cli.py:19:from trend.diagnostics import DiagnosticPayload, DiagnosticResult\nsrc/trend/cli.py:120:def _init_perf_logger(app_name: str = \"app\") -> DiagnosticResult[Path]:\nsrc/trend/cli.py:133:        return DiagnosticResult(value=None, diagnostic=diagnostic)\nsrc/trend/cli.py:144:        return DiagnosticResult(value=None, diagnostic=diagnostic)\nsrc/trend/cli.py:148:    return DiagnosticResult.success(log_path)\nsrc/trend/cli.py:660:def _maybe_write_turnover_csv(directory: Path, details: Any) -> DiagnosticResult[Path]:\nsrc/trend/cli.py:662:        return DiagnosticResult.failure(\nsrc/trend/cli.py:669:        return DiagnosticResult.failure(\nsrc/trend/cli.py:682:        return DiagnosticResult.failure(\nsrc/trend/cli.py:690:        return DiagnosticResult.failure(\nsrc/trend/cli.py:696:        return DiagnosticResult.failure(\nsrc/trend/cli.py:706:    return DiagnosticResult.success(path)\nsrc/trend/cli.py:751:def _persist_turnover_ledger(run_id: str, details: Any) -> DiagnosticResult[Path]:\nsrc/trend/cli.py:753:        return DiagnosticResult.failure(\nsrc/trend/cli.py:760:        return DiagnosticResult.failure(\nsrc/trend/cli.py:767:        return DiagnosticResult.failure(\nsrc/trend/cli.py:774:            return DiagnosticResult.failure(\nsrc/trend/cli.py:781:            return DiagnosticResult.failure(\nsrc/trend/cli.py:788:            return DiagnosticResult.failure(\nsrc/trend/cli.py:794:        return DiagnosticResult.failure(\nsrc/trend/cli.py:806:    return DiagnosticResult.success(path_result.value)\nsrc/trend/cli.py:865:            proc = subprocess.run([\"streamlit\", \"run\", str(APP_PATH)])\nsrc/trend_analysis/pipeline_runner.py:8:from .diagnostics import AnalysisResult, PipelineResult\nsrc/trend_analysis/pipeline_runner.py:57:) -> PipelineResult:\nsrc/trend_analysis/pipeline_runner.py:68:    if isinstance(preprocess_stage, PipelineResult):\nsrc/trend_analysis/pipeline_runner.py:78:    if isinstance(window_stage, PipelineResult):\nsrc/trend_analysis/pipeline_runner.py:118:    if isinstance(selection_stage_result, PipelineResult):\nsrc/trend_analysis/pipeline_runner.py:191:) -> AnalysisResult | None:\nsrc/trend_analysis/pipeline_helpers.py:27:    \"_empty_run_full_result\",\nsrc/trend_analysis/pipeline_helpers.py:257:def _empty_run_full_result() -> dict[str, object]:\nsrc/trend/reporting/quick_summary.py:27:from trend.diagnostics import DiagnosticPayload, DiagnosticResult\nsrc/trend/reporting/quick_summary.py:145:def _equity_drawdown_chart(returns: pd.Series) -> DiagnosticResult[str]:\nsrc/trend/reporting/quick_summary.py:148:        return DiagnosticResult.failure(\nsrc/trend/reporting/quick_summary.py:163:    return DiagnosticResult.success(_encode_plot(fig))\nsrc/trend/reporting/quick_summary.py:166:def _turnover_chart(turnover: pd.Series) -> DiagnosticResult[str]:\nsrc/trend/reporting/quick_summary.py:168:        return DiagnosticResult.failure(\nsrc/trend/reporting/quick_summary.py:180:    return DiagnosticResult.success(_encode_plot(fig))\nsrc/trend_analysis/diagnostics.py:10:from trend.diagnostics import DiagnosticPayload, DiagnosticResult\nsrc/trend_analysis/diagnostics.py:12:AnalysisResult = dict[str, object]\nsrc/trend_analysis/diagnostics.py:16:class PipelineResult(Mapping[str, object]):\nsrc/trend_analysis/diagnostics.py:19:    value: AnalysisResult | None\nsrc/trend_analysis/diagnostics.py:22:    def _require_value(self) -> AnalysisResult:\nsrc/trend_analysis/diagnostics.py:58:    def copy(self) -> AnalysisResult:\nsrc/trend_analysis/diagnostics.py:62:    def unwrap(self) -> AnalysisResult | None:\nsrc/trend_analysis/diagnostics.py:95:def pipeline_success(value: AnalysisResult) -> PipelineResult:\nsrc/trend_analysis/diagnostics.py:98:    return PipelineResult(value=value, diagnostic=None)\nsrc/trend_analysis/diagnostics.py:106:) -> PipelineResult:\nsrc/trend_analysis/diagnostics.py:122:    return PipelineResult(value=None, diagnostic=payload)\nsrc/trend_analysis/diagnostics.py:127:) -> Tuple[AnalysisResult | None, DiagnosticPayload | None]:\nsrc/trend_analysis/diagnostics.py:130:    if isinstance(result, PipelineResult):\nsrc/trend_analysis/diagnostics.py:143:    if isinstance(result, DiagnosticResult):\nsrc/trend_analysis/diagnostics.py:144:        payload = cast(AnalysisResult | None, result.value)\nsrc/trend_analysis/diagnostics.py:146:        payload = cast(AnalysisResult | None, result)\nsrc/trend_analysis/diagnostics.py:148:        payload = cast(AnalysisResult | None, getattr(result, \"value\"))\nsrc/trend_analysis/diagnostics.py:150:        payload = cast(AnalysisResult | None, result)\nsrc/trend_analysis/diagnostics.py:166:    \"AnalysisResult\",\nsrc/trend_analysis/diagnostics.py:167:    \"PipelineResult\",\nstreamlit_app/components/disclaimer.py:42:                st.rerun()\nsrc/trend/reporting/unified.py:18:from trend.diagnostics import DiagnosticPayload, DiagnosticResult\nsrc/trend/reporting/unified.py:180:def _build_backtest(result: Any) -> DiagnosticResult[BacktestResult]:\nsrc/trend/reporting/unified.py:190:        return DiagnosticResult.failure(\nsrc/trend/reporting/unified.py:264:    return DiagnosticResult.success(\nsrc/trend/reporting/unified.py:552:def _turnover_chart(backtest: BacktestResult | None) -> DiagnosticResult[str]:\nsrc/trend/reporting/unified.py:554:        return DiagnosticResult.failure(\nsrc/trend/reporting/unified.py:572:    return DiagnosticResult.success(_render_chart(fig))\nsrc/trend_analysis/stages/portfolio.py:14:from ..diagnostics import PipelineResult, pipeline_success\nsrc/trend_analysis/stages/portfolio.py:686:) -> PipelineResult:\nsrc/trend_analysis/stages/selection.py:18:from ..diagnostics import PipelineReasonCode, PipelineResult, pipeline_failure\nsrc/trend_analysis/stages/selection.py:256:def single_period_run(\nsrc/trend_analysis/stages/selection.py:373:) -> _SelectionStage | PipelineResult:\nsrc/trend_analysis/stages/selection.py:519:    score_frame = single_period_run(\nsrc/trend_analysis/stages/preprocessing.py:8:from ..diagnostics import PipelineReasonCode, PipelineResult, pipeline_failure\nsrc/trend_analysis/stages/preprocessing.py:84:) -> _PreprocessStage | PipelineResult:\nsrc/trend_analysis/stages/preprocessing.py:252:) -> _WindowStage | PipelineResult:\nstreamlit_app/pages/2_Model.py:689:                    st.rerun()\nstreamlit_app/pages/2_Model.py:721:                    st.rerun()\nstreamlit_app/pages/2_Model.py:745:                        st.rerun()\nstreamlit_app/pages/2_Model.py:759:                    st.rerun()\nstreamlit_app/pages/2_Model.py:816:                        st.rerun()\nstreamlit_app/pages/2_Model.py:1129:            st.rerun()\nstreamlit_app/pages/1_Data.py:334:                st.rerun()\nstreamlit_app/pages/1_Data.py:374:        st.rerun()\nstreamlit_app/pages/1_Data.py:666:                    st.rerun()\nstreamlit_app/pages/1_Data.py:673:                    st.rerun()\nstreamlit_app/pages/1_Data.py:687:                    st.rerun()\nstreamlit_app/pages/1_Data.py:722:                        st.rerun()\nstreamlit_app/pages/1_Data.py:727:                        st.rerun()\nstreamlit_app/pages/3_Results.py:2006:                    st.rerun()\nsrc/trend_analysis/api_server/__init__.py:64:def run(host: str = \"127.0.0.1\", port: int = 8000) -> Tuple[str, int]:\nsrc/trend_analysis/api_server/__init__.py:76:    uvicorn.run(\nsrc/trend_analysis/api_server/__init__.py:87:    run()\nsrc/trend_analysis/api_server/__main__.py:7:    run(host=\"0.0.0.0\", port=8000)\ntests/test_cli_smoke.py:13:    result = subprocess.run(\ntests/test_cli_smoke.py:28:    result = subprocess.run(\ntests/test_cli_smoke.py:45:    result = subprocess.run(\ntests/test_cli_smoke.py:59:    result = subprocess.run(\ntests/test_cli_smoke.py:82:    result = subprocess.run(\nsrc/trend_analysis/gui/app.py:857:    def on_run(_: Any, *, store: ParamStore) -> None:\nsrc/trend_analysis/gui/app.py:859:        metrics = pipeline.run(cfg)\nsrc/trend_analysis/gui/app.py:867:            full_result = pipeline.run_full(cfg)\nsrc/trend_analysis/gui/app.py:897:    run_btn.on_click(lambda btn, store=store: on_run(btn, store=store))\nsrc/trend_analysis/proxy/server.py:279:    asyncio.run(main())\ntests/test_score_frame.py:24:    sf = single_period_run(df, \"2020-01\", \"2020-03\")\ntests/test_score_frame.py:33:    sf = single_period_run(df, \"2020-01\", \"2020-03\", stats_cfg=cfg)\ntests/test_score_frame.py:40:    sf = single_period_run(df, \"2020-01\", \"2020-03\")\ntests/test_score_frame.py:48:        single_period_run(df, \"2020-01\", \"2020-03\")\ntests/test_score_frame.py:55:        single_period_run(df, \"2020-01\", \"2020-03\", stats_cfg=cfg)\nsrc/trend_analysis/cli.py:394:        proc = subprocess.run([\"streamlit\", \"run\", str(APP_PATH)])\nsrc/trend_analysis/cli.py:524:            metrics_df = pipeline.run(cfg)\nsrc/trend_analysis/cli.py:525:            full_result = pipeline.run_full(cfg)\ntests/test_multi_period_engine_incremental_extra.py:80:    def fake_run(*args: Any, **kwargs: Any) -> Dict[str, Any]:\ntests/test_multi_period_engine_incremental_extra.py:110:    results = mp_engine.run(cfg, df=_make_df())\ntests/test_multi_period_engine_incremental_extra.py:146:    def fake_run(*args: Any, **kwargs: Any) -> Dict[str, Any]:\ntests/test_multi_period_engine_incremental_extra.py:168:    results = mp_engine.run(cfg, df=_make_df())\ntests/test_multi_period_engine_incremental_extra.py:203:    def fake_run(*args: Any, **kwargs: Any) -> Dict[str, Any]:\ntests/test_multi_period_engine_incremental_extra.py:221:    results = mp_engine.run(cfg, df=_make_df())\ntests/test_multi_period_engine_incremental_extra.py:255:    def fake_run(*_args: Any, **_kwargs: Any) -> Dict[str, Any]:\ntests/test_multi_period_engine_incremental_extra.py:266:    results = mp_engine.run(cfg, df=_make_df())\ntests/test_app_coverage.py:911:        asyncio.run(result)\ntests/test_app_coverage.py:1373:        \"run_full\",\ntests/test_app_coverage.py:1495:        \"run_full\",\ntests/test_app_coverage.py:1496:        lambda cfg: pytest.fail(\"run_full should not execute\"),\ntests/test_app_coverage.py:1556:        \"run_full\",\ntests/test_app_coverage.py:1557:        lambda cfg: pytest.fail(\"run_full should not run\"),\ntests/test_gui_launch_run.py:25:    monkeypatch.setattr(app.pipeline, \"run_full\", lambda cfg: {\"result\": 1})\ntests/test_gui_launch_run.py:63:    monkeypatch.setattr(app.pipeline, \"run_full\", lambda cfg: {\"full\": \"results\"})\ntests/test_pipeline_run_cache_fallbacks.py:85:    result = pipeline.run(cfg)\ntests/test_pipeline_run_cache_fallbacks.py:94:def test_run_full_uses_nan_fallbacks(monkeypatch: pytest.MonkeyPatch) -> None:\ntests/test_pipeline_run_cache_fallbacks.py:136:    result = pipeline.run_full(cfg)\ntests/test_pipeline_run_cache_fallbacks.py:146:        pipeline.run({\"data\": {}})\ntests/test_pipeline_run_cache_fallbacks.py:149:def test_run_full_requires_csv_path() -> None:\ntests/test_pipeline_run_cache_fallbacks.py:151:        pipeline.run_full({\"data\": {}})\ntests/test_pipeline_run_cache_fallbacks.py:170:        pipeline, \"_run_analysis\", lambda *a, **k: pipeline._empty_run_full_result()\ntests/test_pipeline_run_cache_fallbacks.py:187:    result = pipeline.run(cfg)\ntests/test_pipeline_run_cache_fallbacks.py:193:def test_run_full_handles_missing_data_section(monkeypatch: pytest.MonkeyPatch) -> None:\ntests/test_pipeline_run_cache_fallbacks.py:228:    payload = pipeline.run_full(cfg)\ntests/test_pipeline_run_cache_fallbacks.py:272:    empty = pipeline._empty_run_full_result()\ntests/test_pipeline_run_cache_fallbacks.py:363:        pipeline.single_period_run(df, \"2020-01\", \"2020-02\")\ntests/test_pipeline_run_cache_fallbacks.py:373:        pipeline.single_period_run(monthly, \"2020-01\", \"2020-02\", stats_cfg=cfg)\ntests/test_pipeline_run_cache_fallbacks.py:637:    res = pipeline.single_period_run(df, \"2020-01\", \"2020-03\")\nsrc/trend_analysis/multi_period/engine.py:29:from trend.diagnostics import DiagnosticPayload, DiagnosticResult\nsrc/trend_analysis/multi_period/engine.py:35:from ..diagnostics import PipelineResult\nsrc/trend_analysis/multi_period/engine.py:80:def _run_analysis(*args: Any, **kwargs: Any) -> PipelineResult:\nsrc/trend_analysis/multi_period/engine.py:86:) -> DiagnosticResult[dict[str, Any] | None]:\nsrc/trend_analysis/multi_period/engine.py:87:    \"\"\"Execute ``_run_analysis`` and normalise into a DiagnosticResult.\"\"\"\nsrc/trend_analysis/multi_period/engine.py:89:    payload, diag = _coerce_analysis_result(_run_analysis(*args, **kwargs))\nsrc/trend_analysis/multi_period/engine.py:90:    return DiagnosticResult(value=payload, diagnostic=diag)\nsrc/trend_analysis/multi_period/engine.py:93:def _coerce_analysis_result(\nsrc/trend_analysis/multi_period/engine.py:682:def run(\nsrc/trend_analysis/multi_period/engine.py:3920:    return run(cfg, df=prices, membership=membership_arg)\ntests/test_import_sanity.py:77:    subprocess.run(\ntests/test_rebalance_frequency_wiring.py:91:    monthly_results = mp_engine.run(cfg, _sample_returns())\ntests/test_rebalance_frequency_wiring.py:95:    annual_results = mp_engine.run(cfg, _sample_returns())\ntests/test_multi_period_engine_branch_coverage.py:227:    results = mp_engine.run(cfg, df=df)\ntests/test_multi_period_engine_branch_coverage.py:312:    output = mp_engine.run(cfg, df=None)\ntests/test_multi_period_engine_branch_coverage.py:323:        mp_engine.run(cfg, df=None)\ntests/test_multi_period_engine_branch_coverage.py:360:    results = mp_engine.run(cfg, df=None, price_frames=price_frames)\ntests/test_diagnostics.py:5:from trend.diagnostics import DiagnosticPayload, DiagnosticResult\ntests/test_diagnostics.py:11:    result = DiagnosticResult.success(payload)\ntests/test_diagnostics.py:19:    result = DiagnosticResult.failure(\ntests/test_diagnostics.py:46:    result = diagnostics.PipelineResult(value=None)\ntests/test_diagnostics.py:72:    diag_result = DiagnosticResult(value={\"score\": 5})\ntests/test_diagnostics.py:89:    diag_result = DiagnosticResult(value=[1, 2, 3])\ntests/test_diagnostics.py:149:    result = diagnostics.PipelineResult(value=None)\ntests/test_streamlit_smoke_ci.py:41:        subprocess.run(\ntests/test_streamlit_smoke_ci.py:384:    subprocess.run([\"python\", \"scripts/generate_demo.py\"], check=True)\ntests/test_cli.py:141:    def fake_run(cfg):\ntests/test_cli.py:146:    monkeypatch.setattr(cli.pipeline, \"run_full\", lambda cfg: {\"dummy\": 1})\ntests/test_cli.py:163:    monkeypatch.setattr(cli.pipeline, \"run_full\", lambda cfg: None)\ntests/test_cli.py:266:    monkeypatch.setattr(cli.pipeline, \"run_full\", lambda cfg: results_payload)\nsrc/trend_analysis/pipeline_entrypoints.py:10:    from .diagnostics import PipelineResult\nsrc/trend_analysis/pipeline_entrypoints.py:159:def run_full_from_config(cfg: Any, *, bindings: ConfigBindings) -> PipelineResult:\nsrc/trend_analysis/pipeline_entrypoints.py:261:                \"pipeline.run_full aborted (%s): %s\",\nsrc/trend_analysis/pipeline_entrypoints.py:266:            logger.warning(\"pipeline.run_full aborted with no diagnostic context\")\ntests/test_threshold_hold_alignment.py:112:    results = mp_engine.run(cfg, df)\ntests/test_pipeline_branch_coverage.py:85:    result = pipeline.single_period_run(df, \"2024-01\", \"2024-03\", stats_cfg=stats_cfg)\ntests/test_pipeline_branch_coverage.py:186:    def fake_single_period_run(*args: Any, **kwargs: Any) -> pd.DataFrame:\ntests/test_multi_period_engine_incremental_fallback.py:140:    results = mp_engine.run(cfg, df=df)\nsrc/trend_analysis/pipeline.py:8:from trend.diagnostics import DiagnosticResult\nsrc/trend_analysis/pipeline.py:18:from .diagnostics import PipelineReasonCode, PipelineResult\nsrc/trend_analysis/pipeline.py:28:from .pipeline_entrypoints import ConfigBindings, run_from_config, run_full_from_config\nsrc/trend_analysis/pipeline.py:35:    _empty_run_full_result,\nsrc/trend_analysis/pipeline.py:187:def _run_analysis_with_diagnostics(*args: Any, **kwargs: Any) -> PipelineResult:\nsrc/trend_analysis/pipeline.py:205:def _invoke_analysis_with_diag(*args: Any, **kwargs: Any) -> PipelineResult:\nsrc/trend_analysis/pipeline.py:206:    \"\"\"Call the patched analysis hook and normalise into a PipelineResult.\"\"\"\nsrc/trend_analysis/pipeline.py:211:    if isinstance(patched_result, PipelineResult):\nsrc/trend_analysis/pipeline.py:213:    if isinstance(patched_result, DiagnosticResult):\nsrc/trend_analysis/pipeline.py:214:        return PipelineResult(\nsrc/trend_analysis/pipeline.py:218:    return PipelineResult(value=patched_result, diagnostic=None)\nsrc/trend_analysis/pipeline.py:259:) -> PipelineResult:\nsrc/trend_analysis/pipeline.py:329:def run(cfg: Any) -> pd.DataFrame:\nsrc/trend_analysis/pipeline.py:334:def run_full(cfg: Any) -> PipelineResult:\nsrc/trend_analysis/pipeline.py:336:    return run_full_from_config(cfg, bindings=_bindings())\nsrc/trend_analysis/pipeline.py:393:    \"_empty_run_full_result\",\nsrc/trend_analysis/pipeline.py:413:    \"run_full\",\ntests/test_multi_period_sticky_periods.py:139:    return mp_engine.run(cfg, df=_sample_df())\ntests/test_api_run_simulation.py:103:    expected_details = pipeline.run_full(cfg).unwrap()\ntests/test_api_run_simulation.py:105:    expected_metrics = pipeline.run(cfg)\ntests/test_multi_period_inception_filter.py:117:    results = engine.run(cfg, df=_returns_with_preinception_zeros())\ntests/test_volatility_settings_wiring.py:105:    results = mp_engine.run(cfg, _sample_returns())\ntests/test_pipeline_entrypoints.py:25:        pipeline.run({})\ntests/test_pipeline_entrypoints.py:101:    result = pipeline.run(base_config)\ntests/test_pipeline_entrypoints.py:132:    result = pipeline.run(base_config)\ntests/test_pipeline_entrypoints.py:136:def test_run_full_propagates_analysis_payload(\ntests/test_pipeline_entrypoints.py:157:    result = pipeline.run_full(base_config)\ntests/test_pipeline_entrypoints.py:161:def test_run_full_returns_empty_when_analysis_none(\ntests/test_pipeline_entrypoints.py:176:    result = pipeline.run_full(base_config)\ntests/test_pipeline_entrypoints.py:180:def test_empty_run_full_result_template() -> None:\ntests/test_pipeline_entrypoints.py:181:    payload = pipeline._empty_run_full_result()\ntests/test_multi_period_engine_extended.py:170:        engine.run(cfg, df=df, price_frames={\"2020-01\": \"not a frame\"})\ntests/test_multi_period_engine_extended.py:173:        engine.run(\ntests/test_multi_period_engine_extended.py:180:        engine.run(cfg, df=df, price_frames={})\ntests/test_multi_period_engine_extended.py:212:    results = engine.run(cfg, df=None, price_frames=frames)\ntests/test_gui_utils_extra.py:43:    async def run() -> None:\ntests/test_gui_utils_extra.py:48:    asyncio.run(run())\ntests/test_gui_utils_extra.py:72:    async def run() -> None:\ntests/test_gui_utils_extra.py:76:    asyncio.run(run())\ntests/test_gui_utils_extra.py:88:    async def run() -> None:\ntests/test_gui_utils_extra.py:92:    asyncio.run(run())\ntests/test_gui_utils_extra.py:117:    async def run() -> None:\ntests/test_gui_utils_extra.py:121:    asyncio.run(run())\ntests/test_gui_utils_extra.py:143:    async def run() -> None:\ntests/test_gui_utils_extra.py:147:    asyncio.run(run())\ntests/golden/test_demo.py:186:        result = subprocess.run(\ntests/golden/test_demo.py:209:        result = subprocess.run(\ntests/golden/test_demo.py:221:            result = subprocess.run(\ntests/golden/test_demo.py:326:            subprocess.run(\ntests/golden/test_demo.py:336:            subprocess.run(\ntests/test_cli_installed.py:18:    result = subprocess.run([\"trend-model\", \"--help\"], capture_output=True, text=True)\ntests/test_cli_installed.py:33:    result = subprocess.run(\ntests/test_cli_installed.py:49:    result = subprocess.run(\ntests/test_cli_installed.py:80:    result = subprocess.run(\ntests/test_cli_installed.py:98:    result = subprocess.run(\ntests/test_cli_installed.py:106:    result = subprocess.run([\"trend-model\", \"run\"], capture_output=True, text=True)\ntests/test_pipeline_stage_isolation.py:4:from trend_analysis.pipeline import PipelineResult, RiskStatsConfig\ntests/test_pipeline_stage_isolation.py:51:    assert not isinstance(preprocess, PipelineResult)\ntests/test_pipeline_stage_isolation.py:60:    assert not isinstance(window, PipelineResult)\ntests/test_pipeline_stage_isolation.py:78:    assert not isinstance(selection, PipelineResult)\ntests/test_risk_free_default_alignment.py:8:from trend.diagnostics import DiagnosticResult\ntests/test_risk_free_default_alignment.py:139:    def fake_single_run(*_: Any, **kwargs: Any):\ntests/test_risk_free_default_alignment.py:160:    def fake_multi_run(\ntests/test_risk_free_default_alignment.py:162:    ) -> DiagnosticResult[dict[str, Any] | None]:\ntests/test_risk_free_default_alignment.py:167:    mp_engine.run(multi_cfg, frame)\ntests/test_risk_free_default_alignment.py:204:        mp_engine.run(multi_cfg, frame)\ntests/test_pipeline_run_analysis_helpers.py:9:    PipelineResult,\ntests/test_pipeline_run_analysis_helpers.py:207:    assert isinstance(selection, PipelineResult)\ntests/test_pipeline_run_analysis_helpers.py:226:    def _fake_single_period_run(\ntests/test_pipeline_run_analysis_helpers.py:310:    def _fake_single_period_run(\ntests/test_pipeline_run_analysis_helpers.py:397:    def _fake_single_period_run(\ntests/test_pipeline_run_analysis_helpers.py:482:    def _fake_single_period_run(\ntests/test_multi_period_engine_additional.py:114:        engine.run(cfg, price_frames=123)  # type: ignore[arg-type]\ntests/test_multi_period_engine_additional.py:118:        engine.run(cfg, price_frames={\"p1\": bad_frame})\ntests/test_multi_period_engine_additional.py:149:    results = engine.run(cfg, price_frames=price_frames)\ntests/test_multi_period_engine_additional.py:224:    result = engine.run(cfg)\ntests/test_multi_period_engine_additional.py:277:    results = engine.run(cfg, price_frames=price_frames)\ntests/test_multi_period_engine_additional.py:354:    results = engine.run(cfg, df=df)\ntests/test_multi_period_engine_additional.py:409:        engine.run(cfg)\ntests/test_multi_period_engine_additional.py:577:    results = engine.run(cfg, df=df)\ntests/test_multi_period_engine_additional.py:713:    results = engine.run(cfg, df=df)\ntests/test_multi_period_engine_additional.py:844:    results = engine.run(cfg, df=df)\ntests/test_multi_period_engine_additional.py:1004:    results = engine.run(cfg, df=df)\ntests/test_multi_period_engine_additional.py:1022:        engine.run(cfg)\ntests/test_multi_period_engine_additional.py:1032:        engine.run(cfg, df=df)\ntests/test_multi_period_engine_additional.py:1130:    results = engine.run(cfg, df=df)\ntests/test_multi_period_engine_additional.py:1285:    results = engine.run(cfg, df=df)\ntests/test_multi_period_engine_incremental_cov.py:106:    results = mp_engine.run(cfg, df=df)\ntests/test_multi_period_engine_incremental_cov.py:155:    results = mp_engine.run(cfg, df=df)\ntests/test_multi_period_engine_incremental_cov.py:182:    results = mp_engine.run(cfg, df=df)\ntests/test_multi_period_engine_incremental_cov.py:223:    results = mp_engine.run(cfg, df=df)\ntests/test_multi_period_engine_incremental_cov.py:258:    results = mp_engine.run(cfg, df=df)\ntests/test_multi_period_engine_incremental_cov.py:301:    results = mp_engine.run(cfg, df=df)\ntests/test_multi_period_engine_incremental_cov.py:362:    results = mp_engine.run(cfg, df=df)\ntests/test_multi_period_engine_incremental_cov.py:415:    results = mp_engine.run(cfg, df=df)\ntests/test_multi_period_engine_incremental_cov.py:459:    results = mp_engine.run(cfg, df=df)\ntests/test_multi_period_engine_incremental_cov.py:502:    results = mp_engine.run(cfg, df=df)\ntests/test_multi_period_engine_incremental_cov.py:560:    results = mp_engine.run(cfg, df=df)\ntests/test_multi_period_engine_incremental_cov.py:594:    results = mp_engine.run(cfg, df=df)\ntests/test_multi_period_engine_branch_completion.py:11:from trend.diagnostics import DiagnosticResult\ntests/test_multi_period_engine_branch_completion.py:155:        mp_engine.run(cfg, df=None, price_frames={})\ntests/test_multi_period_engine_branch_completion.py:164:        mp_engine.run(cfg, df=None, price_frames=None)\ntests/test_multi_period_engine_branch_completion.py:174:        mp_engine.run(cfg, df=df, price_frames=None)\ntests/test_multi_period_engine_branch_completion.py:185:        mp_engine.run(cfg, df=df, price_frames=None)\ntests/test_multi_period_engine_branch_completion.py:230:    results = mp_engine.run(cfg, df=None, price_frames=price_frames)\ntests/test_multi_period_engine_branch_completion.py:246:    def fake_run(*args: Any, **kwargs: Any) -> dict[str, object]:\ntests/test_multi_period_engine_branch_completion.py:275:    ) -> DiagnosticResult[dict[str, object] | None]:\ntests/test_multi_period_engine_branch_completion.py:277:        return DiagnosticResult(value={\"selected_funds\": [\"Alpha\"]}, diagnostic=None)\ntests/test_multi_period_engine_branch_completion.py:280:    mp_engine.run(cfg, df=_simple_df(), price_frames=None)\ntests/test_multi_period_engine_branch_completion.py:297:    ) -> DiagnosticResult[dict[str, object] | None]:\ntests/test_multi_period_engine_branch_completion.py:299:        return DiagnosticResult(value={\"selected_funds\": [\"Alpha\"]}, diagnostic=None)\ntests/test_multi_period_engine_branch_completion.py:302:    mp_engine.run(cfg, df=_simple_df(), price_frames=None)\ntests/test_multi_period_engine_branch_completion.py:318:    ) -> DiagnosticResult[dict[str, object] | None]:\ntests/test_multi_period_engine_branch_completion.py:320:        return DiagnosticResult(value={\"selected_funds\": [\"Alpha\"]}, diagnostic=None)\ntests/test_multi_period_engine_branch_completion.py:323:    mp_engine.run(cfg, df=_simple_df(), price_frames=None)\ntests/test_multi_period_engine_turnover_extended.py:215:    results = mp_engine.run(cfg, df=df)\ntests/test_pipeline_optional_features.py:76:    score_frame = pipeline.single_period_run(\ntests/test_pipeline_optional_features.py:109:        pipeline.single_period_run(df, \"2020-01\", \"2020-03\", stats_cfg=stats_cfg)\ntests/test_pipeline_optional_features.py:517:    def fake_single_period_run(*args, **kwargs) -> pd.DataFrame:\ntests/test_multi_period_engine_branch_new.py:74:    results = mp_engine.run(cfg, df=None, price_frames=None)\ntests/test_multi_period_engine_branch_new.py:102:    results = mp_engine.run(cfg, df=None, price_frames=price_frames)\ntests/test_multi_period_engine_branch_new.py:126:        mp_engine.run(cfg, df=None, price_frames=None)\ntests/test_determinism_cli.py:34:    r1 = subprocess.run(\ntests/test_determinism_cli.py:41:    r2 = subprocess.run(\ntests/test_determinism_cli.py:86:    subprocess.run(\ntests/test_gui.py:27:    async def run():\ntests/test_gui.py:31:    asyncio.run(run())\ntests/test_trend_cli.py:570:    def fake_run(cmd: list[str]) -> DummyProcess:  # type: ignore[override]\ntests/test_cli_no_structured_log.py:70:    proc = subprocess.run(\ntests/test_trend_cli_soft_coverage.py:319:    def fake_run(args: list[str]) -> DummyResult:\ntests/test_trend_cli_soft_coverage.py:468:    monkeypatch.setattr(cli.pipeline, \"run_full\", lambda cfg_obj: {})\ntests/test_trend_cli_soft_coverage.py:696:        \"run_full\",\ntests/proxy/test_server.py:83:    asyncio.run(proxy._websocket_entry(websocket, \"path\"))\ntests/proxy/test_server.py:95:    asyncio.run(proxy._http_entry(request, \"endpoint\"))\ntests/proxy/test_server.py:103:        asyncio.run(proxy._handle_websocket(types.SimpleNamespace(), \"route\"))\ntests/proxy/test_server.py:133:    asyncio.run(proxy._handle_websocket(websocket, \"stream/path\"))\ntests/proxy/test_server.py:187:    asyncio.run(proxy._handle_websocket(websocket, \"plain/path\"))\ntests/proxy/test_server.py:224:    result = asyncio.run(proxy._handle_http_request(request, \"api/data\"))\ntests/proxy/test_server.py:248:    result = asyncio.run(proxy._handle_http_request(request, \"status\"))\ntests/proxy/test_server.py:266:    asyncio.run(proxy._handle_http_request(request, \"feed\"))\ntests/proxy/test_server.py:283:    asyncio.run(proxy._handle_http_request(request, \"noop\"))\ntests/proxy/test_server.py:293:        asyncio.run(proxy.start(\"127.0.0.1\", 9000))\ntests/proxy/test_server.py:319:    asyncio.run(proxy.start(\"0.0.0.0\", 7777))\ntests/proxy/test_server.py:330:    asyncio.run(proxy.close())\ntests/proxy/test_server.py:352:    def fake_run(coro):\ntests/proxy/test_server.py:353:        return original_run(coro)\ntests/test_selector_weighting.py:36:    result = subprocess.run(cmd, cwd=cwd, capture_output=True, text=True)\ntests/test_multi_period_engine_missing_policy.py:46:def _fake_pipeline_result() -> mp_engine.DiagnosticResult[dict[str, Any]]:\ntests/test_multi_period_engine_missing_policy.py:47:    return mp_engine.DiagnosticResult(value={\"selected_funds\": []}, diagnostic=None)\ntests/test_multi_period_engine_missing_policy.py:74:    results = mp_engine.run(cfg, df=df)\ntests/test_multi_period_engine_missing_policy.py:112:    results = mp_engine.run(cfg, df=df)\ntests/test_multi_period_engine_price_frames.py:56:        mp_engine.run(cfg, price_frames=\"not-a-dict\")\ntests/test_multi_period_engine_price_frames.py:63:        mp_engine.run(cfg, price_frames=bad_frames)\ntests/test_multi_period_engine_price_frames.py:71:        mp_engine.run(cfg, price_frames=bad_frames)\ntests/test_multi_period_engine_price_frames.py:82:        mp_engine.run(cfg, price_frames=frames)\ntests/test_multi_period_engine_price_frames.py:88:        mp_engine.run(cfg, price_frames={})\ntests/test_multi_period_engine_price_frames.py:96:        mp_engine.run(cfg, price_frames={\"2020-01-31\": bad_frame})\ntests/test_multi_period_engine_price_frames.py:136:    results = mp_engine.run(\ntests/test_multi_period_engine_price_frames.py:159:        mp_engine.run(cfg, df=None)\ntests/test_multi_period_engine_price_frames.py:171:        mp_engine.run(cfg, df=None)\ntests/test_api_server_entrypoint.py:9:def test_api_server_entrypoint_invokes_run(monkeypatch):\ntests/test_api_server_entrypoint.py:15:    def fake_run(*, host: str, port: int) -> None:\ntests/test_api_server_entrypoint.py:36:def test_api_server_package_main_invokes_run(monkeypatch):\ntests/test_api_server_entrypoint.py:43:        def run(app, host: str, port: int, *, reload: bool, log_level: str) -> None:\ntests/test_multi_period_engine_keepalive.py:170:    results = mp_engine.run(cfg, df=df)\ntests/test_multi_period_engine_keepalive.py:225:    results = mp_engine.run(\ntests/test_multi_period_engine_keepalive.py:246:        mp_engine.run(cfg)\ntests/test_multi_period_engine_keepalive.py:307:    results = mp_engine.run(cfg, df=df)\ntests/test_multi_period_engine_keepalive.py:338:    def fake_run(\ntests/test_multi_period_engine_price_frames_extra.py:41:        mp_engine.run(cfg, df=df, price_frames=\"not-a-mapping\")\ntests/test_multi_period_engine_price_frames_extra.py:50:        mp_engine.run(cfg, df=df, price_frames=bad_frames)\ntests/test_multi_period_engine_price_frames_extra.py:59:        mp_engine.run(cfg, df=df, price_frames=bad_frames)\ntests/test_multi_period_engine_price_frames_extra.py:69:        mp_engine.run(cfg, df=df, price_frames=price_frames)\ntests/test_multi_period_engine_price_frames_extra.py:82:        mp_engine.run(cfg, df=df, price_frames=bad_frames)\ntests/test_multi_period_engine_price_frames_extra.py:93:        mp_engine.run(cfg, df=None, price_frames={})\ntests/test_multi_period_engine_price_frames_extra.py:132:    results = mp_engine.run(cfg, df=None, price_frames=price_frames)\ntests/test_multi_period_engine_price_frames_extra.py:188:    mp_engine.run(cfg, df=existing_df, price_frames=price_frames)\ntests/test_multi_period_exits_cooldown.py:134:    results = engine.run(cfg, df=_df_5_funds())\ntests/test_multi_period_exits_cooldown.py:186:    results = engine.run(cfg, df=_df_5_funds())\ntests/test_multi_period_exits_cooldown.py:227:    def _run(cooldown_periods: int) -> list[dict[str, Any]]:\ntests/test_multi_period_exits_cooldown.py:231:        return engine.run(cfg, df=_df_5_funds())\ntests/test_multi_period_exits_cooldown.py:233:    no_cooldown = _run(0)\ntests/test_multi_period_exits_cooldown.py:234:    cooldown = _run(2)\ntests/test_multi_period_exits_cooldown.py:288:    results = engine.run(cfg, df=_df_5_funds())\ntests/test_multi_period_exits_cooldown.py:330:    results_no_tenure = engine.run(cfg_no_tenure, df=_df_5_funds())\ntests/test_multi_period_exits_cooldown.py:331:    results_min_tenure = engine.run(cfg_min_tenure, df=_df_5_funds())\ntests/test_multi_period_exits_cooldown.py:374:    results = engine.run(cfg, df=_df_5_funds())\ntests/test_multi_period_exits_cooldown.py:411:    results = engine.run(cfg, df=_df_5_funds())\ntests/test_multi_period_exits_cooldown.py:447:    results = engine.run(cfg, df=_df_5_funds())\ntests/test_multi_period_exits_cooldown.py:483:    results = engine.run(cfg, df=_df_5_funds())\ntests/test_lockfile_consistency.py:71:    result = subprocess.run(\ntests/test_proxy_server_runtime_extra.py:184:    asyncio.run(runner())\ntests/test_proxy_server_runtime_extra.py:199:    asyncio.run(runner())\ntests/test_multi_period_engine_threshold_edgecases.py:125:    def _run(\ntests/test_multi_period_engine_threshold_edgecases.py:180:    results = mp_engine.run(cfg, df=df)\ntests/test_multi_period_engine_threshold_edgecases.py:310:    results = mp_engine.run(cfg, df=df)\ntests/test_multi_period_engine_threshold_edgecases.py:402:    results = mp_engine.run(cfg, df=df)\ntests/test_multi_period_engine_threshold_edgecases.py:599:    results = mp_engine.run(cfg, df=df)\ntests/test_multi_period_engine_threshold_edgecases.py:913:    results = mp_engine.run(cfg, df=df)\ntests/test_multi_period_engine_threshold_edgecases.py:1022:    results = mp_engine.run(cfg, df=df)\ntests/test_multi_period_engine_threshold_edgecases.py:1131:    results = mp_engine.run(cfg, df=df)\ntests/workflows/test_autofix_pipeline.py:13:def _run(cmd: list[str], cwd: Path) -> subprocess.CompletedProcess[str]:\ntests/workflows/test_autofix_pipeline.py:14:    result = subprocess.run(\ntests/workflows/test_autofix_pipeline.py:47:    fail = subprocess.run(\ntests/workflows/test_autofix_pipeline.py:63:        _run(cmd, cwd=tmp_path)\ntests/workflows/test_autofix_pipeline.py:70:    final = subprocess.run(\ntests/test_script_error_handling.py:91:            result = subprocess.run(\ntests/test_script_error_handling.py:105:            result = subprocess.run(\ntests/test_script_error_handling.py:118:            result = subprocess.run(\ntests/workflows/test_keepalive_guard_utils.py:25:        result = subprocess.run(\ntests/tools/test_enforce_gate_branch_protection.py:593:def test_main_reports_no_changes_in_dry_run(\ntests/tools/test_enforce_gate_branch_protection.py:808:def test_main_reports_missing_rule_in_dry_run(\ntests/workflows/test_ci_cosmetic_repair.py:241:    def fake_run(cmd, *, text, capture_output):\ntests/workflows/test_ci_cosmetic_repair.py:262:    def fake_run(cmd, *, cwd=None, text=True, capture_output=True):\ntests/workflows/test_ci_cosmetic_repair.py:268:        ci_cosmetic_repair._run([\"git\", \"status\"], cwd=Path(\"/tmp\"))\ntests/workflows/test_ci_cosmetic_repair.py:280:    result = ci_cosmetic_repair._run([\"git\", \"status\"], cwd=Path(\"/tmp\"))\ntests/workflows/test_ci_cosmetic_repair.py:393:    def fake_run(cmd, *, cwd, text, capture_output, check):\ntests/workflows/test_ci_cosmetic_repair.py:410:    def fake_run(cmd, *, cwd=None):\ntests/workflows/test_ci_cosmetic_repair.py:438:    def fake_run(cmd, *, cwd=None):\ntests/workflows/test_ci_cosmetic_repair.py:481:    def fake_run(cmd, *, cwd=None):\ntests/test_multi_period_engine_threshold_events_extended.py:368:    results = mp_engine.run(cfg, df=df)\ntests/test_multi_period_engine_threshold_events_extended.py:471:    results = mp_engine.run(cfg, df=df)\ntests/workflows/test_autofix_full_pipeline.py:14:def _run(\ntests/workflows/test_autofix_full_pipeline.py:20:    result = subprocess.run(cmd, cwd=cwd, capture_output=True, text=True)\ntests/workflows/test_autofix_full_pipeline.py:69:    initial_ruff = subprocess.run(\ntests/workflows/test_autofix_full_pipeline.py:78:    initial_mypy = subprocess.run(\ntests/workflows/test_autofix_full_pipeline.py:117:        _run(cmd, cwd=tmp_path, ok_exit_codes=ok_codes)\ntests/workflows/test_autofix_full_pipeline.py:143:        _run(cmd, cwd=tmp_path)\ntests/workflows/test_autofix_full_pipeline.py:145:    _run([sys.executable, \"-m\", \"ruff\", \"check\", str(sample)], cwd=tmp_path)\ntests/workflows/test_autofix_full_pipeline.py:146:    _run([sys.executable, \"-m\", \"black\", \"--check\", str(sample)], cwd=tmp_path)\ntests/workflows/test_autofix_full_pipeline.py:147:    _run(\ntests/test_gui_app_extended.py:326:    monkeypatch.setattr(app.pipeline, \"run_full\", lambda cfg: {\"extra\": 1})\ntests/test_pipeline_helpers_additional.py:18:    _empty_run_full_result,\ntests/test_pipeline_helpers_additional.py:94:    empty = _empty_run_full_result()\ntests/test_pipeline_helpers_additional.py:574:    def fake_single_period_run(*args, **kwargs):\ntests/test_pipeline_helpers_additional.py:684:    def fake_single_period_run(*args, **kwargs):\ntests/test_pipeline_helpers_additional.py:803:        result = pipeline.run(cfg)\ntests/test_pipeline_helpers_additional.py:811:def test_run_full_passes_through_results(monkeypatch: pytest.MonkeyPatch) -> None:\ntests/test_pipeline_helpers_additional.py:863:        result = pipeline.run_full(cfg)\ntests/test_pipeline_helpers_additional.py:878:    out = single_period_run(df, \"2020-01\", \"2020-03\")\ntests/test_pipeline_helpers_additional.py:886:        single_period_run(df, \"2020-01\", \"2020-02\")\ntests/test_pipeline_helpers_additional.py:897:    out = single_period_run(df, \"2020-01\", \"2020-03\")\ntests/test_pipeline_helpers_additional.py:913:        single_period_run(df, \"2020-01\", \"2020-02\", stats_cfg=EmptyConfig())\ntests/test_pipeline_helpers_additional.py:925:        single_period_run(df, \"2021-01\", \"2021-02\")\ntests/test_pipeline_helpers_additional.py:938:        single_period_run(df, \"2020-01\", \"2020-03\")\ntests/test_pipeline_helpers_additional.py:1736:        result = pipeline.run(cfg)\ntests/test_pipeline_helpers_additional.py:1789:        pipeline.run(cfg)\ntests/test_pipeline_helpers_additional.py:1795:def test_run_full_requires_csv_path() -> None:\ntests/test_pipeline_helpers_additional.py:1798:        pipeline.run_full(cfg)\ntests/test_pipeline_helpers_additional.py:1801:def test_run_full_uses_nan_policy_defaults(monkeypatch: pytest.MonkeyPatch) -> None:\ntests/test_pipeline_helpers_additional.py:1844:        result = pipeline.run_full(cfg)\ntests/test_pipeline_helpers_additional.py:1851:def test_run_full_respects_explicit_policy(monkeypatch: pytest.MonkeyPatch) -> None:\ntests/test_pipeline_helpers_additional.py:1893:        pipeline.run_full(cfg)\ntests/test_multi_period_missing_policy_observability.py:77:    results = mp_engine.run(cfg, df=df)\ntests/test_multi_period_missing_policy_observability.py:110:    results = mp_engine.run(cfg, df=df)\ntests/test_trend_model_app.py:25:    def fake_run(command, check):\ntests/test_trend_model_app.py:53:    def fake_run(command, check):\ntests/test_pipeline.py:67:    out = pipeline.run(cfg)\ntests/test_pipeline.py:90:    out = pipeline.run(cfg)\ntests/test_pipeline.py:99:        pipeline.run(cfg)\ntests/test_pipeline.py:151:    result = pipeline.run(cfg)\ntests/test_pipeline.py:171:    result = pipeline.run(cfg)\ntests/test_pipeline.py:180:        pipeline.run(cfg)\ntests/test_pipeline.py:283:        pipeline.run(cfg)\ntests/test_pipeline.py:302:def test_run_full_includes_risk_diagnostics(tmp_path):\ntests/test_pipeline.py:304:    res = pipeline.run_full(cfg)\ntests/test_pipeline.py:314:def test_run_full_robustness_settings_affect_weights(tmp_path):\ntests/test_pipeline.py:327:    res_rp = pipeline.run_full(cfg)\ntests/test_pipeline.py:334:    res_diag = pipeline.run_full(cfg)\ntests/test_pipeline.py:345:def test_run_full_robustness_condition_threshold_uses_cov_condition(tmp_path):\ntests/test_pipeline.py:378:    assert not isinstance(preprocess, pipeline.PipelineResult)\ntests/test_pipeline.py:386:    assert not isinstance(window, pipeline.PipelineResult)\ntests/test_pipeline.py:391:    res = pipeline.run_full(cfg)\ntests/workflows/test_autofix_pipeline_diverse.py:19:def _run(\ntests/workflows/test_autofix_pipeline_diverse.py:25:    result = subprocess.run(cmd, cwd=cwd, capture_output=True, text=True)\ntests/workflows/test_autofix_pipeline_diverse.py:158:        _run(command, cwd=repo_root, ok_exit_codes=ok_codes)\ntests/workflows/test_autofix_pipeline_diverse.py:232:    _run([sys.executable, \"-m\", \"isort\", str(sample_module)], cwd=repo_root)\ntests/workflows/test_autofix_pipeline_diverse.py:233:    _run([sys.executable, \"-m\", \"black\", str(repo_root)], cwd=repo_root)\ntests/workflows/test_autofix_pipeline_diverse.py:234:    _run(\ntests/workflows/test_autofix_pipeline_diverse.py:247:    _run([sys.executable, \"-m\", \"ruff\", \"check\", str(repo_root)], cwd=repo_root)\ntests/workflows/test_autofix_pipeline_diverse.py:248:    _run([sys.executable, \"-m\", \"black\", \"--check\", str(repo_root)], cwd=repo_root)\ntests/workflows/test_autofix_pipeline_diverse.py:249:    _run(\ntests/workflows/test_autofix_pipeline_live_docs.py:19:def _run(\ntests/workflows/test_autofix_pipeline_live_docs.py:25:    result = subprocess.run(cmd, cwd=cwd, capture_output=True, text=True)\ntests/workflows/test_autofix_pipeline_live_docs.py:242:        _run(command, cwd=repo_root, ok_exit_codes=ok_codes)\ntests/workflows/test_autofix_pipeline_live_docs.py:254:        _run(command, cwd=repo_root, ok_exit_codes=ok_codes)\ntests/workflows/test_autofix_pipeline_live_docs.py:263:    _run([sys.executable, \"-m\", \"ruff\", \"check\", *relative_targets], cwd=repo_root)\ntests/workflows/test_autofix_pipeline_live_docs.py:264:    _run([sys.executable, \"-m\", \"black\", \"--check\", *relative_targets], cwd=repo_root)\ntests/workflows/test_autofix_pipeline_live_docs.py:265:    _run(\ntests/workflows/test_keepalive_post_work.py:23:    result = subprocess.run(\ntests/test_single_period_run_avg_corr.py:28:    baseline_frame = single_period_run(\ntests/test_single_period_run_avg_corr.py:40:    score_frame = single_period_run(\ntests/app/test_demo_runner_component.py:439:    def rerun(self) -> None:\ntests/test_trend_analysis_diagnostics.py:3:from trend.diagnostics import DiagnosticPayload, DiagnosticResult\ntests/test_trend_analysis_diagnostics.py:6:    PipelineResult,\ntests/test_trend_analysis_diagnostics.py:25:    assert isinstance(result, PipelineResult)\ntests/test_trend_analysis_diagnostics.py:65:            DiagnosticResult(\ntests/app/test_results_page.py:209:    def fake_run(\ntests/app/test_results_page.py:291:    def fake_run(\ntests/app/test_data_page.py:154:    def rerun(self) -> None:\ntests/test_gui_support.py:71:    asyncio.run(runner())\ntests/test_gui_support.py:90:    asyncio.run(runner())\ntests/test_test_dependencies.py:115:        result = subprocess.run(\ntests/test_test_dependencies.py:206:        result = subprocess.run(\ntests/test_cli_api_golden_master.py:85:    subprocess.run(\ntests/test_buy_and_hold_selection.py:176:        results = multi_period_run(cfg, df=df)\ntests/test_buy_and_hold_selection.py:216:        results = multi_period_run(cfg, df=df)\ntests/test_buy_and_hold_selection.py:238:        results = multi_period_run(cfg, df=df)\ntests/test_buy_and_hold_selection.py:257:        results = multi_period_run(cfg, df=df)\ntests/test_buy_and_hold_selection.py:273:        results = multi_period_run(cfg_with_policy, df=df)\ntests/test_buy_and_hold_selection.py:306:        results = multi_period_run(cfg, df=df)\ntests/test_buy_and_hold_selection.py:322:        results = multi_period_run(cfg, df=df)\ntests/test_proxy_server_additional.py:159:    result = asyncio.run(proxy._handle_http_request(DummyRequest(), \"status\"))\ntests/test_proxy_server_additional.py:171:    chunks = asyncio.run(collect())\ntests/test_proxy_server_additional.py:180:    asyncio.run(proxy.close())\ntests/test_proxy_server_additional.py:187:    asyncio.run(proxy.start(host=\"127.0.0.1\", port=8600))\ntests/test_proxy_server_additional.py:212:    def fake_run(coro: Any) -> Any:\ntests/test_proxy_server_additional.py:259:    asyncio.run(proxy._websocket_entry(websocket, \"foo/bar\"))\ntests/test_proxy_server_additional.py:269:        asyncio.run(\ntests/test_proxy_server_additional.py:287:    result = asyncio.run(proxy._http_entry(object(), \"status\"))\ntests/test_proxy_server_additional.py:298:        asyncio.run(proxy.start())\ntests/test_proxy_server_additional.py:334:    asyncio.run(proxy._handle_websocket(websocket, \"ws/path\"))\ntests/test_proxy_server_additional.py:351:    result = asyncio.run(proxy._handle_http_request(DummyRequest(), \"/metrics\"))\ntests/test_trend_analysis_cli_main.py:209:    def fake_run(args):\ntests/test_trend_analysis_cli_main.py:589:    monkeypatch.setattr(cli.pipeline, \"run_full\", lambda cfg: {\"portfolio\": [1, 2, 3]})\ntests/test_run_full.py:5:from trend_analysis.diagnostics import PipelineResult\ntests/test_run_full.py:6:from trend_analysis.pipeline import run, run_full\ntests/test_run_full.py:41:def test_run_full_matches_run(tmp_path):\ntests/test_run_full.py:43:    detailed = run_full(cfg)\ntests/test_run_full.py:44:    summary = run(cfg)\ntests/test_run_full.py:45:    assert isinstance(detailed, PipelineResult)\ntests/test_run_full.py:51:def test_run_full_missing_csv_key(tmp_path):\ntests/test_run_full.py:64:        run_full(cfg)\ntests/test_run_full.py:67:def test_run_full_missing_file(tmp_path):\ntests/test_run_full.py:80:        run_full(cfg)\ntests/test_run_full.py:83:def test_run_full_with_benchmarks(tmp_path):\ntests/test_run_full.py:88:    res = run_full(cfg)\ntests/test_run_full.py:94:def test_run_full_respects_metric_registry(tmp_path):\ntests/test_run_full.py:98:    res = run_full(cfg)\ntests/test_pipeline_helpers.py:177:    # _prepare_preprocess_stage returns either a _PreprocessStage or a PipelineResult\ntests/test_pipeline_constraints_integration.py:115:def test_run_full_max_weight_with_vol_adjust_enabled(tmp_path):\ntests/test_pipeline_constraints_integration.py:144:    result = pipeline.run_full(cfg).unwrap()\ntests/test_multi_period_engine_cov_cache.py:61:        engine.run(cfg, price_frames=[])  # type: ignore[arg-type]\ntests/test_multi_period_engine_cov_cache.py:65:        engine.run(cfg, price_frames={\"2020-01\": bad})\ntests/test_multi_period_engine_cov_cache.py:68:        engine.run(cfg, price_frames={})\ntests/test_multi_period_engine_cov_cache.py:101:    results = engine.run(cfg, price_frames={\"a\": frame_one, \"b\": frame_two})\ntests/test_multi_period_engine_cov_cache.py:127:    results = engine.run(cfg, df=df)\ntests/test_trend_model_cli.py:76:        cli.run([\"--config\", str(cfg_path), \"--formats\", \"csv\"])\ntests/test_trend_model_cli.py:144:    exit_code = cli.run(\ntests/test_trend_model_cli.py:204:    exit_code = cli.run(\ntests/test_trend_model_cli.py:252:    exit_code = cli.run([\"--config\", str(cfg_path), \"--pdf\"])\ntests/test_trend_model_cli.py:268:    exit_code = cli.run([\"--config\", \"foo.yml\"])\ntests/test_trend_model_cli.py:283:    exit_code = cli.run([\"--config\", \"missing.yml\"])\ntests/test_multi_period_engine_threshold_bounds.py:239:    results = mp_engine.run(cfg, df=df)\ntests/test_multi_period_engine_threshold_bounds.py:368:    results = mp_engine.run(cfg, df=df)\ntests/test_api_server.py:68:    async def _run() -> None:\ntests/test_api_server.py:73:    asyncio.run(_run())\ntests/test_api_server.py:87:    host, port = api_server.run(host=\"0.0.0.0\", port=1234)\ntests/test_api_server.py:109:    def fake_run(host: str, port: int) -> None:\ntests/test_multi_period_engine.py:166:        engine.run(cfg, price_frames=[])  # type: ignore[arg-type]\ntests/test_multi_period_engine.py:169:        engine.run(cfg, price_frames={\"2024-01\": pd.Series([1, 2, 3])})\ntests/test_multi_period_engine.py:173:        engine.run(cfg, price_frames={\"2024-01\": frame})\ntests/test_multi_period_engine.py:176:        engine.run(cfg, price_frames={})\ntests/test_multi_period_engine.py:184:        engine.run(cfg)\ntests/test_multi_period_engine.py:215:    results = engine.run(cfg, price_frames=price_frames)\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_8","type":"reasoning","text":"**Implementing runpayload function**"}}
{"type":"item.started","item":{"id":"item_9","type":"command_execution","command":"/bin/bash -lc \"sed -n '1,220p' src/trend_analysis/diagnostics.py\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_9","type":"command_execution","command":"/bin/bash -lc \"sed -n '1,220p' src/trend_analysis/diagnostics.py\"","aggregated_output":"\"\"\"Diagnostics helpers for the trend-analysis pipeline.\"\"\"\n\nfrom __future__ import annotations\n\nfrom collections.abc import ItemsView, Iterator, KeysView, Mapping, ValuesView\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import Tuple, cast\n\nfrom trend.diagnostics import DiagnosticPayload, DiagnosticResult\n\nAnalysisResult = dict[str, object]\n\n\n@dataclass(slots=True)\nclass PipelineResult(Mapping[str, object]):\n    \"\"\"Dictionary-like container pairing a payload with diagnostics.\"\"\"\n\n    value: AnalysisResult | None\n    diagnostic: DiagnosticPayload | None = None\n\n    def _require_value(self) -> AnalysisResult:\n        if self.value is None:\n            raise KeyError(\"Pipeline result does not contain a payload\")\n        return self.value\n\n    def __getitem__(self, key: str) -> object:\n        return self._require_value()[key]\n\n    def __iter__(self) -> Iterator[str]:\n        data = self.value or {}\n        return iter(data)\n\n    def __len__(self) -> int:\n        data = self.value or {}\n        return len(data)\n\n    def __bool__(self) -> bool:  # pragma: no cover - trivial truthiness\n        return bool(self.value)\n\n    def get(self, key: str, default: object | None = None) -> object | None:\n        if self.value is None:\n            return default\n        return self.value.get(key, default)\n\n    def keys(self) -> KeysView[str]:\n        data = self.value or {}\n        return data.keys()\n\n    def items(self) -> ItemsView[str, object]:\n        data = self.value or {}\n        return data.items()\n\n    def values(self) -> ValuesView[object]:\n        data = self.value or {}\n        return data.values()\n\n    def copy(self) -> AnalysisResult:\n        data = self.value or {}\n        return dict(data)\n\n    def unwrap(self) -> AnalysisResult | None:\n        \"\"\"Return the underlying analysis payload without copying.\"\"\"\n\n        return self.value\n\n\nclass PipelineReasonCode(str, Enum):\n    \"\"\"Canonical reason codes for pipeline early exits.\"\"\"\n\n    INPUT_NONE = \"PIPELINE_INPUT_NONE\"\n    NO_VALID_DATES = \"PIPELINE_NO_VALID_DATES\"\n    CALENDAR_ALIGNMENT_WIPE = \"PIPELINE_CALENDAR_ALIGNMENT_WIPE\"\n    PREPARED_FRAME_EMPTY = \"PIPELINE_PREPARED_FRAME_EMPTY\"\n    NO_VALUE_COLUMNS = \"PIPELINE_NO_VALUE_COLUMNS\"\n    INSUFFICIENT_COLUMNS = \"PIPELINE_INSUFFICIENT_COLUMNS\"\n    SAMPLE_WINDOW_EMPTY = \"PIPELINE_SAMPLE_WINDOW_EMPTY\"\n    NO_FUNDS_SELECTED = \"PIPELINE_NO_FUNDS_SELECTED\"\n    INDICES_ABSENT = \"PIPELINE_INDICES_ABSENT\"\n\n\n_DEFAULT_MESSAGES: Mapping[PipelineReasonCode, str] = {\n    PipelineReasonCode.INPUT_NONE: \"Input DataFrame must be provided.\",\n    PipelineReasonCode.NO_VALID_DATES: \"No valid timestamps were found in the Date column.\",\n    PipelineReasonCode.CALENDAR_ALIGNMENT_WIPE: \"Calendar alignment removed every observation.\",\n    PipelineReasonCode.PREPARED_FRAME_EMPTY: \"All rows were dropped during preprocessing.\",\n    PipelineReasonCode.NO_VALUE_COLUMNS: \"No fund or index columns remain after preprocessing.\",\n    PipelineReasonCode.INSUFFICIENT_COLUMNS: \"Insufficient data columns to continue the analysis.\",\n    PipelineReasonCode.SAMPLE_WINDOW_EMPTY: \"In-sample or out-of-sample window is empty.\",\n    PipelineReasonCode.NO_FUNDS_SELECTED: \"No investable funds satisfy the selection filters.\",\n    PipelineReasonCode.INDICES_ABSENT: \"Requested indices are missing from the analysis window.\",\n}\n\n\ndef pipeline_success(value: AnalysisResult) -> PipelineResult:\n    \"\"\"Return a successful pipeline diagnostic wrapper.\"\"\"\n\n    return PipelineResult(value=value, diagnostic=None)\n\n\ndef pipeline_failure(\n    code: PipelineReasonCode,\n    *,\n    message: str | None = None,\n    context: Mapping[str, object] | None = None,\n) -> PipelineResult:\n    \"\"\"Create a failure diagnostic populated with pipeline metadata.\"\"\"\n\n    default_message = _DEFAULT_MESSAGES.get(code)\n    # Fallback order: (1) custom message, (2) default message, (3) enum value itself\n    if message is not None:\n        payload_message = message\n    elif default_message is not None:\n        payload_message = default_message\n    else:\n        payload_message = code.value\n    payload = DiagnosticPayload(\n        reason_code=code.value,\n        message=payload_message,\n        context=dict(context) if context else None,\n    )\n    return PipelineResult(value=None, diagnostic=payload)\n\n\ndef coerce_pipeline_result(\n    result: object,\n) -> Tuple[AnalysisResult | None, DiagnosticPayload | None]:\n    \"\"\"Return a ``(payload, diagnostic)`` pair for arbitrary pipeline outputs.\"\"\"\n\n    if isinstance(result, PipelineResult):\n        return result.value, result.diagnostic\n\n    diagnostic_attr = getattr(result, \"diagnostic\", None)\n    if diagnostic_attr is not None and not isinstance(\n        diagnostic_attr, DiagnosticPayload\n    ):\n        raise TypeError(\n            \"Pipeline diagnostics must be DiagnosticPayload instances; received \"\n            f\"{type(diagnostic_attr)!r}\"\n        )\n    diagnostic: DiagnosticPayload | None = diagnostic_attr\n\n    if isinstance(result, DiagnosticResult):\n        payload = cast(AnalysisResult | None, result.value)\n    elif isinstance(result, Mapping):\n        payload = cast(AnalysisResult | None, result)\n    elif hasattr(result, \"value\"):\n        payload = cast(AnalysisResult | None, getattr(result, \"value\"))\n    else:\n        payload = cast(AnalysisResult | None, result)\n\n    if payload is None:\n        return None, diagnostic\n\n    if not isinstance(payload, Mapping):\n        raise TypeError(\n            f\"Pipeline outputs must be mapping-like; received {type(payload)!r}\"\n        )\n\n    if isinstance(payload, dict):\n        return payload, diagnostic\n    return dict(payload), diagnostic\n\n\n__all__ = [\n    \"AnalysisResult\",\n    \"PipelineResult\",\n    \"PipelineReasonCode\",\n    \"pipeline_failure\",\n    \"pipeline_success\",\n    \"coerce_pipeline_result\",\n]\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_10","type":"reasoning","text":"**Designing RunPayload structure**"}}
{"type":"item.started","item":{"id":"item_11","type":"command_execution","command":"/bin/bash -lc \"sed -n '1,200p' src/trend/diagnostics.py\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_11","type":"command_execution","command":"/bin/bash -lc \"sed -n '1,200p' src/trend/diagnostics.py\"","aggregated_output":"\"\"\"Lightweight diagnostic payloads for early-exit signalling.\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nfrom dataclasses import dataclass\nfrom typing import Generic, Mapping, TypeVar\n\ntry:  # pragma: no cover - optional instrumentation\n    from trend_analysis.config.coverage import (\n        ConfigCoverageTracker,\n        get_config_coverage_tracker,\n    )\nexcept Exception:  # pragma: no cover - defensive fallback\n\n    def get_config_coverage_tracker() -> ConfigCoverageTracker | None:\n        return None\n\n    ConfigCoverageTracker = None  # type: ignore[assignment,misc]\n\n\nT = TypeVar(\"T\")\n\n\n@dataclass(slots=True)\nclass DiagnosticPayload:\n    \"\"\"Structured context for early exits and skipped work.\"\"\"\n\n    reason_code: str\n    message: str\n    context: Mapping[str, object] | None = None\n\n\n@dataclass(slots=True)\nclass DiagnosticResult(Generic[T]):\n    \"\"\"Container pairing a value with an optional diagnostic.\"\"\"\n\n    value: T | None\n    diagnostic: DiagnosticPayload | None = None\n    coverage_report: object | None = None\n\n    def __post_init__(self) -> None:\n        if self.coverage_report is not None:\n            return\n        if not logging.getLogger().isEnabledFor(logging.DEBUG):\n            return\n        tracker = get_config_coverage_tracker()\n        if tracker is None:\n            return\n        try:\n            self.coverage_report = tracker.generate_report()\n        except Exception:\n            return\n\n    @classmethod\n    def success(cls, value: T) -> \"DiagnosticResult[T]\":\n        return cls(value=value, diagnostic=None)\n\n    @classmethod\n    def failure(\n        cls,\n        *,\n        reason_code: str,\n        message: str,\n        context: Mapping[str, object] | None = None,\n    ) -> \"DiagnosticResult[T]\":\n        return cls(\n            value=None, diagnostic=DiagnosticPayload(reason_code, message, context)\n        )\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_12","type":"reasoning","text":"**Locating RunPayload and considering module placement**"}}
{"type":"item.started","item":{"id":"item_13","type":"command_execution","command":"/bin/bash -lc \"sed -n '1,240p' src/trend_analysis/pipeline.py\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_13","type":"command_execution","command":"/bin/bash -lc \"sed -n '1,240p' src/trend_analysis/pipeline.py\"","aggregated_output":"from __future__ import annotations\n\nimport logging\nfrom typing import Any, Mapping\n\nimport pandas as pd\n\nfrom trend.diagnostics import DiagnosticResult\n\nfrom . import pipeline_helpers\nfrom .core.rank_selection import (\n    RiskStatsConfig,\n    get_window_metric_bundle,\n    make_window_key,\n    rank_select_funds,\n)\nfrom .data import identify_risk_free_fund, load_csv\nfrom .diagnostics import PipelineReasonCode, PipelineResult\nfrom .metrics import (\n    annual_return,\n    information_ratio,\n    max_drawdown,\n    sharpe_ratio,\n    sortino_ratio,\n    volatility,\n)\nfrom .perf.rolling_cache import compute_dataset_hash, get_cache\nfrom .pipeline_entrypoints import ConfigBindings, run_from_config, run_full_from_config\nfrom .pipeline_helpers import (\n    _attach_calendar_settings,\n    _build_trend_spec,\n    _cfg_section,\n    _cfg_value,\n    _derive_split_from_periods,\n    _empty_run_full_result,\n    _policy_from_config,\n    _resolve_target_vol,\n    _section_get,\n    _unwrap_cfg,\n)\nfrom .pipeline_helpers import (\n    _resolve_sample_split as _resolve_sample_split_impl,\n)\nfrom .pipeline_helpers import (\n    compute_signal as _compute_signal_impl,\n)\nfrom .pipeline_helpers import (\n    position_from_signal as _position_from_signal_impl,\n)\nfrom .pipeline_runner import (\n    _run_analysis as _run_analysis_impl,\n)\nfrom .pipeline_runner import (\n    _run_analysis_with_diagnostics as _run_analysis_with_diagnostics_impl,\n)\nfrom .portfolio import apply_weight_policy\nfrom .regimes import build_regime_payload\nfrom .risk import compute_constrained_weights, realised_volatility\nfrom .signals import TrendSpec, compute_trend_signals\nfrom .stages import portfolio as portfolio_stage\nfrom .stages import preprocessing as preprocessing_stage\nfrom .stages import selection as selection_stage\nfrom .stages.portfolio import (\n    _assemble_analysis_output as _assemble_analysis_output_impl,\n)\nfrom .stages.portfolio import (\n    _compute_stats,\n    _Stats,\n    calc_portfolio_returns,\n)\nfrom .stages.portfolio import (\n    _compute_weights_and_stats as _compute_weights_and_stats_impl,\n)\nfrom .stages.preprocessing import (\n    _build_sample_windows as _build_sample_windows_impl,\n)\nfrom .stages.preprocessing import (\n    _frequency_label,\n    _preprocessing_summary,\n    _WindowStage,\n)\nfrom .stages.preprocessing import (\n    _prepare_input_data as _prepare_input_data_impl,\n)\nfrom .stages.preprocessing import (\n    _prepare_preprocess_stage as _prepare_preprocess_stage_impl,\n)\nfrom .stages.selection import (\n    _resolve_risk_free_column,\n    single_period_run,\n)\nfrom .stages.selection import (\n    _select_universe as _select_universe_impl,\n)\nfrom .time_utils import align_calendar\nfrom .util.frequency import FrequencySummary, detect_frequency\nfrom .util.missing import MissingPolicyResult, apply_missing_policy\nfrom .weights.robust_config import weight_engine_params_from_robustness\n\nlogger = logging.getLogger(__name__)\n\n\ndef _sync_stage_dependencies() -> None:\n    \"\"\"Synchronize stage module globals with pipeline-level bindings.\n\n    This ensures monkeypatching pipeline functions affects stage execution.\n    \"\"\"\n    # These assignments are for runtime patching; mypy may or may not see the\n    # attributes depending on module resolution. Suppress with type: ignore[attr-defined].\n    setattr(preprocessing_stage, \"detect_frequency\", detect_frequency)\n    setattr(preprocessing_stage, \"apply_missing_policy\", apply_missing_policy)\n    setattr(preprocessing_stage, \"align_calendar\", align_calendar)\n    setattr(preprocessing_stage, \"_prepare_input_data\", _prepare_input_data)\n\n    setattr(selection_stage, \"rank_select_funds\", rank_select_funds)\n    setattr(selection_stage, \"get_window_metric_bundle\", get_window_metric_bundle)\n    setattr(selection_stage, \"make_window_key\", make_window_key)\n    setattr(selection_stage, \"single_period_run\", single_period_run)\n    setattr(selection_stage, \"_resolve_risk_free_column\", _resolve_risk_free_column)\n    setattr(selection_stage, \"identify_risk_free_fund\", identify_risk_free_fund)\n\n    setattr(portfolio_stage, \"compute_trend_signals\", compute_trend_signals)\n    setattr(portfolio_stage, \"compute_constrained_weights\", compute_constrained_weights)\n    setattr(portfolio_stage, \"realised_volatility\", realised_volatility)\n    setattr(portfolio_stage, \"apply_weight_policy\", apply_weight_policy)\n    setattr(portfolio_stage, \"information_ratio\", information_ratio)\n    setattr(portfolio_stage, \"annual_return\", annual_return)\n    setattr(portfolio_stage, \"volatility\", volatility)\n    setattr(portfolio_stage, \"sharpe_ratio\", sharpe_ratio)\n    setattr(portfolio_stage, \"sortino_ratio\", sortino_ratio)\n    setattr(portfolio_stage, \"max_drawdown\", max_drawdown)\n    setattr(portfolio_stage, \"build_regime_payload\", build_regime_payload)\n    setattr(portfolio_stage, \"avg_corr_handler\", _avg_corr_handler)\n    setattr(portfolio_stage, \"calc_portfolio_returns\", calc_portfolio_returns)\n\n\ndef _call_with_sync(func: Any, *args: Any, **kwargs: Any) -> Any:\n    _sync_stage_dependencies()\n    return func(*args, **kwargs)\n\n\ndef _avg_corr_handler(\n    in_scaled: pd.DataFrame,\n    out_scaled: pd.DataFrame,\n    fund_cols: list[str],\n) -> tuple[dict[str, float] | None, dict[str, float] | None]:\n    corr_in = in_scaled[fund_cols].corr()\n    corr_out = out_scaled[fund_cols].corr()\n    n_f = len(fund_cols)\n    is_avg_corr: dict[str, float] = {}\n    os_avg_corr: dict[str, float] = {}\n    denominator = float(n_f - 1) if n_f > 1 else 1.0\n    for f in fund_cols:\n        in_sum = float(corr_in.loc[f].sum())\n        out_sum = float(corr_out.loc[f].sum())\n        in_val = (in_sum - 1.0) / denominator\n        out_val = (out_sum - 1.0) / denominator\n        is_avg_corr[f] = float(in_val)\n        os_avg_corr[f] = float(out_val)\n    return is_avg_corr, os_avg_corr\n\n\ndef _prepare_input_data(*args: Any, **kwargs: Any) -> Any:\n    return _call_with_sync(_prepare_input_data_impl, *args, **kwargs)\n\n\ndef _prepare_preprocess_stage(*args: Any, **kwargs: Any) -> Any:\n    return _call_with_sync(_prepare_preprocess_stage_impl, *args, **kwargs)\n\n\ndef _build_sample_windows(*args: Any, **kwargs: Any) -> Any:\n    return _call_with_sync(_build_sample_windows_impl, *args, **kwargs)\n\n\ndef _select_universe(*args: Any, **kwargs: Any) -> Any:\n    return _call_with_sync(_select_universe_impl, *args, **kwargs)\n\n\ndef _compute_weights_and_stats(*args: Any, **kwargs: Any) -> Any:\n    return _call_with_sync(_compute_weights_and_stats_impl, *args, **kwargs)\n\n\ndef _assemble_analysis_output(*args: Any, **kwargs: Any) -> Any:\n    return _call_with_sync(_assemble_analysis_output_impl, *args, **kwargs)\n\n\ndef _run_analysis_with_diagnostics(*args: Any, **kwargs: Any) -> PipelineResult:\n    result = _call_with_sync(_run_analysis_with_diagnostics_impl, *args, **kwargs)\n    return result  # type: ignore[no-any-return]\n\n\ndef _run_analysis(*args: Any, **kwargs: Any) -> Any:\n    \"\"\"Backward-compatible wrapper returning raw payloads for tests.\"\"\"\n    return _call_with_sync(_run_analysis_impl, *args, **kwargs)\n\n\ndef _resolve_sample_split(*args: Any, **kwargs: Any) -> Any:\n    pipeline_helpers._derive_split_from_periods = _derive_split_from_periods\n    return _resolve_sample_split_impl(*args, **kwargs)\n\n\n_DEFAULT_RUN_ANALYSIS = _run_analysis\n\n\ndef _invoke_analysis_with_diag(*args: Any, **kwargs: Any) -> PipelineResult:\n    \"\"\"Call the patched analysis hook and normalise into a PipelineResult.\"\"\"\n\n    if _run_analysis is _DEFAULT_RUN_ANALYSIS:\n        return _run_analysis_with_diagnostics(*args, **kwargs)\n    patched_result = _run_analysis(*args, **kwargs)\n    if isinstance(patched_result, PipelineResult):\n        return patched_result\n    if isinstance(patched_result, DiagnosticResult):\n        return PipelineResult(\n            value=patched_result.value,\n            diagnostic=patched_result.diagnostic,\n        )\n    return PipelineResult(value=patched_result, diagnostic=None)\n\n\ndef run_analysis(\n    df: pd.DataFrame,\n    in_start: str,\n    in_end: str,\n    out_start: str,\n    out_end: str,\n    target_vol: float | None,\n    monthly_cost: float,\n    *,\n    floor_vol: float | None = None,\n    warmup_periods: int = 0,\n    selection_mode: str = \"all\",\n    random_n: int = 8,\n    custom_weights: dict[str, float] | None = None,\n    rank_kwargs: Mapping[str, Any] | None = None,\n    manual_funds: list[str] | None = None,\n    indices_list: list[str] | None = None,\n    benchmarks: dict[str, str] | None = None,\n    seed: int = 42,\n    stats_cfg: RiskStatsConfig | None = None,\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_14","type":"reasoning","text":"**Considering placement for RunPayload**"}}
{"type":"item.started","item":{"id":"item_15","type":"command_execution","command":"/bin/bash -lc \"sed -n '1,220p' src/trend_analysis/pipeline_entrypoints.py\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_15","type":"command_execution","command":"/bin/bash -lc \"sed -n '1,220p' src/trend_analysis/pipeline_entrypoints.py\"","aggregated_output":"from __future__ import annotations\n\nimport logging\nfrom dataclasses import dataclass\nfrom typing import TYPE_CHECKING, Any, Mapping, cast\n\nimport pandas as pd\n\nif TYPE_CHECKING:\n    from .diagnostics import PipelineResult\n\nlogger = logging.getLogger(\"trend_analysis.pipeline\")\n\n\n@dataclass(frozen=True, slots=True)\nclass ConfigBindings:\n    load_csv: Any\n    attach_calendar_settings: Any\n    unwrap_cfg: Any\n    cfg_section: Any\n    section_get: Any\n    cfg_value: Any\n    resolve_sample_split: Any\n    policy_from_config: Any\n    build_trend_spec: Any\n    resolve_target_vol: Any\n    invoke_analysis_with_diag: Any\n    weight_engine_params_from_robustness: Any\n    RiskStatsConfig: Any\n\n\ndef run_from_config(cfg: Any, *, bindings: ConfigBindings) -> pd.DataFrame:\n    \"\"\"Execute the analysis pipeline based on ``cfg``.\"\"\"\n    cfg = bindings.unwrap_cfg(cfg)\n    preprocessing_section = bindings.cfg_section(cfg, \"preprocessing\")\n    data_settings = bindings.cfg_section(cfg, \"data\")\n    csv_path = bindings.section_get(data_settings, \"csv_path\")\n    if csv_path is None:\n        raise KeyError(\"cfg.data['csv_path'] must be provided\")\n\n    missing_policy_cfg = bindings.section_get(data_settings, \"missing_policy\")\n    if missing_policy_cfg is None:\n        missing_policy_cfg = bindings.section_get(data_settings, \"nan_policy\")\n    missing_limit_cfg = bindings.section_get(data_settings, \"missing_limit\")\n    if missing_limit_cfg is None:\n        missing_limit_cfg = bindings.section_get(data_settings, \"nan_limit\")\n\n    df = bindings.load_csv(\n        csv_path,\n        errors=\"raise\",\n        missing_policy=missing_policy_cfg,\n        missing_limit=missing_limit_cfg,\n    )\n    df = cast(pd.DataFrame, df)\n\n    bindings.attach_calendar_settings(df, cfg)\n\n    split_cfg = bindings.cfg_section(cfg, \"sample_split\")\n    resolved_split = bindings.resolve_sample_split(df, split_cfg)\n    metrics_section = bindings.cfg_section(cfg, \"metrics\")\n    metrics_list = bindings.section_get(metrics_section, \"registry\")\n    stats_cfg = None\n    if metrics_list:\n        from .core.rank_selection import canonical_metric_list\n\n        stats_cfg = bindings.RiskStatsConfig(\n            metrics_to_run=canonical_metric_list(metrics_list),\n            risk_free=0.0,\n        )\n\n    missing_section = bindings.section_get(preprocessing_section, \"missing_data\")\n    if not isinstance(missing_section, Mapping):\n        missing_section = None\n    policy_spec, limit_spec = bindings.policy_from_config(\n        missing_section if isinstance(missing_section, Mapping) else None\n    )\n\n    vol_adjust = bindings.cfg_section(cfg, \"vol_adjust\")\n    run_settings = bindings.cfg_section(cfg, \"run\")\n    portfolio_cfg = bindings.cfg_section(cfg, \"portfolio\")\n    weighting_scheme = bindings.section_get(portfolio_cfg, \"weighting_scheme\", \"equal\")\n    robustness_cfg = bindings.section_get(portfolio_cfg, \"robustness\")\n    if not isinstance(robustness_cfg, Mapping):\n        robustness_cfg = bindings.cfg_section(cfg, \"robustness\")\n    weight_engine_params = bindings.weight_engine_params_from_robustness(\n        weighting_scheme, robustness_cfg\n    )\n    trend_spec = bindings.build_trend_spec(cfg, vol_adjust)\n    lambda_tc_val = bindings.section_get(portfolio_cfg, \"lambda_tc\", 0.0)\n    risk_free_column = bindings.section_get(data_settings, \"risk_free_column\")\n    allow_risk_free_fallback = bindings.section_get(\n        data_settings, \"allow_risk_free_fallback\"\n    )\n\n    diag_res = bindings.invoke_analysis_with_diag(\n        df,\n        resolved_split[\"in_start\"],\n        resolved_split[\"in_end\"],\n        resolved_split[\"out_start\"],\n        resolved_split[\"out_end\"],\n        bindings.resolve_target_vol(vol_adjust),\n        bindings.section_get(run_settings, \"monthly_cost\", 0.0),\n        floor_vol=bindings.section_get(vol_adjust, \"floor_vol\"),\n        warmup_periods=int(bindings.section_get(vol_adjust, \"warmup_periods\", 0) or 0),\n        selection_mode=bindings.section_get(portfolio_cfg, \"selection_mode\", \"all\"),\n        random_n=bindings.section_get(portfolio_cfg, \"random_n\", 8),\n        custom_weights=bindings.section_get(portfolio_cfg, \"custom_weights\"),\n        rank_kwargs=bindings.section_get(portfolio_cfg, \"rank\"),\n        manual_funds=bindings.section_get(portfolio_cfg, \"manual_list\"),\n        indices_list=bindings.section_get(portfolio_cfg, \"indices_list\"),\n        benchmarks=bindings.cfg_value(cfg, \"benchmarks\"),\n        seed=bindings.cfg_value(cfg, \"seed\", 42),\n        weighting_scheme=weighting_scheme,\n        constraints=bindings.section_get(portfolio_cfg, \"constraints\"),\n        stats_cfg=stats_cfg,\n        missing_policy=policy_spec,\n        missing_limit=limit_spec,\n        risk_window=bindings.section_get(vol_adjust, \"window\"),\n        previous_weights=bindings.section_get(portfolio_cfg, \"previous_weights\"),\n        lambda_tc=lambda_tc_val,\n        max_turnover=bindings.section_get(portfolio_cfg, \"max_turnover\"),\n        signal_spec=trend_spec,\n        regime_cfg=bindings.cfg_section(cfg, \"regime\"),\n        weight_policy=bindings.section_get(portfolio_cfg, \"weight_policy\"),\n        risk_free_column=risk_free_column,\n        allow_risk_free_fallback=allow_risk_free_fallback,\n        weight_engine_params=weight_engine_params,\n    )\n    diag = diag_res.diagnostic\n    if diag_res.value is None:\n        if diag:\n            logger.warning(\n                \"pipeline.run aborted (%s): %s\",\n                diag.reason_code,\n                diag.message,\n            )\n        empty = pd.DataFrame()\n        if diag:\n            empty.attrs[\"diagnostic\"] = diag\n        return empty\n\n    res = diag_res.value\n    stats = res[\"out_sample_stats\"]\n    df = pd.DataFrame({k: vars(v) for k, v in stats.items()}).T\n    for label, ir_map in res.get(\"benchmark_ir\", {}).items():\n        col = f\"ir_{label}\"\n        df[col] = pd.Series(\n            {\n                k: v\n                for k, v in ir_map.items()\n                if k not in {\"equal_weight\", \"user_weight\"}\n            }\n        )\n    if diag:\n        df.attrs[\"diagnostic\"] = diag\n    return df\n\n\ndef run_full_from_config(cfg: Any, *, bindings: ConfigBindings) -> PipelineResult:\n    \"\"\"Return the full analysis results (with diagnostics) based on ``cfg``.\"\"\"\n    cfg = bindings.unwrap_cfg(cfg)\n    preprocessing_section = bindings.cfg_section(cfg, \"preprocessing\")\n    data_settings = bindings.cfg_section(cfg, \"data\")\n    csv_path = bindings.section_get(data_settings, \"csv_path\")\n    if csv_path is None:\n        raise KeyError(\"cfg.data['csv_path'] must be provided\")\n\n    missing_policy_cfg = bindings.section_get(data_settings, \"missing_policy\")\n    if missing_policy_cfg is None:\n        missing_policy_cfg = bindings.section_get(data_settings, \"nan_policy\")\n    missing_limit_cfg = bindings.section_get(data_settings, \"missing_limit\")\n    if missing_limit_cfg is None:\n        missing_limit_cfg = bindings.section_get(data_settings, \"nan_limit\")\n\n    df = bindings.load_csv(\n        csv_path,\n        errors=\"raise\",\n        missing_policy=missing_policy_cfg,\n        missing_limit=missing_limit_cfg,\n    )\n    df = cast(pd.DataFrame, df)\n\n    bindings.attach_calendar_settings(df, cfg)\n\n    split_cfg = bindings.cfg_section(cfg, \"sample_split\")\n    resolved_split = bindings.resolve_sample_split(df, split_cfg)\n    metrics_section = bindings.cfg_section(cfg, \"metrics\")\n    metrics_list = bindings.section_get(metrics_section, \"registry\")\n    stats_cfg = None\n    if metrics_list:\n        from .core.rank_selection import canonical_metric_list\n\n        stats_cfg = bindings.RiskStatsConfig(\n            metrics_to_run=canonical_metric_list(metrics_list),\n            risk_free=0.0,\n        )\n\n    missing_section = bindings.section_get(preprocessing_section, \"missing_data\")\n    if not isinstance(missing_section, Mapping):\n        missing_section = None\n    policy_spec, limit_spec = bindings.policy_from_config(\n        missing_section if isinstance(missing_section, Mapping) else None\n    )\n\n    vol_adjust = bindings.cfg_section(cfg, \"vol_adjust\")\n    run_settings = bindings.cfg_section(cfg, \"run\")\n    portfolio_cfg = bindings.cfg_section(cfg, \"portfolio\")\n    weighting_scheme = bindings.section_get(portfolio_cfg, \"weighting_scheme\", \"equal\")\n    robustness_cfg = bindings.section_get(portfolio_cfg, \"robustness\")\n    if not isinstance(robustness_cfg, Mapping):\n        robustness_cfg = bindings.cfg_section(cfg, \"robustness\")\n    weight_engine_params = bindings.weight_engine_params_from_robustness(\n        weighting_scheme, robustness_cfg\n    )\n    risk_free_column = bindings.section_get(data_settings, \"risk_free_column\")\n    trend_spec = bindings.build_trend_spec(cfg, vol_adjust)\n    lambda_tc_val = bindings.section_get(portfolio_cfg, \"lambda_tc\", 0.0)\n    risk_free_column = bindings.section_get(data_settings, \"risk_free_column\")\n    allow_risk_free_fallback = bindings.section_get(\n        data_settings, \"allow_risk_free_fallback\"\n","exit_code":0,"status":"completed"}}
{"type":"item.started","item":{"id":"item_16","type":"command_execution","command":"/bin/bash -lc \"sed -n '220,520p' src/trend_analysis/pipeline_entrypoints.py\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_16","type":"command_execution","command":"/bin/bash -lc \"sed -n '220,520p' src/trend_analysis/pipeline_entrypoints.py\"","aggregated_output":"        data_settings, \"allow_risk_free_fallback\"\n    )\n\n    diag_res = bindings.invoke_analysis_with_diag(\n        df,\n        resolved_split[\"in_start\"],\n        resolved_split[\"in_end\"],\n        resolved_split[\"out_start\"],\n        resolved_split[\"out_end\"],\n        bindings.resolve_target_vol(vol_adjust),\n        bindings.section_get(run_settings, \"monthly_cost\", 0.0),\n        floor_vol=bindings.section_get(vol_adjust, \"floor_vol\"),\n        warmup_periods=int(bindings.section_get(vol_adjust, \"warmup_periods\", 0) or 0),\n        selection_mode=bindings.section_get(portfolio_cfg, \"selection_mode\", \"all\"),\n        random_n=bindings.section_get(portfolio_cfg, \"random_n\", 8),\n        custom_weights=bindings.section_get(portfolio_cfg, \"custom_weights\"),\n        rank_kwargs=bindings.section_get(portfolio_cfg, \"rank\"),\n        manual_funds=bindings.section_get(portfolio_cfg, \"manual_list\"),\n        indices_list=bindings.section_get(portfolio_cfg, \"indices_list\"),\n        benchmarks=bindings.cfg_value(cfg, \"benchmarks\"),\n        seed=bindings.cfg_value(cfg, \"seed\", 42),\n        weighting_scheme=weighting_scheme,\n        constraints=bindings.section_get(portfolio_cfg, \"constraints\"),\n        stats_cfg=stats_cfg,\n        missing_policy=policy_spec,\n        missing_limit=limit_spec,\n        risk_window=bindings.section_get(vol_adjust, \"window\"),\n        previous_weights=bindings.section_get(portfolio_cfg, \"previous_weights\"),\n        lambda_tc=lambda_tc_val,\n        max_turnover=bindings.section_get(portfolio_cfg, \"max_turnover\"),\n        signal_spec=trend_spec,\n        regime_cfg=bindings.cfg_section(cfg, \"regime\"),\n        weight_policy=bindings.section_get(portfolio_cfg, \"weight_policy\"),\n        risk_free_column=risk_free_column,\n        allow_risk_free_fallback=allow_risk_free_fallback,\n        weight_engine_params=weight_engine_params,\n    )\n    diag = diag_res.diagnostic\n    if diag_res.value is None:\n        if diag:\n            logger.warning(\n                \"pipeline.run_full aborted (%s): %s\",\n                diag.reason_code,\n                diag.message,\n            )\n        else:\n            logger.warning(\"pipeline.run_full aborted with no diagnostic context\")\n    return diag_res  # type: ignore[no-any-return]\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_17","type":"reasoning","text":"**Preparing RunPayload implementation**"}}
{"type":"item.started","item":{"id":"item_18","type":"command_execution","command":"/bin/bash -lc 'rg -n \"metadata\" src/trend_analysis src/trend streamlit_app tests'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_18","type":"command_execution","command":"/bin/bash -lc 'rg -n \"metadata\" src/trend_analysis src/trend streamlit_app tests'","aggregated_output":"src/trend/__init__.py:11:from importlib import metadata as _metadata\nsrc/trend/__init__.py:19:            return _metadata.version(\"trend-model\")\nsrc/trend/__init__.py:20:        except _metadata.PackageNotFoundError:  # pragma: no cover - dev mode\nsrc/trend/cli.py:557:    # Attach metadata expected by export_bundle\nsrc/trend_analysis/diagnostics.py:107:    \"\"\"Create a failure diagnostic populated with pipeline metadata.\"\"\"\nstreamlit_app/components/data_schema.py:30:    \"\"\"Lightweight metadata structure shared with the Streamlit app.\"\"\"\nstreamlit_app/components/data_schema.py:33:    metadata: Optional[MarketDataMetadata]\nstreamlit_app/components/data_schema.py:121:    metadata = validated.metadata\nstreamlit_app/components/data_schema.py:124:    rows = metadata.rows\nstreamlit_app/components/data_schema.py:135:    if metadata.frequency_missing_periods > 0:\nstreamlit_app/components/data_schema.py:138:            f\"{metadata.frequency_missing_periods} missing {metadata.frequency_label} periods \"\nstreamlit_app/components/data_schema.py:139:            f\"(max gap {metadata.frequency_max_gap_periods}).\"\nstreamlit_app/components/data_schema.py:141:    if metadata.missing_policy_dropped:\nstreamlit_app/components/data_schema.py:142:        dropped = \", \".join(sorted(metadata.missing_policy_dropped))\nstreamlit_app/components/data_schema.py:145:            f\"{dropped} (policy={metadata.missing_policy}).\"\nstreamlit_app/components/data_schema.py:147:    if metadata.missing_policy_summary and (\nstreamlit_app/components/data_schema.py:148:        metadata.frequency_missing_periods > 0\nstreamlit_app/components/data_schema.py:149:        or bool(metadata.missing_policy_filled)\nstreamlit_app/components/data_schema.py:150:        or bool(metadata.missing_policy_dropped)\nstreamlit_app/components/data_schema.py:153:            f\"Missing-data policy applied: {metadata.missing_policy_summary}.\"\nstreamlit_app/components/data_schema.py:172:    metadata = validated.metadata\nstreamlit_app/components/data_schema.py:174:    meta[\"metadata\"] = metadata\nstreamlit_app/components/data_schema.py:178:    meta[\"original_columns\"] = list(metadata.columns or metadata.symbols)\nstreamlit_app/components/data_schema.py:179:    meta[\"symbols\"] = list(metadata.symbols)\nstreamlit_app/components/data_schema.py:180:    meta[\"n_rows\"] = metadata.rows\nstreamlit_app/components/data_schema.py:181:    meta[\"mode\"] = metadata.mode.value\nstreamlit_app/components/data_schema.py:182:    meta[\"frequency\"] = metadata.frequency_label\nstreamlit_app/components/data_schema.py:183:    meta[\"frequency_code\"] = metadata.frequency\nstreamlit_app/components/data_schema.py:184:    meta[\"frequency_detected\"] = metadata.frequency_detected\nstreamlit_app/components/data_schema.py:185:    meta[\"frequency_missing_periods\"] = metadata.frequency_missing_periods\nstreamlit_app/components/data_schema.py:186:    meta[\"frequency_max_gap_periods\"] = metadata.frequency_max_gap_periods\nstreamlit_app/components/data_schema.py:187:    meta[\"frequency_tolerance_periods\"] = metadata.frequency_tolerance_periods\nstreamlit_app/components/data_schema.py:188:    meta[\"missing_policy\"] = metadata.missing_policy\nstreamlit_app/components/data_schema.py:189:    meta[\"missing_policy_limit\"] = metadata.missing_policy_limit\nstreamlit_app/components/data_schema.py:190:    meta[\"missing_policy_summary\"] = metadata.missing_policy_summary\nstreamlit_app/components/data_schema.py:191:    meta[\"date_range\"] = metadata.date_range\nstreamlit_app/components/data_schema.py:192:    meta[\"start\"] = metadata.start\nstreamlit_app/components/data_schema.py:193:    meta[\"end\"] = metadata.end\nsrc/trend_analysis/stages/portfolio.py:12:from analysis.results import build_metadata\nsrc/trend_analysis/stages/portfolio.py:764:    metadata = build_metadata(\nsrc/trend_analysis/stages/portfolio.py:780:    metadata[\"frequency\"] = preprocess.frequency_payload\nsrc/trend_analysis/stages/portfolio.py:781:    metadata[\"missing_data\"] = preprocess.missing_payload\nsrc/trend_analysis/stages/portfolio.py:782:    metadata[\"risk_free_column\"] = rf_col\nsrc/trend_analysis/stages/portfolio.py:783:    metadata[\"indices\"] = {\nsrc/trend_analysis/stages/portfolio.py:832:            \"metadata\": metadata,\nsrc/trend/reporting/unified.py:546:    fig.savefig(buf, format=\"png\", metadata={\"Software\": \"trend-report\"})\nstreamlit_app/components/data_cache.py:101:    \"\"\"Return a mapping from human readable label to dataset metadata.\"\"\"\nsrc/trend_analysis/stages/selection.py:279:        and carries ``insample_len`` and ``period`` metadata so callers\nsrc/trend_analysis/__init__.py:4:import importlib.metadata\nsrc/trend_analysis/__init__.py:207:# Get version from package metadata\nsrc/trend_analysis/__init__.py:209:    __version__ = importlib.metadata.version(\"trend-analysis\")\nsrc/trend_analysis/__init__.py:210:except importlib.metadata.PackageNotFoundError:\nsrc/trend_analysis/gui/plugins.py:19:    import importlib.metadata as metadata\nsrc/trend_analysis/gui/plugins.py:21:    for ep in metadata.entry_points(group=\"trend_analysis.gui_plugins\"):\ntests/test_build_autofix_pr_comment.py:146:def test_build_comment_includes_trigger_metadata(\ntests/test_export_bundle.py:260:    \"\"\"_git_hash should return a sentinel when git metadata is unavailable.\"\"\"\ntests/test_export_bundle.py:296:    \"\"\"If importlib metadata lookup fails, fallback version should be used.\"\"\"\ntests/test_export_bundle.py:298:    fake_meta = ModuleType(\"importlib.metadata\")\ntests/test_export_bundle.py:304:    monkeypatch.setitem(sys.modules, \"importlib.metadata\", fake_meta)\ntests/test_analysis_results_module.py:11:from analysis import Results, build_metadata, compute_universe_fingerprint\ntests/test_analysis_results_module.py:79:def test_build_metadata_includes_core_fields(monkeypatch) -> None:\ntests/test_analysis_results_module.py:83:    meta = build_metadata(\ntests/test_analysis_results_module.py:118:        \"metadata\": {\ntests/test_portfolio_app_data_schema.py:21:    metadata = SimpleNamespace(\ntests/test_portfolio_app_data_schema.py:41:    return SimpleNamespace(metadata=metadata, frame=frame)\ntests/test_trend_cli_entrypoints.py:190:def test_run_pipeline_sets_metadata_and_bundle(\nsrc/trend_analysis/multi_period/engine.py:980:    missing_policy_metadata = {\nsrc/trend_analysis/multi_period/engine.py:1062:            res_dict.update(missing_policy_metadata)\nsrc/trend_analysis/multi_period/engine.py:3539:        res_dict.update(missing_policy_metadata)\ntests/test_trend_package_init.py:13:    monkeypatch.setattr(trend._metadata, \"version\", lambda name: \"1.2.3\")\ntests/test_trend_package_init.py:19:        raise trend._metadata.PackageNotFoundError\ntests/test_trend_package_init.py:21:    monkeypatch.setattr(trend._metadata, \"version\", fake_version)\ntests/test_trend_package_init.py:33:    monkeypatch.setattr(trend._metadata, \"version\", lambda name: \"2.0.0\")\ntests/test_pipeline_integration_direct.py:222:    # Ensure bundle receipt exposes config hash metadata.\ntests/test_validators.py:21:    def test_report_includes_metadata(self) -> None:\ntests/test_validators.py:38:    def test_valid_dataframe_returns_metadata(self) -> None:\ntests/test_validators.py:48:        assert result.metadata is not None\ntests/test_validators.py:49:        assert result.metadata.mode == MarketDataMode.RETURNS\ntests/test_validators.py:50:        assert result.metadata.frequency == \"M\"\ntests/test_validators.py:51:        assert result.metadata.frequency_detected == \"M\"\ntests/test_validators.py:52:        assert result.metadata.frequency_label == \"monthly\"\ntests/test_validators.py:53:        assert result.metadata.frequency_missing_periods == 0\ntests/test_validators.py:54:        assert result.metadata.frequency_tolerance_periods >= 0\ntests/test_validators.py:96:        assert result.metadata is not None\ntests/test_validators.py:97:        assert result.metadata.columns == [\"FundB\"]\ntests/test_validators.py:127:        assert df.attrs[\"market_data\"][\"metadata\"].mode == MarketDataMode.RETURNS\ntests/test_validators.py:128:        metadata = meta[\"metadata\"]\ntests/test_validators.py:129:        assert metadata.frequency_label == \"monthly\"\ntests/test_validators.py:130:        assert metadata.frequency_detected == \"M\"\nsrc/trend_analysis/api.py:63:def _attach_reporting_metadata(res_dict: dict[str, Any], config: ConfigType) -> None:\nsrc/trend_analysis/api.py:64:    \"\"\"Attach reporting-only metadata from config without affecting computation.\"\"\"\nsrc/trend_analysis/api.py:80:    metadata = res_dict.get(\"metadata\")\nsrc/trend_analysis/api.py:81:    if not isinstance(metadata, dict):\nsrc/trend_analysis/api.py:82:        metadata = {}\nsrc/trend_analysis/api.py:83:        res_dict[\"metadata\"] = metadata\nsrc/trend_analysis/api.py:84:    reporting = metadata.get(\"reporting\")\nsrc/trend_analysis/api.py:87:        metadata[\"reporting\"] = reporting\nsrc/trend_analysis/api.py:104:        Environment metadata (python/numpy/pandas versions).\nsrc/trend_analysis/api.py:122:    metadata: dict[str, Any] | None = None\nsrc/trend_analysis/api.py:145:        Environment metadata.\nsrc/trend_analysis/api.py:249:            rr.metadata = structured.metadata\nsrc/trend_analysis/api.py:517:        _attach_reporting_metadata(res_dict, config)\nsrc/trend_analysis/api.py:648:            rr.metadata = structured.metadata\ntests/test_multi_period_engine_incremental_extra.py:270:    # Cache instantiation failure should not attach cache_stats metadata\ntests/test_trend_analysis_data_module.py:29:def _build_metadata(index: pd.DatetimeIndex) -> MarketDataMetadata:\ntests/test_trend_analysis_data_module.py:121:    metadata = _build_metadata(index)\ntests/test_trend_analysis_data_module.py:122:    validated = ValidatedMarketData(validated_frame, metadata)\ntests/test_trend_analysis_data_module.py:163:    assert market_attrs[\"metadata\"] == metadata\ntests/test_trend_analysis_data_module.py:164:    assert result.attrs[\"market_data_mode\"] == metadata.mode.value\ntests/test_trend_analysis_data_module.py:165:    assert result.attrs[\"market_data_missing_policy\"] == metadata.missing_policy\ntests/test_trend_analysis_data_module.py:166:    assert result.attrs[\"market_data_columns\"] == metadata.columns\ntests/test_trend_analysis_data_module.py:207:    metadata = _build_metadata(index)\ntests/test_trend_analysis_data_module.py:208:    validated = ValidatedMarketData(frame, metadata)\ntests/test_trend_analysis_data_module.py:213:    assert result.attrs[\"market_data_rows\"] == metadata.rows\ntests/test_trend_analysis_data_module.py:230:        _build_metadata(index),\ntests/test_trend_analysis_data_module.py:286:        _build_metadata(index),\ntests/test_trend_analysis_data_module.py:346:        _build_metadata(index),\ntests/test_trend_analysis_data_module.py:419:        _build_metadata(index),\ntests/test_trend_analysis_data_module.py:445:            _build_metadata(idx),\ntests/test_io_validators_extra.py:33:def _metadata_with_warnings() -> MarketDataMetadata:\ntests/test_io_validators_extra.py:57:def _metadata_without_warnings() -> MarketDataMetadata:\ntests/test_io_validators_extra.py:80:    metadata = _metadata_with_warnings()\ntests/test_io_validators_extra.py:87:    summary = _ValidationSummary(metadata, frame)\ntests/test_io_validators_extra.py:96:def test_build_result_propagates_metadata() -> None:\ntests/test_io_validators_extra.py:97:    metadata = _metadata_with_warnings()\ntests/test_io_validators_extra.py:98:    frame = pd.DataFrame({\"FundA\": [0.01] * metadata.rows})\ntests/test_io_validators_extra.py:99:    validated = ValidatedMarketData(frame=frame, metadata=metadata)\ntests/test_io_validators_extra.py:102:    assert result.metadata is metadata\ntests/test_io_validators_extra.py:107:    metadata = _metadata_without_warnings()\ntests/test_io_validators_extra.py:109:        {\"FundA\": [0.01] * metadata.rows, \"FundB\": [0.02] * metadata.rows}\ntests/test_io_validators_extra.py:111:    summary = _ValidationSummary(metadata, frame)\ntests/test_io_validators_extra.py:285:def test_load_and_validate_upload_returns_metadata(\ntests/test_io_validators_extra.py:288:    metadata = _metadata_with_warnings()\ntests/test_io_validators_extra.py:291:            \"Date\": pd.date_range(\"2024-01-31\", periods=metadata.rows, freq=\"ME\"),\ntests/test_io_validators_extra.py:303:        return ValidatedMarketData(frame=data.set_index(\"Date\"), metadata=metadata)\ntests/test_io_validators_extra.py:311:    assert meta[\"metadata\"] is metadata\ntests/test_io_validators_extra.py:312:    assert meta[\"mode\"] == metadata.mode.value\ntests/test_io_validators_extra.py:313:    assert list(meta[\"original_columns\"]) == metadata.columns\ntests/test_io_validators_extra.py:352:def test_validation_result_report_includes_metadata() -> None:\ntests/test_io_validators_extra.py:353:    metadata = _metadata_with_warnings()\ntests/test_io_validators_extra.py:360:        metadata=metadata,\ntests/test_io_validators_extra.py:371:def test_validation_result_report_omits_optional_metadata() -> None:\ntests/test_io_validators_extra.py:375:        True, [], [], frequency=None, date_range=None, metadata=None\ntests/test_data_schema.py:33:    assert meta[\"metadata\"].mode == MarketDataMode.RETURNS\ntests/test_data_schema.py:67:    assert meta[\"metadata\"].mode == MarketDataMode.RETURNS\ntests/test_data_schema.py:139:def test_build_meta_populates_warnings_and_metadata_fields():\ntests/test_data_schema.py:140:    metadata = MarketDataMetadata(\ntests/test_data_schema.py:162:        metadata,\nsrc/trend_analysis/data.py:171:    attrs[\"market_data\"][\"metadata\"] = validated.metadata\nsrc/trend_analysis/data.py:172:    attrs[\"market_data_mode\"] = validated.metadata.mode.value\nsrc/trend_analysis/data.py:173:    attrs[\"market_data_frequency\"] = validated.metadata.frequency\nsrc/trend_analysis/data.py:174:    attrs[\"market_data_frequency_code\"] = validated.metadata.frequency_detected\nsrc/trend_analysis/data.py:175:    attrs[\"market_data_frequency_label\"] = validated.metadata.frequency_label\nsrc/trend_analysis/data.py:177:        validated.metadata.frequency_median_spacing_days\nsrc/trend_analysis/data.py:180:        validated.metadata.frequency_missing_periods\nsrc/trend_analysis/data.py:183:        validated.metadata.frequency_max_gap_periods\nsrc/trend_analysis/data.py:186:        validated.metadata.frequency_tolerance_periods\nsrc/trend_analysis/data.py:188:    attrs[\"market_data_columns\"] = list(validated.metadata.columns)\nsrc/trend_analysis/data.py:189:    attrs[\"market_data_rows\"] = validated.metadata.rows\nsrc/trend_analysis/data.py:190:    attrs[\"market_data_date_range\"] = validated.metadata.date_range\nsrc/trend_analysis/data.py:191:    attrs[\"market_data_missing_policy\"] = validated.metadata.missing_policy\nsrc/trend_analysis/data.py:192:    attrs[\"market_data_missing_policy_limit\"] = validated.metadata.missing_policy_limit\nsrc/trend_analysis/data.py:194:        validated.metadata.missing_policy_summary\nsrc/trend_analysis/data.py:210:    except Exception:  # pragma: no cover - best effort metadata\nsrc/trend_analysis/data.py:231:        metadata = market_meta.get(\"metadata\")\nsrc/trend_analysis/data.py:232:        if metadata is not None:\nsrc/trend_analysis/data.py:233:            candidates.append(getattr(metadata, \"frequency_detected\", None))\nsrc/trend_analysis/data.py:234:            candidates.append(getattr(metadata, \"frequency\", None))\nsrc/trend_analysis/cli.py:9:from importlib import metadata\nsrc/trend_analysis/cli.py:299:            installed = metadata.version(name)\nsrc/trend_analysis/cli.py:300:        except metadata.PackageNotFoundError:\ntests/test_validators_branch_coverage.py:20:def _metadata(**overrides: object) -> MarketDataMetadata:\ntests/test_validators_branch_coverage.py:37:    metadata = _metadata(\ntests/test_validators_branch_coverage.py:44:    summary = validators._ValidationSummary(metadata, frame)\ntests/test_validators_branch_coverage.py:53:def test_validation_result_report_includes_metadata() -> None:\ntests/test_validators_branch_coverage.py:54:    metadata = _metadata(\ntests/test_validators_branch_coverage.py:66:        date_range=metadata.date_range,\ntests/test_validators_branch_coverage.py:67:        metadata=metadata,\ntests/test_export_formatter.py:124:        \"metadata\": {\ntests/test_export_formatter.py:234:        \"metadata\": {\ntests/test_gui.py:39:    monkeypatch.setattr(importlib.metadata, \"entry_points\", lambda group=None: eps)\ntests/test_trend_cli_soft_coverage.py:6:from importlib import metadata\ntests/test_trend_cli_soft_coverage.py:227:        raise metadata.PackageNotFoundError(name)\ntests/test_trend_cli_soft_coverage.py:229:    monkeypatch.setattr(metadata, \"version\", fake_version)\ntests/test_trend_cli_soft_coverage.py:249:        raise metadata.PackageNotFoundError(name)\ntests/test_trend_cli_soft_coverage.py:251:    monkeypatch.setattr(metadata, \"version\", fake_version)\ntests/test_time_utils.py:9:def test_align_calendar_empty_preserves_metadata():\ntests/test_time_utils.py:40:def test_holiday_overrides_drop_rows_and_record_metadata():\ntests/test_export_manifest_schema.py:3:These tests ensure that the metadata generated by :func:`export_bundle`\ntests/test_export_manifest_schema.py:87:    \"\"\"The generated metadata should satisfy the schema.\"\"\"\ntests/test_data.py:205:def _build_metadata(columns: list[str]) -> MarketDataMetadata:\ntests/test_data.py:233:def test_finalise_validated_frame_transfers_metadata() -> None:\ntests/test_data.py:238:    metadata = _build_metadata([\"fund\"])\ntests/test_data.py:239:    metadata.missing_policy_limit = 2\ntests/test_data.py:240:    metadata.missing_policy_summary = \"filled\"\ntests/test_data.py:241:    metadata.missing_policy_filled = {\ntests/test_data.py:244:    validated = ValidatedMarketData(frame=frame, metadata=metadata)\ntests/test_data.py:249:    assert attrs[\"metadata\"].missing_policy == \"ffill\"\ntests/test_data.py:267:        meta = _build_metadata([\"A\", \"B\", \"C\"])\ntests/test_data.py:268:        return ValidatedMarketData(frame=payload.set_index(\"Date\"), metadata=meta)\ntests/test_data.py:307:        meta = _build_metadata([\"A\"])\ntests/test_data.py:308:        return ValidatedMarketData(frame=payload.set_index(\"Date\"), metadata=meta)\ntests/test_data.py:330:        meta = _build_metadata([\"A\"])\ntests/test_data.py:331:        return ValidatedMarketData(frame=payload.set_index(\"Date\"), metadata=meta)\ntests/test_data.py:402:    metadata = _build_metadata([\"fund\"])\ntests/test_data.py:403:    metadata.mode = MarketDataMode.PRICE\ntests/test_data.py:404:    metadata.frequency_detected = \"D\"\ntests/test_data.py:405:    validated = ValidatedMarketData(frame=frame, metadata=metadata)\nsrc/trend_analysis/perf/timing.py:31:        Additional key/value metadata appended to the log line.\nsrc/trend_analysis/perf/timing.py:56:    Additional metadata can also be appended via ``result[\"extra\"]``.\ntests/test_trend_init_public_interface.py:6:import importlib.metadata as metadata\ntests/test_trend_init_public_interface.py:14:    \"\"\"Reload :mod:`trend` with a patched metadata lookup.\"\"\"\ntests/test_trend_init_public_interface.py:17:    monkeypatch.setattr(metadata, \"version\", version_callable)\ntests/test_trend_init_public_interface.py:23:def test_dunder_version_uses_package_metadata(monkeypatch):\ntests/test_trend_init_public_interface.py:24:    \"\"\"``trend.__version__`` should proxy to the installed package metadata.\"\"\"\ntests/test_trend_init_public_interface.py:33:    \"\"\"Missing package metadata should trigger the development fallback\ntests/test_trend_init_public_interface.py:37:        raise metadata.PackageNotFoundError\ntests/test_trend_analysis_data_keepalive.py:13:def make_validated(frame: pd.DataFrame, **metadata_fields: Any) -> ValidatedMarketData:\ntests/test_trend_analysis_data_keepalive.py:14:    metadata_defaults: dict[str, Any] = {\ntests/test_trend_analysis_data_keepalive.py:30:    metadata_defaults.update(metadata_fields)\ntests/test_trend_analysis_data_keepalive.py:31:    metadata = SimpleNamespace(**metadata_defaults)\ntests/test_trend_analysis_data_keepalive.py:32:    return ValidatedMarketData(frame=frame, metadata=metadata)  # type: ignore[arg-type]\ntests/test_trend_analysis_data_keepalive.py:133:def test_finalise_validated_frame_copies_metadata(sample_frame: pd.DataFrame) -> None:\ntests/test_trend_analysis_data_keepalive.py:137:    assert result.attrs[\"market_data\"][\"metadata\"] is payload.metadata\ntests/test_trend_analysis_data_keepalive.py:145:    assert result.attrs[\"market_data_mode\"] == payload.metadata.mode.value\ntests/test_trend_analysis_data_additional.py:35:    \"\"\"Return a simple marker to avoid relying on real metadata structures.\"\"\"\ntests/test_trend_analysis_data_additional.py:74:        return SimpleNamespace(frame=payload, metadata=SimpleNamespace())\ntests/test_trend_analysis_data_additional.py:119:        return SimpleNamespace(frame=payload, metadata=SimpleNamespace())\ntests/test_trend_analysis_data_additional.py:191:        return SimpleNamespace(frame=payload, metadata=SimpleNamespace())\ntests/test_trend_analysis_data_additional.py:284:def test_contract_frequency_prefers_detected_and_metadata_fallbacks() -> None:\ntests/test_trend_analysis_data_additional.py:295:        \"market_data\": {\"metadata\": meta},\ntests/test_io_validators_additional.py:31:    assert df.attrs[\"market_data\"][\"metadata\"].mode == MarketDataMode.PRICE\ntests/golden/test_demo.py:52:            # Skip or normalize metadata lines that contain timestamps/versions\ntests/golden/test_demo.py:65:                # Replace with normalized metadata line\ntests/golden/test_demo.py:75:                    continue  # Skip other timestamp-like metadata\ntests/golden/test_demo.py:406:        #   including any indented metadata lines, followed by a ``default``\ntests/test_streamlit_state.py:57:        meta = {\"test\": \"metadata\"}\ntests/test_streamlit_state.py:74:        meta = {\"test\": \"metadata\"}\ntests/test_streamlit_state.py:86:        meta = {\"test\": \"metadata\"}\nsrc/trend_analysis/export/__init__.py:283:        metadata = res.get(\"metadata\")\nsrc/trend_analysis/export/__init__.py:284:        if isinstance(metadata, Mapping):\nsrc/trend_analysis/export/__init__.py:285:            freq = cast(Mapping[str, Any], metadata.get(\"frequency\", {}))\nsrc/trend_analysis/export/__init__.py:293:        metadata = res.get(\"metadata\")\nsrc/trend_analysis/export/__init__.py:294:        if isinstance(metadata, Mapping):\nsrc/trend_analysis/export/__init__.py:295:            policy = cast(Mapping[str, Any], metadata.get(\"missing_data\", {}))\nsrc/trend_analysis/export/__init__.py:339:def _format_metadata_entries(meta: Mapping[str, Any]) -> list[str]:\nsrc/trend_analysis/export/__init__.py:410:def _metadata_summary_lines(res: Mapping[str, Any]) -> list[str]:\nsrc/trend_analysis/export/__init__.py:422:    metadata = res.get(\"metadata\")\nsrc/trend_analysis/export/__init__.py:423:    if isinstance(metadata, Mapping):\nsrc/trend_analysis/export/__init__.py:424:        segments.extend(_format_metadata_entries(metadata))\nsrc/trend_analysis/export/__init__.py:427:    # The legacy workbook layout expects a single metadata row before the\nsrc/trend_analysis/export/__init__.py:430:    # preserve the new metadata while keeping the row budget constant.\nsrc/trend_analysis/export/__init__.py:482:        meta_lines = _metadata_summary_lines(res)\nsrc/trend_analysis/export/__init__.py:911:    header.extend(_metadata_summary_lines(res))\nsrc/trend_analysis/export/bundle.py:31:        # repository metadata isn't available (e.g. in a zipped release\nsrc/trend_analysis/export/bundle.py:234:            fig.savefig(charts_dir / \"equity_curve.png\", metadata={\"run_id\": run_id})\nsrc/trend_analysis/export/bundle.py:251:            fig.savefig(charts_dir / \"drawdown.png\", metadata={\"run_id\": run_id})\nsrc/trend_analysis/export/bundle.py:304:            import importlib.metadata as _ilmd\nsrc/trend_analysis/export/bundle.py:360:charts, and metadata necessary for reproducibility and sharing.\nsrc/trend_analysis/export/bundle.py:377:run_meta.json        - Configuration, environment, and reproducibility metadata\ntests/test_tearsheet.py:20:        metadata={\"fingerprint\": \"abc123\"},\ntests/test_config_models_additional.py:67:    \"\"\"``load`` should fold ``output`` metadata into the export settings.\"\"\"\ntests/test_trend_analysis_init_module.py:6:from importlib import metadata\ntests/test_trend_analysis_init_module.py:91:    # Arrange for importlib.metadata.version to raise PackageNotFoundError so\ntests/test_trend_analysis_init_module.py:95:        metadata,\ntests/test_trend_analysis_init_module.py:97:        lambda _: (_ for _ in ()).throw(metadata.PackageNotFoundError()),\ntests/test_trend_analysis_init_module.py:201:def test_version_fallback_when_package_metadata_missing(\ntests/test_trend_analysis_init_module.py:204:    from importlib import metadata\ntests/test_trend_analysis_init_module.py:207:        raise metadata.PackageNotFoundError\ntests/test_trend_analysis_init_module.py:209:    monkeypatch.setattr(metadata, \"version\", _raise_package_not_found, raising=False)\ntests/test_walkforward_engine.py:42:    # Window metadata should appear before metric columns and maintain ordering.\ntests/test_walkforward_engine.py:235:    # Without metric columns the OOS aggregates should be empty but metadata retained.\ntests/test_walkforward_engine.py:357:    # The window metadata columns stay at the front of the frame.\ntests/test_trend_analysis_package.py:100:        importlib.metadata,\ntests/test_trend_analysis_package.py:103:            importlib.metadata.PackageNotFoundError()\ntests/test_export_additional_coverage.py:77:    period_metadata: bool = True,\ntests/test_export_additional_coverage.py:128:        if period_metadata\ntests/test_export_additional_coverage.py:193:def test_export_phase1_workbook_missing_period_metadata(monkeypatch, tmp_path):\ntests/test_export_additional_coverage.py:195:        _make_period_result(\"1\", include_changes=True, period_metadata=False),\ntests/test_export_additional_coverage.py:196:        _make_period_result(\"2\", period_metadata=False),\ntests/test_export_additional_coverage.py:225:    # Missing period metadata should yield generated sheet names and blank ranges\ntests/workflows/test_ci_cosmetic_repair.py:507:        metadata={\"value\": 1.23},\ntests/workflows/test_ci_cosmetic_repair.py:520:            \"metadata\": {\"value\": 1.23},\ntests/workflows/test_ci_cosmetic_repair.py:552:        metadata={},\ntests/workflows/test_ci_cosmetic_repair.py:668:        metadata={},\ntests/test_single_period_run_avg_corr.py:11:def test_single_period_run_avg_corr_metadata() -> None:\nsrc/trend_analysis/io/__init__.py:9:    attach_metadata,\nsrc/trend_analysis/io/__init__.py:30:    \"attach_metadata\",\ntests/workflows/test_ci_history.py:113:def test_build_history_record_enriches_metadata(\nsrc/trend_analysis/io/validators.py:18:    attach_metadata,\nsrc/trend_analysis/io/validators.py:28:    metadata: MarketDataMetadata\nsrc/trend_analysis/io/validators.py:33:        rows = self.metadata.rows\nsrc/trend_analysis/io/validators.py:44:        if self.metadata.frequency_missing_periods > 0:\nsrc/trend_analysis/io/validators.py:47:                f\"{self.metadata.frequency_missing_periods} missing {self.metadata.frequency_label} periods \"\nsrc/trend_analysis/io/validators.py:48:                f\"(max gap {self.metadata.frequency_max_gap_periods}).\"\nsrc/trend_analysis/io/validators.py:50:        if self.metadata.missing_policy_dropped:\nsrc/trend_analysis/io/validators.py:51:            dropped = \", \".join(sorted(self.metadata.missing_policy_dropped))\nsrc/trend_analysis/io/validators.py:54:                f\"{dropped} (policy={self.metadata.missing_policy}).\"\nsrc/trend_analysis/io/validators.py:56:        if self.metadata.missing_policy_summary and (\nsrc/trend_analysis/io/validators.py:57:            self.metadata.frequency_missing_periods > 0\nsrc/trend_analysis/io/validators.py:58:            or bool(self.metadata.missing_policy_filled)\nsrc/trend_analysis/io/validators.py:59:            or bool(self.metadata.missing_policy_dropped)\nsrc/trend_analysis/io/validators.py:62:                f\"Missing-data policy applied: {self.metadata.missing_policy_summary}.\"\nsrc/trend_analysis/io/validators.py:78:        metadata: MarketDataMetadata | None = None,\nsrc/trend_analysis/io/validators.py:85:        self.metadata = metadata\nsrc/trend_analysis/io/validators.py:86:        self.mode: MarketDataMode | None = metadata.mode if metadata else None\nsrc/trend_analysis/io/validators.py:101:                self.metadata\nsrc/trend_analysis/io/validators.py:102:                and self.metadata.missing_policy_summary\nsrc/trend_analysis/io/validators.py:104:                    self.metadata.frequency_missing_periods > 0\nsrc/trend_analysis/io/validators.py:105:                    or bool(self.metadata.missing_policy_filled)\nsrc/trend_analysis/io/validators.py:106:                    or bool(self.metadata.missing_policy_dropped)\nsrc/trend_analysis/io/validators.py:110:                    f\"ğŸ§¹ Missing data policy: {self.metadata.missing_policy_summary}\"\nsrc/trend_analysis/io/validators.py:160:    summary = _ValidationSummary(validated.metadata, validated.frame)\nsrc/trend_analysis/io/validators.py:166:        frequency=validated.metadata.frequency_label,\nsrc/trend_analysis/io/validators.py:167:        date_range=validated.metadata.date_range,\nsrc/trend_analysis/io/validators.py:168:        metadata=validated.metadata,\nsrc/trend_analysis/io/validators.py:267:    \"\"\"Load uploaded content, validate it, and attach metadata.\"\"\"\nsrc/trend_analysis/io/validators.py:275:    attach_metadata(validated.frame, validated.metadata)\nsrc/trend_analysis/io/validators.py:278:        \"metadata\": validated.metadata,\nsrc/trend_analysis/io/validators.py:280:        \"n_rows\": validated.metadata.rows,\nsrc/trend_analysis/io/validators.py:281:        \"original_columns\": list(validated.metadata.columns),\nsrc/trend_analysis/io/validators.py:282:        \"mode\": validated.metadata.mode.value,\nsrc/trend_analysis/io/validators.py:283:        \"frequency\": validated.metadata.frequency_label,\nsrc/trend_analysis/io/validators.py:284:        \"date_range\": validated.metadata.date_range,\nsrc/trend_analysis/io/market_data.py:247:    \"\"\"Container that pairs a validated frame with its metadata.\"\"\"\nsrc/trend_analysis/io/market_data.py:250:    metadata: MarketDataMetadata\nsrc/trend_analysis/io/market_data.py:888:    metadata = MarketDataMetadata(\nsrc/trend_analysis/io/market_data.py:916:    attach_metadata(validated, metadata)\nsrc/trend_analysis/io/market_data.py:918:    return ValidatedMarketData(frame=validated, metadata=metadata)\nsrc/trend_analysis/io/market_data.py:957:def attach_metadata(frame: pd.DataFrame, metadata: MarketDataMetadata) -> pd.DataFrame:\nsrc/trend_analysis/io/market_data.py:958:    \"\"\"Attach metadata to a DataFrame in-place and return it.\"\"\"\nsrc/trend_analysis/io/market_data.py:963:            \"metadata\": metadata,\nsrc/trend_analysis/io/market_data.py:964:            \"mode\": \"returns\" if metadata.mode == MarketDataMode.RETURNS else \"prices\",\nsrc/trend_analysis/io/market_data.py:965:            \"mode_enum\": metadata.mode,\nsrc/trend_analysis/io/market_data.py:966:            \"frequency\": metadata.frequency_label,\nsrc/trend_analysis/io/market_data.py:967:            \"frequency_code\": metadata.frequency,\nsrc/trend_analysis/io/market_data.py:968:            \"frequency_detected\": metadata.frequency_detected,\nsrc/trend_analysis/io/market_data.py:969:            \"frequency_median_spacing_days\": metadata.frequency_median_spacing_days,\nsrc/trend_analysis/io/market_data.py:970:            \"frequency_missing_periods\": metadata.frequency_missing_periods,\nsrc/trend_analysis/io/market_data.py:971:            \"frequency_max_gap_periods\": metadata.frequency_max_gap_periods,\nsrc/trend_analysis/io/market_data.py:972:            \"frequency_tolerance_periods\": metadata.frequency_tolerance_periods,\nsrc/trend_analysis/io/market_data.py:973:            \"start\": metadata.start.isoformat(),\nsrc/trend_analysis/io/market_data.py:974:            \"end\": metadata.end.isoformat(),\nsrc/trend_analysis/io/market_data.py:975:            \"rows\": metadata.rows,\nsrc/trend_analysis/io/market_data.py:976:            \"columns\": list(metadata.columns),\nsrc/trend_analysis/io/market_data.py:977:            \"symbols\": list(metadata.symbols),\nsrc/trend_analysis/io/market_data.py:978:            \"missing_policy\": metadata.missing_policy,\nsrc/trend_analysis/io/market_data.py:979:            \"missing_policy_limit\": metadata.missing_policy_limit,\nsrc/trend_analysis/io/market_data.py:980:            \"missing_policy_overrides\": dict(metadata.missing_policy_overrides),\nsrc/trend_analysis/io/market_data.py:981:            \"missing_policy_limits\": dict(metadata.missing_policy_limits),\nsrc/trend_analysis/io/market_data.py:988:                for column, details in metadata.missing_policy_filled.items()\nsrc/trend_analysis/io/market_data.py:990:            \"missing_policy_dropped\": list(metadata.missing_policy_dropped),\nsrc/trend_analysis/io/market_data.py:991:            \"missing_policy_summary\": metadata.missing_policy_summary,\nsrc/trend_analysis/presets.py:139:    \"\"\"Container holding preset metadata and normalised configuration.\"\"\"\ntests/test_package_init.py:96:        raise importlib.metadata.PackageNotFoundError\ntests/test_package_init.py:98:    monkeypatch.setattr(importlib.metadata, \"version\", raise_package_not_found)\ntests/test_trend_analysis_init_extra.py:40:        raise importlib.metadata.PackageNotFoundError\ntests/test_trend_analysis_init_extra.py:42:    monkeypatch.setattr(importlib.metadata, \"version\", _raise_missing)\ntests/test_trend_package_public_interface.py:11:    \"\"\"Reload the ``trend`` package to ensure patched metadata is visible.\"\"\"\ntests/test_trend_package_public_interface.py:18:def test_version_attribute_uses_metadata(monkeypatch) -> None:\ntests/test_trend_package_public_interface.py:21:    monkeypatch.setattr(trend._metadata, \"version\", lambda _: \"9.9.9\")\ntests/test_trend_package_public_interface.py:32:    monkeypatch.setattr(trend._metadata, \"PackageNotFoundError\", DummyError)\ntests/test_trend_package_public_interface.py:37:    monkeypatch.setattr(trend._metadata, \"version\", raise_error)\ntests/trend_analysis/test_data.py:22:def sample_metadata() -> MarketDataMetadata:\ntests/trend_analysis/test_data.py:38:def validated_payload(sample_metadata: MarketDataMetadata) -> ValidatedMarketData:\ntests/trend_analysis/test_data.py:55:    return ValidatedMarketData(frame=frame, metadata=sample_metadata)\ntests/trend_analysis/test_data.py:143:def test_finalise_validated_frame_includes_metadata(validated_payload):\ntests/trend_analysis/test_data.py:148:    assert market_attrs[\"metadata\"] is validated_payload.metadata\ntests/trend_analysis/test_data.py:149:    assert result.attrs[\"market_data_mode\"] == validated_payload.metadata.mode.value\ntests/trend_analysis/test_data.py:221:        validated_payload.metadata.columns\ntests/trend_analysis/test_data.py:263:    assert result.attrs[\"market_data_rows\"] == validated_payload.metadata.rows\ntests/trend_analysis/test_data.py:266:def test_validate_payload_applies_price_contract(monkeypatch, sample_metadata):\ntests/trend_analysis/test_data.py:273:        metadata=sample_metadata,\ntests/trend_analysis/test_data.py:292:def test_validate_payload_raises_value_error_for_contract(monkeypatch, sample_metadata):\ntests/trend_analysis/test_data.py:299:        metadata=sample_metadata,\ntests/trend_analysis/test_data.py:315:def test_validate_payload_logs_contract_error(monkeypatch, caplog, sample_metadata):\ntests/trend_analysis/test_data.py:322:        metadata=sample_metadata,\ntests/test_pipeline_helpers_additional.py:1163:        _metadata = [\"_columns_calls\", \"_true_columns\", \"_debug_calls\"]\ntests/test_pipeline_helpers_additional.py:1318:        _metadata = [\"_force_empty_next\"]\ntests/trend_analysis/test_package_init.py:5:import importlib.metadata\ntests/trend_analysis/test_package_init.py:219:def test_version_metadata_success_path(monkeypatch, load_trend_analysis):\ntests/trend_analysis/test_package_init.py:220:    monkeypatch.setattr(importlib.metadata, \"version\", lambda _: \"9.9.9\")\ntests/trend_analysis/test_package_init.py:225:def test_version_metadata_fallback_path(monkeypatch, load_trend_analysis):\ntests/trend_analysis/test_package_init.py:227:        importlib.metadata,\ntests/trend_analysis/test_package_init.py:229:        lambda *_: (_ for _ in ()).throw(importlib.metadata.PackageNotFoundError()),\ntests/test_trend_analysis_cli.py:103:            raise cli.metadata.PackageNotFoundError(name)\ntests/test_trend_analysis_cli.py:106:    monkeypatch.setattr(cli.metadata, \"version\", fake_version)\ntests/test_trend_analysis_cli.py:121:    monkeypatch.setattr(cli.metadata, \"version\", lambda name: \"4.0.0\")\ntests/app/test_demo_runner_component.py:206:    \"\"\"Ensure the Streamlit session state is updated with demo metadata.\"\"\"\ntests/app/test_data_schema.py:114:            self.metadata = type(\ntests/app/test_data_schema.py:228:    validated = SimpleNamespace(frame=frame, metadata=meta)\ntests/app/test_data_schema.py:264:            self.metadata = SimpleNamespace(\ntests/soft_coverage/test_data_soft.py:35:def sample_metadata() -> MarketDataMetadata:\ntests/soft_coverage/test_data_soft.py:101:def test_finalise_validated_frame_includes_metadata(\ntests/soft_coverage/test_data_soft.py:102:    sample_metadata: MarketDataMetadata,\ntests/soft_coverage/test_data_soft.py:110:    validated = ValidatedMarketData(frame=frame, metadata=sample_metadata)\ntests/soft_coverage/test_data_soft.py:117:    assert attrs[\"metadata\"] is sample_metadata\ntests/soft_coverage/test_data_soft.py:118:    assert result.attrs[\"market_data_mode\"] == sample_metadata.mode.value\ntests/soft_coverage/test_data_soft.py:120:        result.attrs[\"market_data_frequency_label\"] == sample_metadata.frequency_label\ntests/soft_coverage/test_data_soft.py:125:    sample_metadata: MarketDataMetadata,\ntests/soft_coverage/test_data_soft.py:130:    validated = ValidatedMarketData(frame=frame, metadata=sample_metadata)\ntests/soft_coverage/test_data_soft.py:135:    assert result.attrs[\"market_data_rows\"] == sample_metadata.rows\ntests/soft_coverage/test_data_soft.py:158:    monkeypatch: pytest.MonkeyPatch, sample_metadata: MarketDataMetadata\ntests/soft_coverage/test_data_soft.py:181:        return ValidatedMarketData(frame=normalised, metadata=sample_metadata)\ntests/soft_coverage/test_data_soft.py:202:    assert result.attrs[\"market_data_missing_policy\"] == sample_metadata.missing_policy\ntests/soft_coverage/test_data_soft.py:206:    monkeypatch: pytest.MonkeyPatch, sample_metadata: MarketDataMetadata\ntests/soft_coverage/test_data_soft.py:223:        return ValidatedMarketData(frame=df.set_index(\"Date\"), metadata=sample_metadata)\ntests/soft_coverage/test_data_soft.py:300:    monkeypatch: pytest.MonkeyPatch, tmp_path: Path, sample_metadata: MarketDataMetadata\ntests/soft_coverage/test_data_soft.py:313:        return ValidatedMarketData(frame=df.set_index(\"Date\"), metadata=sample_metadata)\ntests/soft_coverage/test_data_soft.py:325:    assert result.attrs[\"market_data_frequency\"] == sample_metadata.frequency\ntests/soft_coverage/test_data_soft.py:383:    monkeypatch: pytest.MonkeyPatch, tmp_path: Path, sample_metadata: MarketDataMetadata\ntests/soft_coverage/test_data_soft.py:399:        return ValidatedMarketData(frame=df.set_index(\"Date\"), metadata=sample_metadata)\ntests/soft_coverage/test_data_soft.py:506:            metadata=MarketDataMetadata(\ntests/soft_coverage/test_trend_analysis_init.py:96:    \"\"\"Missing __module__ metadata should propagate as an AttributeError.\"\"\"\ntests/soft_coverage/test_trend_analysis_init.py:234:def test_version_fallback_used_when_metadata_missing(monkeypatch):\ntests/soft_coverage/test_trend_analysis_init.py:235:    \"\"\"Missing package metadata should trigger the development version fallback.\"\"\"\ntests/soft_coverage/test_trend_analysis_init.py:242:        raise importlib.metadata.PackageNotFoundError\ntests/soft_coverage/test_trend_analysis_init.py:244:    monkeypatch.setattr(importlib.metadata, \"version\", raise_not_found)\ntests/test_validators_extended.py:31:    metadata = MarketDataMetadata.model_construct(\ntests/test_validators_extended.py:49:    summary = _ValidationSummary(metadata, frame)\ntests/test_gui_support.py:50:    monkeypatch.setattr(\"importlib.metadata.entry_points\", fake_entry_points)\ntests/test_trend_analysis_data.py:34:def _metadata(columns: list[str], rows: int = 2) -> MarketDataMetadata:\ntests/test_trend_analysis_data.py:64:        metadata = _metadata(list(frame.columns), rows=len(frame))\ntests/test_trend_analysis_data.py:73:        return ValidatedMarketData(frame=frame, metadata=metadata)\ntests/test_trend_analysis_data.py:120:def test_finalise_validated_frame_populates_metadata_attrs() -> None:\ntests/test_trend_analysis_data.py:124:    metadata = _metadata([\"A\"], rows=2)\ntests/test_trend_analysis_data.py:125:    validated = ValidatedMarketData(frame=frame, metadata=metadata)\ntests/test_trend_analysis_data.py:129:    assert result.attrs[\"market_data\"][\"metadata\"] == metadata\ntests/test_trend_analysis_data.py:130:    assert result.attrs[\"market_data_mode\"] == metadata.mode.value\ntests/test_trend_analysis_data.py:131:    assert result.attrs[\"market_data_frequency_label\"] == metadata.frequency_label\ntests/test_trend_analysis_data.py:134:        == metadata.missing_policy_limit\ntests/test_trend_analysis_data.py:142:    metadata = MarketDataMetadata(\ntests/test_trend_analysis_data.py:159:    validated = ValidatedMarketData(frame=frame, metadata=metadata)\ntests/test_api_run_simulation_branches.py:59:    \"\"\"Exercise the branches that construct metrics and fallback metadata.\"\"\"\ntests/test_api_run_simulation_branches.py:194:    metadata = {\"costs\": {\"monthly_cost\": 0.001}, \"fingerprint\": \"abc123def456\"}\ntests/test_api_run_simulation_branches.py:207:        \"metadata\": metadata,\ntests/test_api_run_simulation_branches.py:216:    assert result.analysis.metadata[\"fingerprint\"] == \"abc123def456\"\ntests/test_api_run_simulation_branches.py:223:def test_run_simulation_attaches_ci_level_metadata(monkeypatch) -> None:\ntests/test_api_run_simulation_branches.py:232:        \"metadata\": {},\ntests/test_api_run_simulation_branches.py:240:    assert result.analysis.metadata[\"reporting\"][\"ci_level\"] == 0.9\ntests/test_api_run_simulation_branches.py:252:        \"metadata\": {},\ntests/test_api_run_simulation_branches.py:260:    assert result.analysis.metadata[\"reporting\"][\"ci_level\"] == 0.85\ntests/test_api_run_simulation_branches.py:277:            \"metadata\": {\"fingerprint\": \"abc123\"},\ntests/test_api_run_simulation_branches.py:290:    assert baseline.analysis.metadata[\"fingerprint\"] == \"abc123\"\ntests/test_api_run_simulation_branches.py:291:    assert with_ci.analysis.metadata[\"fingerprint\"] == \"abc123\"\ntests/test_api_run_simulation_branches.py:292:    assert \"reporting\" not in baseline.analysis.metadata\ntests/test_api_run_simulation_branches.py:293:    assert with_ci.analysis.metadata[\"reporting\"][\"ci_level\"] == 0.9\ntests/test_trend_analysis_cli_main.py:7:from importlib import metadata\ntests/test_trend_analysis_cli_main.py:690:            raise metadata.PackageNotFoundError\ntests/test_trend_analysis_cli_main.py:693:    monkeypatch.setattr(cli.metadata, \"version\", fake_version)\ntests/test_perf_timing.py:38:def test_timed_stage_defaults_and_extra_metadata(caplog):\ntests/test_trend_analysis_package_init.py:2:import importlib.metadata\ntests/test_trend_analysis_package_init.py:39:def test_version_fallback_when_metadata_missing(\ntests/test_trend_analysis_package_init.py:47:            raise importlib.metadata.PackageNotFoundError\ntests/test_trend_analysis_package_init.py:49:        m.setattr(importlib.metadata, \"version\", fake_version)\ntests/test_market_data_validation_additional.py:16:def base_metadata_kwargs() -> dict[str, object]:\ntests/test_market_data_validation_additional.py:17:    \"\"\"Provide the minimal arguments required to build metadata instances.\"\"\"\ntests/test_market_data_validation_additional.py:29:def test_metadata_syncs_columns_and_symbols_and_date_range(\ntests/test_market_data_validation_additional.py:30:    base_metadata_kwargs: dict[str, object],\ntests/test_market_data_validation_additional.py:32:    metadata_with_columns = market_data.MarketDataMetadata(\ntests/test_market_data_validation_additional.py:34:        **base_metadata_kwargs,\ntests/test_market_data_validation_additional.py:37:    assert metadata_with_columns.symbols == [\"FundA\", \"FundB\"]\ntests/test_market_data_validation_additional.py:38:    assert metadata_with_columns.date_range == (\"2024-01-31\", \"2024-04-30\")\ntests/test_market_data_validation_additional.py:40:    metadata_with_symbols = market_data.MarketDataMetadata(\ntests/test_market_data_validation_additional.py:42:        **base_metadata_kwargs,\ntests/test_market_data_validation_additional.py:45:    assert metadata_with_symbols.columns == [\"Alpha\"]\ntests/test_market_data_validation_additional.py:49:    base_metadata_kwargs: dict[str, object],\ntests/test_market_data_validation_additional.py:55:    metadata = market_data.MarketDataMetadata(columns=[\"FundA\"], **base_metadata_kwargs)\ntests/test_market_data_validation_additional.py:56:    validated = market_data.ValidatedMarketData(frame=frame, metadata=metadata)\ntests/test_logging_module.py:76:    # Emit two structured log lines with consistent ``user`` metadata so the\ntests/test_logging_module.py:98:    # shared ``user`` metadata into its own column.\ntests/test_logging_module.py:133:    \"\"\"The reader should gracefully handle logs without ``extra`` metadata.\"\"\"\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_19","type":"reasoning","text":"**Defining RunPayload protocol and dataclass**\n\nI'm preparing to add a new RunPayload protocol and a corresponding dataclass in the diagnostics module, keeping changes minimal to focus on this first incomplete task. I plan to include optional value, diagnostic, and metadata fields, carefully naming the dataclass to avoid conflicts and ensuring proper typing imports. I'll place these definitions in trend_analysis/diagnostics and update exports accordingly."}}
{"type":"item.completed","item":{"id":"item_20","type":"file_change","changes":[{"path":"/home/runner/work/Trend_Model_Project/Trend_Model_Project/src/trend_analysis/diagnostics.py","kind":"update"}],"status":"completed"}}
{"type":"item.completed","item":{"id":"item_21","type":"reasoning","text":"**Updating type annotations and exports**"}}
{"type":"item.completed","item":{"id":"item_22","type":"file_change","changes":[{"path":"/home/runner/work/Trend_Model_Project/Trend_Model_Project/src/trend_analysis/diagnostics.py","kind":"update"}],"status":"completed"}}
{"type":"item.completed","item":{"id":"item_23","type":"reasoning","text":"**Checking git status before commit**"}}
{"type":"item.started","item":{"id":"item_24","type":"command_execution","command":"/bin/bash -lc 'git status --short'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_24","type":"command_execution","command":"/bin/bash -lc 'git status --short'","aggregated_output":" M .workflows-lib\n M src/trend_analysis/diagnostics.py\n?? codex-session-4196.jsonl\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_25","type":"reasoning","text":"**Encountering unexpected changes and seeking guidance**"}}
{"type":"item.completed","item":{"id":"item_26","type":"agent_message","text":"I noticed unexpected changes I didnâ€™t make: ` .workflows-lib ` is modified and `codex-session-4196.jsonl` is untracked. How would you like me to proceed with those?\n\nI did add the canonical `RunPayload` protocol and a `RunPayloadResult` dataclass in `src/trend_analysis/diagnostics.py` to cover `value`, `diagnostic`, and `metadata` fields."}}
{"type":"turn.completed","usage":{"input_tokens":294093,"cached_input_tokens":259840,"output_tokens":1993}}
