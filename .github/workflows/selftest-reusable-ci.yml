name: "Selftest: Reusables"

on:
  schedule:
    - cron: '30 6 * * *'
  workflow_dispatch:
    inputs:
      reason:
        description: 'Why are you running this selftest?'
        required: false
        default: 'manual test'
      mode:
        description: 'How should the self-test results be surfaced?'
        required: true
        type: choice
        options:
          - summary
          - comment
          - dual-runtime
        default: summary
      post_to:
        description: 'Where should comment output be posted?'
        required: true
        type: choice
        options:
          - pr-number
          - none
        default: none
      enable_history:
        description: 'Download the self-test report artifact for local reference.'
        required: true
        type: choice
        options:
          - true
          - false
        default: false
      pull_request_number:
        description: 'Pull request number when posting a comment.'
        required: false
      summary_title:
        description: 'Heading used in the workflow summary block.'
        required: false
        default: "Selftest Reusables Summary"
      comment_title:
        description: 'Heading rendered at the top of the posted comment.'
        required: false
        default: "Selftest Reusables Comment"
      python_versions:
        description: >-
          JSON array of Python versions forwarded to the reusable CI executor.
          Leave empty to exercise the default 3.11 matrix or select dual-runtime
          from the mode input.
        required: false
        default: ''

jobs:
  scenario:
    name: Scenario - ${{ matrix.name }}
    uses: ./.github/workflows/reusable-10-ci-python.yml
    with:
      python-versions: ${{ (github.event_name == 'workflow_dispatch' && inputs.python_versions != '' && inputs.python_versions) || (github.event_name == 'workflow_dispatch' && inputs.mode == 'dual-runtime' && '["3.11","3.12"]') || '["3.11"]' }}
      artifact-prefix: 'sf-${{ matrix.name }}-'
      enable-metrics: ${{ matrix.enable-metrics }}
      enable-history: ${{ matrix.enable-history }}
      enable-classification: ${{ matrix.enable-classification }}
      enable-coverage-delta: ${{ matrix.enable-coverage-delta }}
      enable-soft-gate: ${{ matrix.enable-soft-gate }}
      baseline-coverage: ${{ matrix.baseline-coverage || '0' }}
      coverage-alert-drop: ${{ matrix.coverage-alert-drop || '1' }}
    secrets: inherit
    strategy:
      fail-fast: false
      matrix:
        include:
          - name: minimal
            enable-metrics: false
            enable-history: false
            enable-classification: false
            enable-coverage-delta: false
            enable-soft-gate: false
          - name: metrics_only
            enable-metrics: true
            enable-history: false
            enable-classification: false
            enable-coverage-delta: false
            enable-soft-gate: false
          - name: metrics_history
            enable-metrics: true
            enable-history: true
            enable-classification: false
            enable-coverage-delta: false
            enable-soft-gate: false
          - name: classification_only
            enable-metrics: false
            enable-history: false
            enable-classification: true
            enable-coverage-delta: false
            enable-soft-gate: false
          - name: coverage_delta
            enable-metrics: false
            enable-history: false
            enable-classification: false
            enable-coverage-delta: true
            enable-soft-gate: false
            baseline-coverage: '65'
            coverage-alert-drop: '2'
          - name: full_soft_gate
            enable-metrics: true
            enable-history: true
            enable-classification: true
            enable-coverage-delta: true
            enable-soft-gate: true
            baseline-coverage: '65'
            coverage-alert-drop: '2'

  aggregate:
    name: Aggregate Verification
    if: ${{ always() }}
    needs: scenario
    runs-on: ubuntu-latest
    permissions:
      contents: read
      actions: read
    outputs:
      verification_table: ${{ steps.verify.outputs.table }}
      failures: ${{ steps.verify.outputs.failures }}
      run_id: ${{ steps.run_metadata.outputs.run_id }}
    env:
      SCENARIO_LIST: minimal, metrics_only, metrics_history, classification_only, coverage_delta, full_soft_gate
      PYTHON_VERSIONS: ${{ (github.event_name == 'workflow_dispatch' && inputs.python_versions != '' && inputs.python_versions) || (github.event_name == 'workflow_dispatch' && inputs.mode == 'dual-runtime' && '["3.11","3.12"]') || '["3.11"]' }}
      RUN_REASON: ${{ (github.event_name == 'workflow_dispatch' && inputs.reason != '' && inputs.reason) || (github.event_name == 'workflow_dispatch' && inputs.reason == '' && 'manual test') || 'nightly verification' }}
      TRIGGER_EVENT: ${{ github.event_name }}
    steps:
      - name: Summarize matrix execution
        run: |
          {
            echo "## Self-Test Matrix Summary"
            echo "Workflow result: ${{ needs.scenario.result }}"
            echo "Triggered via: ${TRIGGER_EVENT}"
            if [ -n "${RUN_REASON}" ]; then
              echo "Run reason: ${RUN_REASON}"
            fi
            echo "Requested python-versions: ${PYTHON_VERSIONS}"
            echo "Scenarios: ${SCENARIO_LIST}"
          } >> "$GITHUB_STEP_SUMMARY"

      - name: Capture run metadata
        id: run_metadata
        run: |
          echo "run_id=${GITHUB_RUN_ID}" >> "$GITHUB_OUTPUT"

      - name: Verify artifact expectations
        id: verify
        uses: actions/github-script@v7
        env:
          PYTHON_VERSIONS: ${{ env.PYTHON_VERSIONS }}
          SCENARIO_LIST: ${{ env.SCENARIO_LIST }}
        with:
          script: |
            const defaultPythonVersions = ['3.11'];
            let pythonVersions = defaultPythonVersions;
            const rawPythonVersions = process.env.PYTHON_VERSIONS;

            if (rawPythonVersions && rawPythonVersions.trim()) {
              try {
                const parsed = JSON.parse(rawPythonVersions.trim());
                if (Array.isArray(parsed) && parsed.length) {
                  pythonVersions = parsed.map(String);
                } else {
                  core.warning(`Parsed python_versions input is empty; falling back to ${defaultPythonVersions.join(', ')}`);
                }
              } catch (error) {
                core.warning(`Unable to parse python_versions input (${rawPythonVersions}); using ${defaultPythonVersions.join(', ')}. Error: ${error}`);
              }
            }

            const scenarios = process.env.SCENARIO_LIST.split(',').map((name) => name.trim()).filter(Boolean);
            const scenarioDefinitions = scenarios.map((name) => {
              switch (name) {
                case 'minimal':
                  return { name, metrics:false, history:false, classification:false, covDelta:false, soft:false };
                case 'metrics_only':
                  return { name, metrics:true, history:false, classification:false, covDelta:false, soft:false };
                case 'metrics_history':
                  return { name, metrics:true, history:true, classification:false, covDelta:false, soft:false };
                case 'classification_only':
                  return { name, metrics:false, history:false, classification:true, covDelta:false, soft:false };
                case 'coverage_delta':
                  return { name, metrics:false, history:false, classification:false, covDelta:true, soft:false };
                case 'full_soft_gate':
                  return { name, metrics:true, history:true, classification:true, covDelta:true, soft:true };
                default:
                  core.warning(`Unknown scenario '${name}' discovered; treating as minimal.`);
                  return { name, metrics:false, history:false, classification:false, covDelta:false, soft:false };
              }
            });

            const run_id = context.runId;
            const { owner, repo } = context.repo;

            async function listArtifacts() {
              const collected = [];
              let page = 1;
              while (true) {
                const resp = await github.rest.actions.listWorkflowRunArtifacts({ owner, repo, run_id, per_page: 100, page });
                const batch = resp.data.artifacts || [];
                collected.push(...batch);
                if (batch.length < 100) {
                  break;
                }
                page += 1;
              }
              return collected;
            }

            const artifacts = await listArtifacts();
            const names = new Set(artifacts.map((artifact) => artifact.name));

            function expectedFor(s) {
              const prefix = (suffix) => `sf-${s.name}-${suffix}`;
              const expected = [];

              pythonVersions.forEach((version) => {
                expected.push(prefix(`coverage-${version}`));
              });

              if (s.metrics) expected.push(prefix('ci-metrics'));
              if (s.history) expected.push(prefix('metrics-history'));
              if (s.classification) expected.push(prefix('classification'));
              if (s.covDelta) expected.push(prefix('coverage-delta'));
              if (s.soft) {
                expected.push(prefix('coverage-summary'));
                expected.push(prefix('coverage-trend'));
                expected.push(prefix('coverage-trend-history'));
              }
              return expected;
            }

            const expectedGlobal = new Set();
            scenarioDefinitions.forEach((scenario) => expectedFor(scenario).forEach((name) => expectedGlobal.add(name)));

            const rows = ['| Scenario | Expected Artifacts | Missing | Unexpected Present | Status |', '|---|---|---|---|---|'];
            const report = [];
            let failures = 0;

            for (const scenario of scenarioDefinitions) {
              const expected = expectedFor(scenario);
              const missing = expected.filter((name) => !names.has(name));
              const prefix = `sf-${scenario.name}-`;
              const actual = [...names].filter((name) => name.startsWith(prefix));
              const unexpected = actual.filter((name) => !expected.includes(name));
              const ok = missing.length === 0 && unexpected.length === 0;
              if (!ok) {
                failures += 1;
              }
              rows.push(`| ${scenario.name} | ${expected.join('<br>')} | ${missing.join('<br>') || '—'} | ${unexpected.join('<br>') || '—'} | ${ok ? '✅' : '❌'} |`);
              report.push({ scenario: scenario.name, expected, missing, unexpected, ok });
            }

            const stray = [...names].filter((name) => name.startsWith('sf-') && !expectedGlobal.has(name));
            if (stray.length) {
              rows.push(`| (stray) | (n/a) | (n/a) | ${stray.join('<br>')} | ❌ |`);
              report.push({ scenario: '_stray_', expected: [], missing: [], unexpected: stray, ok: false });
              failures += 1;
            }

            core.setOutput('table', rows.join('\n'));
            core.setOutput('failures', String(failures));
            const summary = { run_id, artifact_count: artifacts.length, failures, scenarios: report };
            require('fs').writeFileSync('selftest-report.json', JSON.stringify(summary, null, 2));

      - name: Append verification table
        run: |
          {
            echo "### Artifact Verification"
            echo "${{ steps.verify.outputs.table }}"
          } >> "$GITHUB_STEP_SUMMARY"

      - name: Upload self-test report
        uses: actions/upload-artifact@v4
        with:
          name: selftest-report
          path: selftest-report.json

      - name: Fail on verification errors
        if: ${{ steps.verify.outputs.failures != '0' }}
        run: |
          echo "Artifact expectation mismatches detected." >&2
          exit 1

  publish-results:
    name: Publish Results
    needs:
      - scenario
      - aggregate
    if: ${{ always() }}
    runs-on: ubuntu-latest
    permissions:
      contents: read
      actions: read
      pull-requests: write
    env:
      MODE: ${{ (github.event_name == 'workflow_dispatch' && inputs.mode) || 'summary' }}
      POST_TO: ${{ (github.event_name == 'workflow_dispatch' && inputs.post_to) || 'none' }}
      ENABLE_HISTORY: ${{ (github.event_name == 'workflow_dispatch' && inputs.enable_history) || 'false' }}
      PR_NUMBER: ${{ (github.event_name == 'workflow_dispatch' && inputs.pull_request_number) || '' }}
      SUMMARY_TITLE: ${{ (github.event_name == 'workflow_dispatch' && inputs.summary_title != '' && inputs.summary_title) || "Selftest Reusables Summary" }}
      COMMENT_TITLE: ${{ (github.event_name == 'workflow_dispatch' && inputs.comment_title != '' && inputs.comment_title) || "Selftest Reusables Comment" }}
      REASON: ${{ (github.event_name == 'workflow_dispatch' && inputs.reason != '' && inputs.reason) || (github.event_name == 'workflow_dispatch' && inputs.reason == '' && 'manual test') || 'nightly verification' }}
      WORKFLOW_RESULT: ${{ needs.scenario.result }}
      VERIFICATION_TABLE: ${{ needs.aggregate.outputs.verification_table }}
      FAILURE_COUNT: ${{ needs.aggregate.outputs.failures }}
      RUN_ID: ${{ needs.aggregate.outputs.run_id }}
      REQUESTED_VERSIONS: ${{ (github.event_name == 'workflow_dispatch' && inputs.python_versions != '' && inputs.python_versions) || (github.event_name == 'workflow_dispatch' && inputs.mode == 'dual-runtime' && '["3.11","3.12"]') || '["3.11"]' }}
    steps:
      - name: Validate pull request target
        if: ${{ env.MODE == 'comment' && env.POST_TO == 'pr-number' }}
        run: |
          if [ -z "${PR_NUMBER}" ]; then
            echo '::error::pull_request_number is required when posting to a pull request.'
            exit 1
          fi
          case "${PR_NUMBER}" in
            (*[!0-9]*)
              echo '::error::pull_request_number must be an integer.'
              exit 1
              ;;
          esac

      - name: Download self-test report
        if: ${{ env.ENABLE_HISTORY == 'true' && env.RUN_ID != '' }}
        uses: actions/download-artifact@v4
        with:
          run-id: ${{ env.RUN_ID }}
          name: selftest-report
          path: selftest-report
        continue-on-error: true

      - name: Append workflow summary
        env:
          TABLE: ${{ env.VERIFICATION_TABLE }}
          FAILURES: ${{ env.FAILURE_COUNT }}
        run: |
          TABLE="${TABLE:-}"
          if [ -z "${TABLE}" ]; then
            TABLE='*Verification table not available.*'
          fi

          FAILURES="${FAILURES:-}"
          if [ -z "${FAILURES}" ]; then
            FAILURES='unknown'
          fi

          {
            echo "## ${SUMMARY_TITLE}"
            echo "Workflow status: ${WORKFLOW_RESULT}"
            echo "Mode: ${MODE}"
            if [ -n "${REASON}" ]; then
              echo "Dispatch reason: ${REASON}"
            fi
            echo "Requested python-versions: ${REQUESTED_VERSIONS}"
            echo ""
            printf '%s\n' "${TABLE}"
            echo ""
            echo "Failures reported: ${FAILURES}"
          } >> "$GITHUB_STEP_SUMMARY"

      - name: Surface failures in logs
        if: ${{ env.MODE != 'comment' }}
        run: |
          if [ -z "${VERIFICATION_TABLE}" ]; then
            echo "::error::Verification table output missing from selftest-reusable-ci aggregate job." >&2
            exit 1
          fi

          if [ -z "${FAILURE_COUNT}" ]; then
            echo "::error::Failure count output missing from selftest-reusable-ci aggregate job." >&2
            exit 1
          fi

          if [ "${FAILURE_COUNT}" != "0" ]; then
            echo "Selftest: Reusables reported ${FAILURE_COUNT} mismatch(es)." >&2
            exit 1
          fi

          if [ "${WORKFLOW_RESULT}" != "success" ]; then
            echo "Selftest matrix completed with status: ${WORKFLOW_RESULT}." >&2
            exit 1
          fi

      - name: Publish PR comment
        if: ${{ env.MODE == 'comment' && env.POST_TO == 'pr-number' }}
        uses: actions/github-script@v7
        env:
          MARKER: '<!-- selftest-reusable-ci-comment -->'
          TABLE: ${{ env.VERIFICATION_TABLE }}
          FAILURES: ${{ env.FAILURE_COUNT }}
          RESULT: ${{ env.WORKFLOW_RESULT }}
          COMMENT_TITLE: ${{ env.COMMENT_TITLE }}
          REASON: ${{ env.REASON }}
          REQUESTED_VERSIONS: ${{ env.REQUESTED_VERSIONS }}
          MODE: ${{ env.MODE }}
          PR_NUMBER: ${{ env.PR_NUMBER }}
        with:
          script: |
            const marker = process.env.MARKER || '<!-- selftest-reusable-ci-comment -->';
            const prRaw = (process.env.PR_NUMBER || '').trim();
            const prNumber = Number.parseInt(prRaw, 10);
            if (!Number.isInteger(prNumber)) {
              core.setFailed(`Invalid pull_request_number input: ${prRaw}`);
              return;
            }

            const mode = (process.env.MODE || 'comment').trim();
            const title = (process.env.COMMENT_TITLE || 'Selftest: Reusables Comment').trim() || 'Selftest: Reusables Comment';
            const reason = (process.env.REASON || '').trim();
            const table = process.env.TABLE && process.env.TABLE.trim() ? process.env.TABLE : '*No verification table available.*';
            const failureRaw = process.env.FAILURES;
            const failures = typeof failureRaw === 'string' && failureRaw.trim().length > 0
              ? failureRaw.trim()
              : 'unknown';
            const runResult = process.env.RESULT || 'unknown';
            const requestedVersions = (process.env.REQUESTED_VERSIONS || '').trim();

            const statusLine = failures === '0'
              ? '✅ **Self-test scenarios succeeded.**'
              : failures === 'unknown'
                ? '⚠️ **Self-test outcome unknown (verification outputs missing).**'
                : `❌ **Self-test reported ${failures} mismatch(es).**`;

            const runBadge = runResult === 'success'
              ? '✅'
              : runResult === 'failure'
                ? '❌'
                : 'ℹ️';

            const lines = [
              marker,
              `### ${title}`,
              `${runBadge} Workflow status: **${runResult}**`,
              statusLine,
              `Mode: \`${mode}\``,
              requestedVersions ? `Requested python-versions: ${requestedVersions}` : undefined,
              reason ? `Dispatch reason: ${reason}` : undefined,
              '',
              table,
              '',
              `_Workflow run: ${context.runId}_`,
            ].filter(Boolean);

            const body = lines.join('\n');

            const { owner, repo } = context.repo;
            const comments = await github.rest.issues.listComments({ owner, repo, issue_number: prNumber, per_page: 100 });
            const existing = comments.data.find((comment) => comment.body && comment.body.includes(marker));

            if (existing) {
              await github.rest.issues.updateComment({ owner, repo, comment_id: existing.id, body });
              core.info(`Updated existing self-test comment (${existing.id}).`);
            } else {
              const created = await github.rest.issues.createComment({ owner, repo, issue_number: prNumber, body });
              core.info(`Created new self-test comment (${created.data.id}).`);
            }

      - name: Finalize status for comment mode
        if: ${{ env.MODE == 'comment' }}
        run: |
          if [ -z "${VERIFICATION_TABLE}" ]; then
            echo "::error::Verification table output missing from selftest-reusable-ci aggregate job." >&2
            exit 1
          fi

          if [ -z "${FAILURE_COUNT}" ]; then
            echo "::error::Failure count output missing from selftest-reusable-ci aggregate job." >&2
            exit 1
          fi

          if [ "${FAILURE_COUNT}" != "0" ]; then
            echo "Selftest: Reusables reported ${FAILURE_COUNT} mismatch(es)." >&2
            exit 1
          fi

          if [ "${WORKFLOW_RESULT}" != "success" ]; then
            echo "Selftest matrix completed with status: ${WORKFLOW_RESULT}." >&2
            exit 1
          fi
