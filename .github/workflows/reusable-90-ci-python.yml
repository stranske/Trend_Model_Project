name: Reusable 90 CI Python

# Clean rebuilt reusable workflow with debug instrumentation retained from strict-lint branch.
# Inputs are feature flags controlling optional artifacts and gates.

on:
  workflow_call:
    inputs:
      python-versions:
        description: JSON list of Python versions
        required: false
        default: '["3.11"]'
        type: string
      coverage-min:
        description: Minimum coverage percent (hard gate)
        required: false
        default: '70'
        type: string
      run-mypy:
        description: Run mypy job
        required: false
        default: true
        type: boolean
      enable-metrics:
        description: Emit ci-metrics.json (test metrics)
        required: false
        default: false
        type: boolean
      enable-history:
        description: Append metrics history artifact
        required: false
        default: false
        type: boolean
      history-artifact-name:
        description: Metrics history artifact filename
        required: false
        default: metrics-history.ndjson
        type: string
      enable-classification:
        description: Emit failure classification
        required: false
        default: false
        type: boolean
      enable-coverage-delta:
        description: Compute coverage delta vs baseline
        required: false
        default: false
        type: boolean
      baseline-coverage:
        description: Baseline coverage percent for delta
        required: false
        default: '0'
        type: string
      coverage-alert-drop:
        description: Coverage drop alert threshold (pct pts)
        required: false
        default: '1'
        type: string
      fail-on-coverage-drop:
        description: Fail if drop >= threshold
        required: false
        default: 'false'
        type: string
      coverage-drop-label:
        description: Optional label to apply when coverage drops
        required: false
        default: coverage-drop
        type: string
      enable-soft-gate:
        description: Enable soft coverage gate (aggregate + issue update)
        required: false
        default: false
        type: boolean
      coverage-hard-fail:
        description: Fail tests job if coverage < min
        required: false
        default: true
        type: boolean
      coverage-artifact-retention-days:
        description: Retention days for coverage artifacts
        required: false
        default: '10'
        type: string
      low-coverage-threshold:
        description: Low coverage hotspot threshold (%)
        required: false
        default: '50'
        type: string
      slow-test-top:
        description: Top N slow tests to report
        required: false
        default: '15'
        type: string
      slow-test-min-seconds:
        description: Minimum seconds for a test to qualify as slow
        required: false
        default: '1'
        type: string
      artifact-prefix:
        description: Prefix applied to uploaded artifact names
        required: false
        default: ''
        type: string

jobs:
  tests:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ${{ fromJson(inputs.python-versions) }}
        include:
          - python-version: '3.11'
            scenario: base
          - python-version: '3.11'
            scenario: metrics
            enable-metrics: true
          - python-version: '3.11'
            scenario: metrics-history
            enable-metrics: true
            enable-history: true
          - python-version: '3.11'
            scenario: classification
            enable-classification: true
          - python-version: '3.11'
            scenario: cov-delta
            enable-coverage-delta: true
            baseline-coverage: ${{ inputs.baseline-coverage }}
            enable-soft-gate: true
    continue-on-error: false
    env:
      ENABLE_METRICS_FLAG: ${{ matrix.enable-metrics || inputs.enable-metrics }}
      ENABLE_HISTORY_FLAG: ${{ matrix.enable-history || inputs.enable-history }}
      ENABLE_CLASSIFICATION_FLAG: ${{ matrix.enable-classification || inputs.enable-classification }}
      ENABLE_COV_DELTA_FLAG: ${{ matrix.enable-coverage-delta || inputs.enable-coverage-delta }}
      ENABLE_SOFT_GATE_FLAG: ${{ matrix.enable-soft-gate || inputs.enable-soft-gate }}
      COVERAGE_HARD_FAIL_FLAG: ${{ inputs.coverage-hard-fail }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
      - name: Install dependencies
        run: |
          python -m venv .venv
          echo "${{ github.workspace }}/.venv/bin" >> "$GITHUB_PATH"
          echo "VIRTUAL_ENV=${{ github.workspace }}/.venv" >> "$GITHUB_ENV"
          source .venv/bin/activate
          pip install -U pip
          pip install -r requirements.txt
          pip install -e '.[dev]'
          pip install pytest pytest-cov jq
      - name: Run tests with coverage
        run: |
          pytest --junitxml=pytest-junit.xml \
            --cov=src --cov-branch \
            --cov-report=xml:coverage.xml \
            --cov-report=json:coverage.json \
            --cov-report=html:htmlcov \
            --cov-report=term-missing
      - name: Prepare coverage artifacts
        run: |
          cp -f pytest-junit.xml pytest-report.xml || true
          mv coverage.json coverage-${{ matrix.python-version }}.json
          test -f coverage-${{ matrix.python-version }}.json
          test -f coverage.xml
      - name: Upload coverage artifacts
        uses: actions/upload-artifact@v4
        with:
          name: ${{ inputs.artifact-prefix }}coverage-${{ matrix.python-version }}
          retention-days: ${{ inputs.coverage-artifact-retention-days }}
          path: |
            coverage.xml
            coverage-${{ matrix.python-version }}.json
            htmlcov/**
            pytest-junit.xml
            pytest-report.xml
      - name: Extract test metrics
        if: ${{ env.ENABLE_METRICS_FLAG == 'true' }}
        run: python scripts/ci_metrics.py
        env:
          JUNIT_PATH: pytest-junit.xml
          OUTPUT_PATH: ci-metrics.json
          TOP_N: ${{ inputs.slow-test-top }}
          MIN_SECONDS: ${{ inputs.slow-test-min-seconds }}
      - name: Upload metrics artifact
        if: ${{ env.ENABLE_METRICS_FLAG == 'true' }}
        uses: actions/upload-artifact@v4
        with:
          name: ${{ inputs.artifact-prefix }}ci-metrics
          path: ci-metrics.json
      - name: Enforce coverage minimum (hard gate)
        if: ${{ env.COVERAGE_HARD_FAIL_FLAG == 'true' }}
        run: |
          RAW=$(grep -Eo 'line-rate="[0-9.]+"' coverage.xml | head -1 | sed -E 's/.*"([0-9.]+)"/\1/')
          PCT=$(python -c "print(float('$RAW')*100.0)")
          echo "Coverage: ${PCT}% (min ${{ inputs.coverage-min }}%)"
          python -c "import sys; cur=float('$PCT'); m=float('${{ inputs.coverage-min }}'); sys.exit(0 if cur>=m else 1)" || { echo 'Coverage below minimum' >&2; exit 1; }
      - name: Coverage delta
        if: ${{ env.ENABLE_COV_DELTA_FLAG == 'true' }}
        run: python scripts/ci_coverage_delta.py
        env:
          BASELINE_COVERAGE: ${{ inputs.baseline-coverage }}
          ALERT_DROP: ${{ inputs.coverage-alert-drop }}
          FAIL_ON_DROP: ${{ inputs.fail-on-coverage-drop }}
      - name: Upload coverage delta
        if: ${{ env.ENABLE_COV_DELTA_FLAG == 'true' }}
        uses: actions/upload-artifact@v4
        with:
          name: ${{ inputs.artifact-prefix }}coverage-delta
          path: coverage-delta.json
      - name: History & classification
        if: ${{ env.ENABLE_HISTORY_FLAG == 'true' || env.ENABLE_CLASSIFICATION_FLAG == 'true' }}
        run: python scripts/ci_history.py
        env:
          JUNIT_PATH: pytest-junit.xml
          METRICS_PATH: ci-metrics.json
          HISTORY_PATH: ${{ inputs.history-artifact-name }}
          CLASSIFICATION_OUT: classification.json
      - name: Upload history artifact
        if: ${{ env.ENABLE_HISTORY_FLAG == 'true' }}
        uses: actions/upload-artifact@v4
        with:
          name: ${{ inputs.artifact-prefix }}metrics-history
          path: ${{ inputs.history-artifact-name }}
      - name: Upload classification artifact
        if: ${{ env.ENABLE_CLASSIFICATION_FLAG == 'true' }}
        uses: actions/upload-artifact@v4
        with:
          name: ${{ inputs.artifact-prefix }}classification
          path: classification.json
      - name: CI summary
        run: |
          {
            echo "## CI Summary"
            if [ -f ci-metrics.json ]; then
              echo "- Metrics: present"
            else
              echo "- Metrics: (disabled)"
            fi
            if [ -f coverage-delta.json ]; then
              CUR=$(jq -r .current coverage-delta.json 2>/dev/null || echo '?')
              BASE=$(jq -r .baseline coverage-delta.json 2>/dev/null || echo '?')
              DELTA=$(jq -r .delta coverage-delta.json 2>/dev/null || echo '?')
              echo "- Coverage delta: current=$CUR baseline=$BASE delta=$DELTA"
            else
              echo "- Coverage delta: (disabled)"
            fi
            if [ -f classification.json ]; then
              echo "- Classification: present"
            else
              echo "- Classification: (disabled)"
            fi
            if [ -f "${{ inputs.history-artifact-name }}" ]; then
              echo "- History: appended"
            else
              echo "- History: (disabled)"
            fi
            if [ "$ENABLE_SOFT_GATE_FLAG" = "true" ]; then
              echo "- Soft gate: enabled (see coverage_soft_gate job)"
            else
              echo "- Soft gate: (disabled)"
            fi
          } >> "$GITHUB_STEP_SUMMARY"
      - name: Assert feature outputs
        run: python scripts/ci_feature_assert.py
        env:
          EXPECT_METRICS: ${{ env.ENABLE_METRICS_FLAG }}
          EXPECT_HISTORY: ${{ env.ENABLE_HISTORY_FLAG }}
          EXPECT_CLASSIFICATION: ${{ env.ENABLE_CLASSIFICATION_FLAG }}
          EXPECT_COV_DELTA: ${{ env.ENABLE_COV_DELTA_FLAG }}
          HISTORY_ARTIFACT_NAME: ${{ inputs.history-artifact-name }}

  mypy:
    if: ${{ inputs.run-mypy }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Load formatter pins
        shell: bash
        run: |
          set -euo pipefail
          pin_file=".github/workflows/autofix-versions.env"
          if [[ ! -f "${pin_file}" ]]; then
            echo "Missing ${pin_file}; aborting." >&2
            exit 1
          fi
          # shellcheck disable=SC1090
          source "${pin_file}"
          if [[ -z "${MYPY_VERSION:-}" ]]; then
            echo "${pin_file} is missing a value for MYPY_VERSION" >&2
            exit 1
          fi
          cat "${pin_file}" >> "${GITHUB_ENV}"
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Install mypy
        run: |
          python -m venv .venv
          echo "${{ github.workspace }}/.venv/bin" >> "$GITHUB_PATH"
          echo "VIRTUAL_ENV=${{ github.workspace }}/.venv" >> "$GITHUB_ENV"
          source .venv/bin/activate
          pip install -U pip
          pip install -r requirements.txt
          pip install "mypy==${MYPY_VERSION}"
      - name: Run mypy
        run: mypy src || (echo 'mypy failed' && exit 1)

  coverage_soft_gate:
    if: ${{ always() && inputs.enable-soft-gate }}
    needs: tests
    runs-on: ubuntu-latest
    outputs:
      has_failures: ${{ steps.classify.outputs.has_failures }}
      only_cosmetic: ${{ steps.classify.outputs.only_cosmetic }}
      summary_b64: ${{ steps.classify.outputs.summary_b64 }}
    permissions:
      contents: read
      issues: write
      actions: read
    env:
      PYTHONPATH: ${{ github.workspace }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
      - name: Download coverage artifacts
        continue-on-error: true
        uses: actions/download-artifact@v4
        with:
          pattern: ${{ inputs.artifact-prefix }}coverage-*
          merge-multiple: true
      - name: Debug coverage artifact contents (Issue #1386)
        run: |
          echo '== Top-level listing =='
          ls -al . || true
          echo ''
          echo '== Find coverage json candidates =='
          find . -maxdepth 2 -type f -name 'coverage*.json' -print || true
          echo -e '\n== Show first 40 lines of each coverage json =='
          for f in $(find . -maxdepth 2 -type f -name 'coverage*.json' | head -5); do
            echo "--- $f ---";
            head -40 "$f" || true;
          done
      - name: Classify test outcomes
        id: classify
        shell: python
        run: |
          import base64
          import glob
          import json
          import os

          from scripts.classify_test_failures import classify_reports

          paths = sorted(
              glob.glob('pytest-report*.xml')
              + glob.glob('**/pytest-report*.xml', recursive=True)
          )
          summary = classify_reports(paths)
          with open('test-failure-summary.json', 'w', encoding='utf-8') as handle:
              json.dump(summary, handle, indent=2)

          if summary.get('has_failures'):
              print('Detected failing tests:')
              for bucket in ('runtime', 'cosmetic', 'unknown'):
                  entries = summary.get(bucket, []) or []
                  if not entries:
                      continue
                  print(f"  {bucket}: {len(entries)}")
                  for entry in entries:
                      markers = entry.get('markers', []) or []
                      joined = ', '.join(markers) if markers else 'unmarked'
                      print(f"    - {entry['id']} ({joined})")
          else:
              print('No failing tests detected across JUnit reports.')

          encoded = base64.b64encode(json.dumps(summary).encode('utf-8')).decode('ascii')
          out_path = os.environ.get('GITHUB_OUTPUT')
          if out_path:
              with open(out_path, 'a', encoding='utf-8') as fh:
                  fh.write(f"has_failures={'true' if summary.get('has_failures') else 'false'}\n")
                  fh.write(f"only_cosmetic={'true' if summary.get('only_cosmetic') else 'false'}\n")
                  fh.write(f"summary_b64={encoded}\n")
      - name: Compute coverage summary & hotspots
        id: cov
        shell: python
        env:
          LOW_COVERAGE_THRESHOLD: ${{ inputs.low-coverage-threshold }}
        run: |
          import glob, json, os, re, sys

          HOTSPOT_LIMIT = 15
          try:
              LOW_COVERAGE_THRESHOLD = float(os.environ.get('LOW_COVERAGE_THRESHOLD', '50.0'))
          except ValueError:
              LOW_COVERAGE_THRESHOLD = 50.0

          candidates = sorted(set(glob.glob('coverage-*.json') + glob.glob('**/coverage.json', recursive=True)))
          seen=set(); ordered=[]
          for c in candidates:
              p=os.path.relpath(c)
              if p not in seen:
                  seen.add(p)
                  ordered.append(p)

          if not ordered:
              msg = 'No coverage JSON files found (Issue #1386 diagnostics)'
              print('[warn]', msg)
              with open('coverage_summary.md','w') as w:
                  w.write('**Coverage data unavailable  see debug step output.**\n')
              print('::notice:: '+msg)
              sys.exit(0)

          totals=[]; hotspots=[]
          for jf in ordered:
              try:
                  with open(jf,'r') as fh:
                      data=json.load(fh)
              except Exception as e:
                  print(f'[warn] Failed to parse {jf}: {e}')
                  continue
              t=data.get('totals', {}) or {}
              pct = t.get('percent_covered')
              if pct is None:
                  disp = t.get('percent_covered_display')
                  try:
                      pct = float(disp) if disp is not None else 0.0
                  except Exception:
                      pct = 0.0
              totals.append(float(pct))
              for fpath, meta in (data.get('files') or {}).items():
                  s=meta.get('summary', {}) or {}
                  fpct = s.get('percent_covered')
                  if fpct is None:
                      disp=s.get('percent_covered_display')
                      try:
                          fpct=float(disp) if disp is not None else 0.0
                      except Exception:
                          fpct=0.0
                  hotspots.append((float(fpct), fpath))

          hotspots.sort(key=lambda x: x[0])
          avg = round(sum(totals)/len(totals), 2) if totals else 0.0
          worst = round(min(totals), 2) if totals else 0.0
          lines=[f"**Coverage (avg across jobs): {avg}% | worst job: {worst}%**", "", "| File | % covered |", "|---|---:|"]
          for fpct,f in hotspots[:HOTSPOT_LIMIT]:
              lines.append(f"| `{f}` | {fpct:.1f}% |")
          if len(lines) == 4:
              lines.append('| *(no files parsed)* | *(n/a)* |')
          low_cov=[(p,f) for p,f in hotspots if p < LOW_COVERAGE_THRESHOLD]
          if low_cov:
              lines += ['', f"#### Low Coverage (<{LOW_COVERAGE_THRESHOLD:.0f}% )", "| File | % covered |", "|---|---:|"]
              for p,f in low_cov[:HOTSPOT_LIMIT]:
                  lines.append(f"| `{f}` | {p:.1f}% |")
              if len(low_cov) > HOTSPOT_LIMIT:
                  lines.append(f"| *(+{len(low_cov)-HOTSPOT_LIMIT} more below {LOW_COVERAGE_THRESHOLD:.0f}%  truncated)* | *(n/a)* |")
          with open('coverage_summary.md','w') as w:
              w.write('\n'.join(lines)+'\n')
          first_line = lines[0]
          match = re.search(r'avg across jobs\):\s*([0-9.]+)% \| worst job: ([0-9.]+)%', first_line)
          if match:
              print(first_line)
          else:
              print('[warn] Unable to parse coverage headline from summary header:', first_line)
      - name: Upload coverage summary artifact
        uses: actions/upload-artifact@v4
        with:
          name: ${{ inputs.artifact-prefix }}coverage-summary
          retention-days: ${{ inputs.coverage-artifact-retention-days }}
          path: coverage_summary.md
      - name: Build job log links table
        id: jobs
        uses: actions/github-script@v7
        with:
          script: |
            const {owner, repo} = context.repo;
            const run_id = context.runId;
            const { data } = await github.rest.actions.listJobsForWorkflowRun({ owner, repo, run_id, per_page: 100 });
            const rows = data.jobs.map(j => `| ${j.name} | ${j.conclusion || j.status} | [logs](${j.html_url}) |`);
            const md = ["### Logs for this run","| Job | Result | Logs |","|---|---|---|",...rows].join("\n");
            core.setOutput('table', md);
            const failed = data.jobs.filter(j => !['success','skipped'].includes((j.conclusion || '').toLowerCase())).map(j => j.name);
            core.setOutput('failed', failed.join(', '));
      - name: Create / update soft coverage issue
        uses: actions/github-script@v7
        env:
          TABLE: ${{ steps.jobs.outputs.table }}
        with:
          script: |
            const fs = require('fs');
            const {owner, repo} = context.repo;
            const title = 'Increase test coverage (soft gate)';
            const label = 'tech:coverage';
            const bodyBlock = fs.readFileSync('coverage_summary.md','utf8');
            const jobTable = process.env.TABLE || '';
            const runUrl = `https://github.com/${owner}/${repo}/actions/runs/${context.runId}`;
            try { await github.rest.issues.getLabel({owner, repo, name: label}); } catch { await github.rest.issues.createLabel({owner, repo, name: label, color: '0e8a16'}); }
            const q = `repo:${owner}/${repo} is:issue is:open in:title "${title}" label:"${label}"`;
            const search = await github.rest.search.issuesAndPullRequests({ q, per_page: 1 });
            const compositeBody = `CI run: ${runUrl}\n\n${bodyBlock}\n\n#### Job logs\n${jobTable}`;
            if (search.data.items.length) {
              const num = search.data.items[0].number;
              await github.rest.issues.update({owner, repo, issue_number: num, title});
              await github.rest.issues.createComment({owner, repo, issue_number: num, body: compositeBody});
            } else {
              await github.rest.issues.create({owner, repo, title, body: compositeBody, labels:[label]});
            }
      - name: Append coverage summary to run summary
        run: |
          {
            echo "## Soft Coverage Gate"
            cat coverage_summary.md
            echo
            echo "${{ steps.jobs.outputs.table }}"
            echo
          } >> "$GITHUB_STEP_SUMMARY"
      - name: Emit coverage trend record
        shell: python
        run: |
          import datetime, json, os, re
          summary_path='coverage_summary.md'
          avg=worst=None
          try:
            with open(summary_path,'r') as fh:
              first=fh.readline().strip()
            m=re.search(r'avg across jobs\):\s*([0-9.]+)% \| worst job: ([0-9.]+)%', first)
            if m:
              avg=float(m.group(1))
              worst=float(m.group(2))
          except Exception:
            pass
          rec={
            'run_id': int(os.environ.get('GITHUB_RUN_ID','0')),
            'run_number': int(os.environ.get('GITHUB_RUN_NUMBER','0')),
            'repo': os.environ.get('GITHUB_REPOSITORY'),
            'sha': os.environ.get('GITHUB_SHA'),
            'ref': os.environ.get('GITHUB_REF'),
            'avg_coverage': avg,
            'worst_job_coverage': worst,
            'timestamp_utc': datetime.datetime.utcnow().isoformat()+'Z'
          }
          with open('coverage-trend.json','w') as w:
            json.dump(rec,w,indent=2,sort_keys=True)
          print('Wrote coverage-trend.json')
      - name: Upload coverage trend artifact
        uses: actions/upload-artifact@v4
        with:
          name: ${{ inputs.artifact-prefix }}coverage-trend
          retention-days: ${{ inputs.coverage-artifact-retention-days }}
          path: coverage-trend.json
      - name: Download existing coverage trend history (if any)
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: ${{ inputs.artifact-prefix }}coverage-trend-history
          path: coverage-history-pre
      - name: Merge coverage trend history
        shell: bash
        run: |
          set -euo pipefail
          HIST=coverage-trend-history.ndjson
          if [ -d coverage-history-pre ]; then
            if [ -f coverage-history-pre/coverage-trend-history.ndjson ]; then
              cp coverage-history-pre/coverage-trend-history.ndjson "$HIST"
            fi
          fi
          python scripts/coverage_history_append.py || echo "[history] append failed" >&2
          ls -l "$HIST" || true
      - name: Upload coverage trend history artifact
        uses: actions/upload-artifact@v4
        with:
          name: ${{ inputs.artifact-prefix }}coverage-trend-history
          retention-days: ${{ inputs.coverage-artifact-retention-days }}
          path: coverage-trend-history.ndjson

  cosmetic_followup:
    name: cosmetic failure triage
    runs-on: ubuntu-latest
    needs: [tests, coverage_soft_gate]
    if: ${{ needs.tests.result != 'success' && needs.coverage_soft_gate.outputs.has_failures == 'true' && needs.coverage_soft_gate.outputs.only_cosmetic == 'true' }}
    permissions:
      contents: read
      pull-requests: write
      actions: read
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
      - name: Run cosmetic fixers
        run: |
          python scripts/auto_type_hygiene.py
          python scripts/fix_cosmetic_aggregate.py
      - name: Summarise cosmetic failures
        id: cosmetic
        shell: python
        env:
          SUMMARY_B64: ${{ needs.coverage_soft_gate.outputs.summary_b64 }}
        run: |
          import base64
          import json
          import os

          summary_data = {}
          payload = os.environ.get('SUMMARY_B64')
          if payload:
              summary_data = json.loads(base64.b64decode(payload).decode('utf-8'))
          cosmetic = summary_data.get('cosmetic', []) if isinstance(summary_data, dict) else []
          lines = []
          for entry in cosmetic:
              markers = entry.get('markers') or []
              marker_text = f" ({', '.join(markers)})" if markers else ''
              message = entry.get('message', '')
              if message:
                  message = message.splitlines()[0]
                  message = f"  {message}"
              lines.append(f"- `{entry.get('id', '<unknown>')}`{marker_text}{message}")

          if not lines:
              lines.append('- No detailed cosmetic failures captured.')

          with open('cosmetic_failures.md', 'w', encoding='utf-8') as fh:
              fh.write("\n".join(lines) + "\n")

          out = os.environ.get('GITHUB_OUTPUT')
          if out:
              with open(out, 'a', encoding='utf-8') as handle:
                  handle.write(f"has_details={'true' if cosmetic else 'false'}\n")
      - name: Capture fixer diff
        id: patch
        shell: bash
        run: |
          set -euo pipefail
          if git diff --quiet; then
            echo "has_patch=false" >> "$GITHUB_OUTPUT"
          else
            git diff > cosmetic-fixes.patch
            echo "has_patch=true" >> "$GITHUB_OUTPUT"
          fi
      - name: Upload cosmetic patch
        if: steps.patch.outputs.has_patch == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: cosmetic-fixes
          path: cosmetic-fixes.patch
          retention-days: 5
      - name: Post advisory comment for cosmetic failures
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        env:
          SUMMARY_B64: ${{ needs.coverage_soft_gate.outputs.summary_b64 }}
          HAS_PATCH: ${{ steps.patch.outputs.has_patch }}
        with:
          script: |
            const summaryPayload = process.env.SUMMARY_B64 || '';
            if (!summaryPayload) {
              core.info('No summary payload found; skipping cosmetic comment.');
              return;
            }
            const summary = JSON.parse(Buffer.from(summaryPayload, 'base64').toString('utf8'));
            const cosmetic = Array.isArray(summary.cosmetic) ? summary.cosmetic : [];
            if (!cosmetic.length) {
              core.info('No cosmetic failures detected; skipping comment.');
              return;
            }
            const list = cosmetic.map(entry => {
              const markers = entry.markers && entry.markers.length ? ` (${entry.markers.join(', ')})` : '';
              return `- \`${entry.id}\`${markers}`;
            }).join('\n');
            const patchNote = process.env.HAS_PATCH === 'true'
              ? '\nA cosmetic fix patch is available as an artifact on this run.'
              : '';
            const body = [
              ' Detected cosmetic-only test failures. CI remains in advisory mode.',
              '',
              list,
              '',
              'Automation attempted to apply lightweight fixes. Please review the results and rerun the pipeline.',
              patchNote.trim()
            ].filter(Boolean).join('\n');
            const { owner, repo } = context.repo;
            const issue_number = context.payload.pull_request.number;
            await github.rest.issues.createComment({ owner, repo, issue_number, body });
            core.info('Posted cosmetic failure advisory comment.');

  logs_summary:
    if: ${{ always() }}
    needs: [tests]
    runs-on: ubuntu-latest
    permissions:
      actions: read
    steps:
      - name: Enumerate jobs
        id: jobs
        uses: actions/github-script@v7
        with:
          script: |
            const {owner, repo} = context.repo;
            const run_id = context.runId;
            const { data } = await github.rest.actions.listJobsForWorkflowRun({ owner, repo, run_id, per_page: 100 });
            const rows = data.jobs.map(j => `| ${j.name} | ${j.conclusion || j.status} | [logs](${j.html_url}) |`);
            const failed = data.jobs.filter(j => !['success','skipped'].includes((j.conclusion || '').toLowerCase())).map(j => j.name);
            const failingTestJobs = failed.filter(n => /test/i.test(n));
            let explanation = '';
            if (failed.length) {
              explanation = `\n>  ${failed.length} job(s) did not succeed: ${failed.join(', ')}.`;
              if (failingTestJobs.length) {
                explanation += ` Failing test jobs: ${failingTestJobs.length}.`;
              }
            } else {
              explanation = `\n>  All jobs succeeded.`;
            }
            const md = ["### Logs for this run","| Job | Result | Logs |","|---|---|---|",...rows, explanation].join("\n");
            core.setOutput('table', md);
      - name: Append logs table to summary
        run: |
          {
            echo "${{ steps.jobs.outputs.table }}"
            echo
          } >> "$GITHUB_STEP_SUMMARY"

# End of workflow

