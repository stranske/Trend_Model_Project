name: Reusable 94 Legacy CI Python

on:
  workflow_call:
    inputs:
      python_matrix:
        description: 'JSON array of Python versions'
        required: false
        default: '["3.11","3.12"]'
        type: string
      cov_min:
        description: 'Coverage minimum threshold'
        required: false
        default: '85'
        type: string
      run_quarantine:
        description: 'Include tests marked quarantine'
        required: false
        default: 'false'
        type: string
    secrets:
      additional_pypi_token:
        required: false

permissions:
  contents: read
  pull-requests: write
  issues: write

jobs:
  core-tests:
    name: core-tests (Python ${{ matrix.py }})
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        py: ${{ fromJson(inputs.python_matrix) }}
    outputs:
      coverage: ${{ steps.cov_out.outputs.cov || '' }}
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.py }}
      - name: Cache pip
        id: cache-pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: >-
            pip-${{ runner.os }}-${{ matrix.py }}-
            ${{ hashFiles('requirements.txt', 'requirements.lock', 'pyproject.toml') }}
          restore-keys: |
            pip-${{ runner.os }}-${{ matrix.py }}-
            pip-${{ runner.os }}-
      - name: Install
        run: |
          python -m venv .venv
          echo "${{ github.workspace }}/.venv/bin" >> "$GITHUB_PATH"
          echo "VIRTUAL_ENV=${{ github.workspace }}/.venv" >> "$GITHUB_ENV"
          source .venv/bin/activate
          pip install -U pip
          pip install -r requirements.txt
          pip install -e '.[dev]' pytest pytest-cov
      - name: Remove old coverage data
        run: rm -f .coverage .coverage.*
      - name: Run tests
        id: run_tests
        env:
          COV_MIN: ${{ inputs.cov_min }}
          RUN_QUAR: ${{ inputs.run_quarantine }}
        run: |
          echo "::notice title=FlakeQuarantine::Single rerun enabled (Issue #1147)"
          MARK_EXPR="not slow"
          if [ "${RUN_QUAR}" != "true" ]; then
            MARK_EXPR="not quarantine and not slow"
          fi
          # Soft gate mode: remove --cov-fail-under hard enforcement; gather rich reports
          pytest -m "$MARK_EXPR" \
            --reruns 1 --reruns-delay 1 \
            --cov=src --cov-report=term-missing:skip-covered \
            --cov-report=xml --cov-report=json --cov-report=html \
            --cov-branch \
            --junitxml=pytest-report.xml || TEST_EXIT=$?
          TEST_EXIT=${TEST_EXIT:-0}
          echo "Tests exit code: $TEST_EXIT"
          # Rename artifacts uniquely per matrix job for downstream aggregation
          mv coverage.xml coverage-${{ matrix.py }}.xml || true
          mv coverage.json coverage-${{ matrix.py }}.json || true
          mv pytest-report.xml pytest-report-${{ matrix.py }}.xml || true
          if [ -d htmlcov ]; then mv htmlcov htmlcov-${{ matrix.py }}; fi
          exit "$TEST_EXIT"
      - name: Extract coverage
        id: cov_out
        run: |
          pct=$(grep -Eo 'TOTAL.+ [0-9]+%' <(coverage report 2>/dev/null || true) | awk '{print $NF}' | tr -d '%') || true
          echo "cov=${pct}" >> "$GITHUB_OUTPUT"
      - name: Upload coverage artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-${{ matrix.py }}
          path: |
            coverage-${{ matrix.py }}.xml
            coverage-${{ matrix.py }}.json
            htmlcov-${{ matrix.py }}/**
            pytest-report-${{ matrix.py }}.xml
          retention-days: 10
      - name: Summary
        if: always()
        run: |
          {
            echo "### CI Python Summary"
            echo "Python: ${{ matrix.py }}"
            echo "Coverage: ${{ steps.cov_out.outputs.cov }}"
          } >> "$GITHUB_STEP_SUMMARY"
  aggregate-and-label:
    name: aggregate & label
    runs-on: ubuntu-latest
    needs: [core-tests]
    if: needs.core-tests.result == 'success'
    permissions:
      contents: read
      pull-requests: write
      issues: write
    steps:
      - name: Add ci:green label
        if: github.event_name == 'pull_request'
        uses: actions-ecosystem/action-add-labels@v1
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          labels: ${{ vars.CI_LABEL || 'ci:green' }}
      - name: Update unified PR comment with test results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const pr = context.payload.pull_request.number;
            const { owner, repo } = context.repo;
            const bodyAdd = `\n\n**Tests:**  All matrix jobs succeeded (see CI checks).`;
            const marker = '<!-- autofix-status: DO NOT EDIT -->';
            const comments = await github.rest.issues.listComments({ owner, repo, issue_number: pr, per_page: 100 });
            const target = comments.data.find(c=>c.body && c.body.includes(marker));
            if (!target) { core.info('Unified comment not found; skipping update'); return; }
            let body = target.body;
            if (body.includes('**Tests:**')) {
              body = body.replace(/\*\*Tests:\*\*[\s\S]*?(?=\n\n|$)/, '**Tests:**  All matrix jobs succeeded (see CI checks).');
            } else {
              body += bodyAdd;
            }
            await github.rest.issues.updateComment({ owner, repo, comment_id: target.id, body });
            core.info('Updated unified status comment with test results.');

  coverage-soft-gate:
    name: coverage soft gate (issue & summary)
    runs-on: ubuntu-latest
    needs: [core-tests]
    if: always()
    outputs:
      has_failures: ${{ steps.classify.outputs.has_failures }}
      only_cosmetic: ${{ steps.classify.outputs.only_cosmetic }}
      summary_b64: ${{ steps.classify.outputs.summary_b64 }}
    permissions:
      contents: read
      issues: write
      actions: read
    steps:
      - name: Download coverage artifacts
        continue-on-error: true
        uses: actions/download-artifact@v4
        with:
          pattern: coverage-*
          merge-multiple: true
      - name: Classify test outcomes
        id: classify
        shell: python
        run: |
          import base64
          import glob
          import json
          import os

          from scripts.classify_test_failures import classify_reports

          paths = sorted(glob.glob('pytest-report-*.xml'))
          summary = classify_reports(paths)
          with open('test-failure-summary.json', 'w', encoding='utf-8') as handle:
            json.dump(summary, handle, indent=2)

          if summary.get('has_failures'):
            print('Detected failing tests:')
            for bucket in ('runtime', 'cosmetic', 'unknown'):
              entries = summary.get(bucket, []) or []
              if not entries:
                continue
              print(f"  {bucket}: {len(entries)}")
              for entry in entries:
                print(f"    - {entry['id']} ({', '.join(entry.get('markers', [])) or 'unmarked'})")
          else:
            print('No failing tests detected across JUnit reports.')

          encoded = base64.b64encode(json.dumps(summary).encode('utf-8')).decode('ascii')
          out_path = os.environ.get('GITHUB_OUTPUT')
          if out_path:
            with open(out_path, 'a', encoding='utf-8') as fh:
              fh.write(f"has_failures={'true' if summary.get('has_failures') else 'false'}\n")
              fh.write(f"only_cosmetic={'true' if summary.get('only_cosmetic') else 'false'}\n")
              fh.write(f"summary_b64={encoded}\n")
      - name: Upload test failure summary
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-failure-summary
          path: test-failure-summary.json
          retention-days: 10
      - name: Compute coverage summary & hotspots
        id: cov
        shell: python
        run: |
          import glob, json, xml.etree.ElementTree as ET
          job_cov = []  # (job, pct)
          file_best = {}  # file -> min pct across jobs
          for jf in glob.glob('coverage-*.json'):
              job = jf.split('coverage-')[-1].rsplit('.json',1)[0]
              data = json.load(open(jf))
              t = data.get('totals',{})
              pct = float(t.get('percent_covered') or t.get('percent_covered_display') or 0)
              job_cov.append((job, pct))
              for f, meta in (data.get('files') or {}).items():
                  s = meta.get('summary',{})
                  fpct = float(s.get('percent_covered') or s.get('percent_covered_display') or 0)
                  if f not in file_best or fpct < file_best[f]:
                      file_best[f] = fpct
          job_cov.sort(key=lambda x: x[0])
          hotspots = sorted(((p,f) for f,p in file_best.items()), key=lambda x: x[0])[:15]
          totals = [p for _,p in job_cov]
          avg = round(sum(totals)/len(totals),2) if totals else 0.0
          worst = round(min(totals),2) if totals else 0.0
          # Parse junit for pass/fail/skip per job
          junit_rows = []  # (job, passed, failed, skipped)
          for jf in glob.glob('pytest-report-*.xml'):
              job = jf.split('pytest-report-')[-1].rsplit('.xml',1)[0]
              try:
                  tree = ET.parse(jf); root = tree.getroot()
                  suites = [root]
                  if root.tag == 'testsuites':
                      suites = list(root)
                  passed=failed=skipped=0
                  for s in suites:
                      failed += int(s.attrib.get('failures',0)) + int(s.attrib.get('errors',0))
                      skipped += int(s.attrib.get('skipped',0))
                      tests = int(s.attrib.get('tests',0))
                      passed += tests - int(s.attrib.get('failures',0)) - int(s.attrib.get('errors',0)) - int(s.attrib.get('skipped',0))
                  junit_rows.append((job, passed, failed, skipped))
              except Exception as e:
                  print('JUnit parse error', jf, e)
          stats = []
          cov_map = {j:p for j,p in job_cov}
          junit_map = {j:(pa,fa,sk) for j,pa,fa,sk in junit_rows}
          jobs = sorted(set(cov_map)|set(junit_map))
          for j in jobs:
              stats.append((j, cov_map.get(j,0.0),)+junit_map.get(j,(0,0,0)))
          with open('coverage_summary.md','w') as w:
              w.write(f"**Coverage (avg across jobs): {avg}% | worst job: {worst}%**\n\n")
              if stats:
                  w.write('| Job | Coverage% | Passed | Failed | Skipped |\n|---|---:|---:|---:|---:|\n')
                  for j,c,pa,fa,sk in stats:
                      w.write(f'| {j} | {c:.2f}% | {pa} | {fa} | {sk} |\n')
              w.write('\n**Lowest Covered Files**\n\n| File | % covered |\n|---|---:|\n')
              for p,f in hotspots:
                  w.write(f'| `{f}` | {p:.1f}% |\n')
          print(open('coverage_summary.md').read())
      - name: Build job log links table
        id: jobs
        uses: actions/github-script@v7
        with:
          script: |
            const { owner, repo } = context.repo;
            const run_id = context.runId;
            const jobs = [];
            let page = 1;
            while (true) {
              const resp = await github.rest.actions.listJobsForWorkflowRun({ owner, repo, run_id, per_page: 100, page });
              const pageJobs = resp.data.jobs || [];
              jobs.push(...pageJobs);
              const link = resp.headers.link || '';
              if (!link.includes('rel="next"')) {
                break;
              }
              page += 1;
            }

            const normalise = (value) => {
              if (!value) return 'in_progress';
              return String(value).toLowerCase();
            };

            const badge = (state) => {
              switch (state) {
                case 'success':
                  return '';
                case 'failure':
                case 'timed_out':
                case 'cancelled':
                  return '';
                case 'skipped':
                  return '';
                default:
                  return '';
              }
            };

            const formatState = (state) => {
              return state
                .split('_')
                .map(segment => segment ? segment.charAt(0).toUpperCase() + segment.slice(1) : segment)
                .join(' ');
            };

            const rows = jobs
              .slice()
              .sort((a, b) => {
                const aStart = a.started_at ? Date.parse(a.started_at) : 0;
                const bStart = b.started_at ? Date.parse(b.started_at) : 0;
                if (aStart !== bStart) return aStart - bStart;
                return a.name.localeCompare(b.name);
              })
              .map(job => {
                const state = normalise(job.conclusion || job.status);
                const label = formatState(state);
                const prefix = badge(state);
                return `| ${job.name} | ${prefix} ${label} | [logs](${job.html_url}) |`;
              });

            const mdLines = [
              '### Logs for this run',
              '| Job | Result | Logs |',
              '|-----|--------|------|',
              ...(rows.length ? rows : ['| _No jobs found_ |  |  |'])
            ];
            const md = mdLines.join('\n');
            core.setOutput('table', md);

      - name: Create or update coverage issue
        id: issue
        uses: actions/github-script@v7
        env:
          TABLE: ${{ steps.jobs.outputs.table }}
        with:
          script: |
            const { owner, repo } = context.repo;
            const title = 'Increase test coverage (soft gate)';
            const label = 'tech:coverage';
            const runUrl = `https://github.com/${owner}/${repo}/actions/runs/${context.runId}`;
            const summary = require('fs').readFileSync('coverage_summary.md','utf8');
            const logsTable = process.env.TABLE || '';
            try { await github.rest.issues.getLabel({ owner, repo, name: label }); } catch { await github.rest.issues.createLabel({ owner, repo, name: label, color: '31a2bf' }); }
            const search = await github.rest.search.issuesAndPullRequests({ q: `repo:${owner}/${repo} is:issue "${title}" in:title state:open` });
            let bodySection = `\n---\n### Run ${new Date().toISOString()}\n${summary}\n${logsTable}\n[Workflow run](${runUrl})`;
            if (search.data.items.length) {
              const issue = search.data.items[0];
              const current = (await github.rest.issues.get({ owner, repo, issue_number: issue.number })).data.body || '';
              const updated = current + bodySection;
              await github.rest.issues.update({ owner, repo, issue_number: issue.number, body: updated });
              core.setOutput('issue_number', issue.number);
              core.info(`Updated coverage issue #${issue.number}`);
            } else {
              const created = await github.rest.issues.create({ owner, repo, title, body: `Tracking test coverage hotspots.\n${bodySection}`, labels: [label] });
              core.setOutput('issue_number', created.data.number);
              core.info(`Created coverage issue #${created.data.number}`);
            }
      - name: Inject coverage into PR unified comment
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const pr = context.payload.pull_request.number;
            const { owner, repo } = context.repo;
            const marker = '<!-- autofix-status: DO NOT EDIT -->';
            const summary = require('fs').readFileSync('coverage_summary.md','utf8');
            const covHeader = '**Coverage:**';
            const comments = await github.rest.issues.listComments({ owner, repo, issue_number: pr, per_page: 100 });
            const target = comments.data.find(c=>c.body && c.body.includes(marker));
            if(!target){ core.info('Unified comment not found; skipping coverage injection'); return; }
            let body = target.body;
            const block = covHeader + '\n' + summary.split('\n')[0];
            if (body.includes(covHeader)) {
              body = body.replace(/\*\*Coverage:\*\*[\s\S]*?(?=\n\n|$)/, block);
            } else {
              body += '\n\n' + block;
            }
            await github.rest.issues.updateComment({ owner, repo, comment_id: target.id, body });
            core.info('Injected coverage summary into unified PR comment');
      - name: Append run summary
        if: always()
        env:
          LOG_TABLE: ${{ steps.jobs.outputs.table }}
        run: |
          {
            echo "### Coverage Soft Gate"
            cat coverage_summary.md
            echo ""
            if [ -n "$LOG_TABLE" ]; then
              printf '%s\n' "$LOG_TABLE"
              echo ""
            else
              echo "_No workflow jobs found to report._"
              echo ""
            fi
          } >> "$GITHUB_STEP_SUMMARY"
  cosmetic-followup:
    name: cosmetic failure triage
    runs-on: ubuntu-latest
    needs: [core-tests, coverage-soft-gate]
    if: needs.core-tests.result != 'success' && needs.coverage-soft-gate.outputs.has_failures == 'true' && needs.coverage-soft-gate.outputs.only_cosmetic == 'true'
    permissions:
      contents: read
      pull-requests: write
      actions: read
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
      - name: Run cosmetic auto-hygiene
        run: |
          python scripts/auto_type_hygiene.py
      - name: Summarise cosmetic failures
        id: cosmetic
        shell: python
        env:
          SUMMARY_B64: ${{ needs.coverage-soft-gate.outputs.summary_b64 }}
        run: |
          import base64
          import json
          import os

          summary_data = {}
          payload = os.environ.get('SUMMARY_B64')
          if payload:
            summary_data = json.loads(base64.b64decode(payload).decode('utf-8'))
          cosmetic = summary_data.get('cosmetic', []) if isinstance(summary_data, dict) else []
          lines = []
          for entry in cosmetic:
            markers = entry.get('markers') or []
            marker_text = f" ({', '.join(markers)})" if markers else ''
            message = entry.get('message', '')
            if message:
              message = message.splitlines()[0]
              message = f"  {message}"
            lines.append(f"- `{entry.get('id', '<unknown>')}`{marker_text}{message}")

          if not lines:
            lines.append('- No detailed cosmetic failures captured.')

          with open('cosmetic_failures.md', 'w', encoding='utf-8') as fh:
            fh.write("\n".join(lines) + "\n")

          out = os.environ.get('GITHUB_OUTPUT')
          if out:
            with open(out, 'a', encoding='utf-8') as handle:
              handle.write(f"has_details={'true' if cosmetic else 'false'}\n")
      - name: Capture fixer diff
        id: patch
        shell: bash
        run: |
          set -euo pipefail
          if git diff --quiet; then
            echo "has_patch=false" >> "$GITHUB_OUTPUT"
          else
            git diff > cosmetic-fixes.patch
            echo "has_patch=true" >> "$GITHUB_OUTPUT"
          fi
      - name: Upload cosmetic patch
        if: steps.patch.outputs.has_patch == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: cosmetic-fixes
          path: cosmetic-fixes.patch
          retention-days: 5
      - name: Post advisory comment for cosmetic failures
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        env:
          SUMMARY_B64: ${{ needs.coverage-soft-gate.outputs.summary_b64 }}
          HAS_PATCH: ${{ steps.patch.outputs.has_patch }}
        with:
          script: |
            const summaryPayload = process.env.SUMMARY_B64 || '';
            if (!summaryPayload) {
              core.info('No summary payload found; skipping cosmetic comment.');
              return;
            }
            const summary = JSON.parse(Buffer.from(summaryPayload, 'base64').toString('utf8'));
            const cosmetic = Array.isArray(summary.cosmetic) ? summary.cosmetic : [];
            if (!cosmetic.length) {
              core.info('No cosmetic failures detected; skipping comment.');
              return;
            }
            const list = cosmetic.map(entry => {
              const markers = entry.markers && entry.markers.length ? ` (${entry.markers.join(', ')})` : '';
              return `- \`${entry.id}\`${markers}`;
            }).join('\n');
            const patchNote = process.env.HAS_PATCH === 'true'
              ? '\nA cosmetic fix patch is available as an artifact on this run.'
              : '';
            const body = [
              ' Detected cosmetic-only test failures. CI remains in advisory mode.',
              '',
              list,
              '',
              'Automation attempted to apply lightweight fixes. Please review the results and rerun the pipeline.',
              patchNote.trim()
            ].filter(Boolean).join('\n');
            const { owner, repo } = context.repo;
            const issue_number = context.payload.pull_request.number;
            await github.rest.issues.createComment({ owner, repo, issue_number, body });
            core.info('Posted cosmetic failure advisory comment.');
  remove-green-on-fail:
    name: strip ci:green on failure
    runs-on: ubuntu-latest
    needs: [core-tests]
    if: needs.core-tests.result != 'success' && github.event_name == 'pull_request'
    permissions:
      pull-requests: write
      issues: write
      contents: read
    steps:
      - name: Remove ci:green label
        uses: actions/github-script@v7
        with:
          script: |
            const pr = context.payload.pull_request.number;
            const { owner, repo } = context.repo;
            const ciLabel = process.env.CI_LABEL || 'ci:green';
            try {
              await github.rest.issues.removeLabel({ owner, repo, issue_number: pr, name: ciLabel });
              core.info('Removed ci:green label due to failing CI run.');
            } catch(e){ core.warning('Could not remove ci:green (maybe absent): ' + e.message); }
        env:
          CI_LABEL: ${{ vars.CI_LABEL || 'ci:green' }}
