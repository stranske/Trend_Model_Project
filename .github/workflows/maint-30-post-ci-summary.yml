name: Maint 30 Post CI Summary

on:
  workflow_run:
    workflows: ["PR 10 CI Python", "PR 12 Docker Smoke"]
    types: [completed]

concurrency:
  group: post-ci-summary-${{ github.event.workflow_run.head_sha }}
  cancel-in-progress: true

permissions:
  contents: read
  pull-requests: write
  issues: read
  actions: read

jobs:
  summarize:
    if: >
      github.event.workflow_run.event == 'pull_request' &&
      github.event.workflow_run.pull_requests[0].number
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Discover workflow runs for head SHA
        id: gather
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const { owner, repo } = context.repo;
            const workflowRun = context.payload.workflow_run || {};
            const headSha = workflowRun.head_sha || context.sha || '';
            const prNumber = Array.isArray(workflowRun.pull_requests) && workflowRun.pull_requests.length > 0
              ? workflowRun.pull_requests[0].number
              : '';

            const parseJsonInput = (raw, fallback) => {
              if (!raw) {
                return fallback;
              }
              try {
                return JSON.parse(raw);
              } catch (error) {
                core.warning(`Failed to parse JSON input: ${error}`);
                return fallback;
              }
            };

            const defaultWorkflowTargets = [
              { key: 'ci', display_name: 'CI', workflow_path: '.github/workflows/pr-10-ci-python.yml' },
              { key: 'docker', display_name: 'Docker', workflow_path: '.github/workflows/pr-12-docker-smoke.yml' },
            ];

            const workflowTargetsRaw = process.env.WORKFLOW_TARGETS_JSON;
            const workflowTargetsInput = parseJsonInput(workflowTargetsRaw, defaultWorkflowTargets);
            const workflowTargetsSource = Array.isArray(workflowTargetsInput) ? workflowTargetsInput : defaultWorkflowTargets;
            // Helper to normalize target properties
            function normalizeTargetProps(target) {
              return {
                key: target.key,
                displayName: target.display_name || target.displayName || target.key || 'workflow',
                workflowPath: target.workflow_path || target.workflowPath || '',
                workflowFile: target.workflow_file || target.workflowFile || target.workflow_id || target.workflowId || '',
                workflowName: target.workflow_name || target.workflowName || '',
                workflowIds: Array.isArray(target.workflow_ids) ? target.workflow_ids : (target.workflowIds && Array.isArray(target.workflowIds) ? target.workflowIds : []),
              };
            }

            const workflowTargets = workflowTargetsSource
              .map(normalizeTargetProps)
              .filter(target => target && target.key);

            const normalizePath = (value) => {
              if (!value) return '';
              return String(value).replace(/^\.\//, '').replace(/^\/+/, '');
            };

            async function loadWorkflowRun(identifier) {
              if (!identifier) {
                return null;
              }
              try {
                const response = await github.rest.actions.listWorkflowRuns({
                  owner,
                  repo,
                  workflow_id: identifier,
                  head_sha: headSha || undefined,
                  event: 'pull_request',
                  per_page: 10,
                });
                const runs = response.data.workflow_runs || [];
                if (!runs.length) {
                  return null;
                }
                if (!headSha) {
                  return runs[0];
                }
                const exact = runs.find(item => item.head_sha === headSha);
                return exact || runs[0];
              } catch (error) {
                core.warning(`Failed to query workflow runs for "${identifier}": ${error}`);
                return null;
              }
            }

            async function loadJobs(runId) {
              if (!runId) {
                return [];
              }
              try {
                const jobs = await github.paginate(
                  github.rest.actions.listJobsForWorkflowRun,
                  {
                    owner,
                    repo,
                    run_id: runId,
                    per_page: 100,
                  },
                );
                return jobs
                  .filter(job => job)
                  .map(job => ({
                    name: job.name,
                    conclusion: job.conclusion,
                    status: job.status,
                    html_url: job.html_url,
                  }));
              } catch (error) {
                core.warning(`Failed to query jobs for workflow run ${runId}: ${error}`);
                return [];
              }
            }

            async function resolveRun(target) {
              const candidates = [];
              if (Array.isArray(target.workflowIds) && target.workflowIds.length) {
                for (const id of target.workflowIds) {
                  if (id) {
                    candidates.push(id);
                  }
                }
              }
              if (target.workflowPath) {
                candidates.push(normalizePath(target.workflowPath));
              }
              if (target.workflowFile) {
                candidates.push(normalizePath(target.workflowFile));
              }
              if (target.workflowName) {
                candidates.push(target.workflowName);
              }
              if (!candidates.length) {
                candidates.push(target.key);
              }

              for (const identifier of candidates) {
                const run = await loadWorkflowRun(identifier);
                if (run) {
                  return run;
                }
              }
              return null;
            }

            const collected = [];
            for (const target of workflowTargets) {
              const run = await resolveRun(target);
              if (run) {
                const jobs = await loadJobs(run.id);
                collected.push({
                  key: target.key,
                  displayName: target.displayName,
                  present: true,
                  id: run.id,
                  run_attempt: run.run_attempt,
                  conclusion: run.conclusion,
                  status: run.status,
                  html_url: run.html_url,
                  jobs,
                });
              } else {
                collected.push({
                  key: target.key,
                  displayName: target.displayName,
                  present: false,
                  jobs: [],
                });
              }
            }

            const ciRun = collected.find(entry => entry.key === 'ci' && entry.present);
            const dockerRun = collected.find(entry => entry.key === 'docker' && entry.present);

            core.setOutput('runs', JSON.stringify(collected));
            core.setOutput('ci_run_id', ciRun ? String(ciRun.id) : '');
            core.setOutput('docker_run_id', dockerRun ? String(dockerRun.id) : '');
            core.setOutput('head_sha', headSha || '');
            core.setOutput('pr_number', prNumber ? String(prNumber) : '');
            core.notice(`Collected ${collected.filter(entry => entry.present).length} workflow runs for head ${headSha}`);
      - name: Download coverage summary
        if: ${{ steps.gather.outputs.ci_run_id }}
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: coverage-summary
          run-id: ${{ steps.gather.outputs.ci_run_id }}
          github-token: ${{ secrets.GITHUB_TOKEN }}
          path: artifacts/coverage-summary
      - name: Download coverage trend
        if: ${{ steps.gather.outputs.ci_run_id }}
        uses: actions/download-artifact@v4
        with:
          run-id: ${{ steps.gather.outputs.ci_run_id }}
          pattern: '*'
          merge-multiple: true
          path: summary_artifacts
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - name: Download Docker artifacts
        if: ${{ steps.gather.outputs.docker_run_id && steps.gather.outputs.docker_run_id != steps.gather.outputs.ci_run_id }}
        uses: actions/download-artifact@v4
        with:
          run-id: ${{ steps.gather.outputs.docker_run_id }}
          pattern: '*'
          merge-multiple: true
          path: summary_artifacts
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - name: Load coverage summary snippet
        id: coverage_summary
        run: |
          set -euo pipefail
          mkdir -p summary_artifacts
          python <<'PY'
          import os
          from pathlib import Path

          output_path = os.environ.get('GITHUB_OUTPUT')
          root = Path('summary_artifacts')
          search_roots = [root, Path('artifacts')]

          summary_path: Path | None = None
          candidates = []
          for base in search_roots:
              if not base.exists():
                  continue
              candidates.extend(base.rglob('coverage-summary.md'))
          if candidates:
              # Prefer files in earlier search_roots; if multiple, pick the first found
              summary_path = candidates[0]

          if summary_path is None:
              print('No coverage summary markdown found; continuing without it.')
          else:
              text = summary_path.read_text(encoding='utf-8')
              dest = root / 'coverage-summary.md'
              dest.write_text(text, encoding='utf-8')
              if output_path:
                  with Path(output_path).open('a', encoding='utf-8') as handle:
                      handle.write('body<<EOF\n')
                      handle.write(text)
                      if not text.endswith('\n'):
                          handle.write('\n')
                      handle.write('EOF\n')
              print(f'Embedded coverage summary from {summary_path}')
          PY

      - name: Compute coverage stats
        id: coverage_stats
        run: |
          set -euo pipefail
          python <<'PY'
          import json
          from pathlib import Path

          root = Path('summary_artifacts')

          def find_one(pattern: str) -> Path | None:
              if not root.exists():
                  return None
              for candidate in root.rglob(pattern):
                  if candidate.is_file():
                      return candidate
              return None

          record_path = find_one('coverage-trend.json')
          history_path = find_one('coverage-trend-history.ndjson')

          def load_record(path: Path | None) -> dict[str, object] | None:
              if not path:
                  return None
              try:
                  data = json.loads(path.read_text(encoding='utf-8'))
              except Exception:
                  return None
              return data if isinstance(data, dict) else None

          latest_record = load_record(record_path)

          history: list[dict[str, object]] = []
          if history_path and history_path.exists():
              for raw in history_path.read_text(encoding='utf-8').splitlines():
                  line = raw.strip()
                  if not line:
                      continue
                  try:
                      parsed = json.loads(line)
                  except json.JSONDecodeError:
                      continue
                  if isinstance(parsed, dict):
                      history.append(parsed)

          def run_identifier(record: dict[str, object] | None) -> tuple[object | None, object | None]:
              if not isinstance(record, dict):
                  return (None, None)
              return (record.get('run_id'), record.get('run_number'))

          latest_id = run_identifier(latest_record)

          if latest_record is None and history:
              latest_record = history[-1]
              latest_id = run_identifier(latest_record)

          previous_record: dict[str, object] | None = None
          if history:
              for candidate in reversed(history):
                  if run_identifier(candidate) == latest_id:
                      continue
                  previous_record = candidate
                  break
              if previous_record is None and len(history) > 1:
                  previous_record = history[-2]

          def extract(record: dict[str, object] | None, key: str) -> float | None:
              if not isinstance(record, dict):
                  return None
              value = record.get(key)
              try:
                  return float(value) if value is not None else None
              except (TypeError, ValueError):
                  return None

          avg_latest = extract(latest_record, 'avg_coverage')
          worst_latest = extract(latest_record, 'worst_job_coverage')
          avg_prev = extract(previous_record, 'avg_coverage')
          worst_prev = extract(previous_record, 'worst_job_coverage')

          def delta(latest: float | None, prev: float | None) -> float | None:
              try:
                  if latest is None or prev is None:
                      return None
                  return round(float(latest) - float(prev), 2)
              except Exception:
                  return None

          stats = {
              'avg_latest': avg_latest,
              'avg_previous': avg_prev,
              'avg_delta': delta(avg_latest, avg_prev),
              'worst_latest': worst_latest,
              'worst_previous': worst_prev,
              'worst_delta': delta(worst_latest, worst_prev),
              'history_len': len(history),
          }

          print(json.dumps(stats))
          Path('coverage-stats.json').write_text(json.dumps(stats), encoding='utf-8')
          PY
          if [ -f coverage-stats.json ]; then
            printf 'stats_json=%s\n' "$(cat coverage-stats.json)" >> "$GITHUB_OUTPUT"
          fi
      - name: Prepare summary body
        id: prep
        run: |
          python tools/post_ci_summary.py
        env:
          RUNS_JSON: ${{ steps.gather.outputs.runs }}
          HEAD_SHA: ${{ steps.gather.outputs.head_sha }}
          COVERAGE_SECTION: ${{ steps.coverage_summary.outputs.body }}
          COVERAGE_STATS: ${{ steps.coverage_stats.outputs.stats_json }}
          REQUIRED_JOB_GROUPS_JSON: ${{ env.REQUIRED_JOB_GROUPS_JSON }}
      - name: Persist summary preview
        run: |
          mkdir -p summary_artifacts
          if [ -n "${SUMMARY_BODY}" ]; then
            printf '%s\n' "${SUMMARY_BODY}" > summary_artifacts/summary_preview.md
          else
            echo "Summary body empty; skipping preview persistence."
          fi
        env:
          SUMMARY_BODY: ${{ steps.prep.outputs.body }}
      - name: Upsert pull request summary comment
        if: ${{ steps.prep.outputs.body && steps.gather.outputs.pr_number }}
        uses: actions/github-script@v7
        env:
          SUMMARY_BODY: ${{ steps.prep.outputs.body }}
          PR_NUMBER: ${{ steps.gather.outputs.pr_number }}
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const marker = '<!-- post-ci-summary:do-not-edit -->';
            const body = process.env.SUMMARY_BODY || '';
            const prNumberRaw = process.env.PR_NUMBER;
            if (!body.trim()) {
              core.info('Summary body is empty; skipping comment update.');
              return;
            }
            const prNumber = Number(prNumberRaw);
            if (!Number.isFinite(prNumber) || prNumber <= 0) {
              core.warning(`Invalid PR number "${prNumberRaw}"; skipping comment update.`);
              return;
            }
            const { owner, repo } = context.repo;
            const allComments = await github.paginate(
              github.rest.issues.listComments,
              {
                owner,
                repo,
                issue_number: prNumber,
                per_page: 100,
              },
            );
            const summaryComments = allComments.filter(comment => {
              if (!comment || typeof comment.body !== 'string') {
                return false;
              }
              return comment.body.includes(marker);
            });
            const [primary, ...duplicates] = summaryComments;
            for (const duplicate of duplicates) {
              try {
                await github.rest.issues.deleteComment({
                  owner,
                  repo,
                  comment_id: duplicate.id,
                });
                core.info(`Removed duplicate summary comment ${duplicate.id}.`);
              } catch (error) {
                core.warning(`Failed to remove duplicate comment ${duplicate.id}: ${error}`);
              }
            }
            if (primary) {
              if (primary.body === body) {
                core.info(`Summary comment ${primary.id} already up to date; no change needed.`);
                return;
              }
              await github.rest.issues.updateComment({
                owner,
                repo,
                comment_id: primary.id,
                body,
              });
              core.info(`Updated summary comment ${primary.id}.`);
            } else {
              const response = await github.rest.issues.createComment({
                owner,
                repo,
                issue_number: prNumber,
                body,
              });
              core.info(`Created summary comment ${response.data.id}.`);
            }
      - name: Publish run summary
        run: |
          if [ -n "${SUMMARY_BODY}" ]; then
            printf '## Automated Status Summary\n\n%s\n' "${SUMMARY_BODY}" >> "$GITHUB_STEP_SUMMARY"
          else
            echo "Automated status summary was empty." >> "$GITHUB_STEP_SUMMARY"
          fi
        env:
          SUMMARY_BODY: ${{ steps.prep.outputs.body }}
