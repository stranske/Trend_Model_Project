name: Post CI Status Summary

on:
  workflow_run:
    workflows: ["CI", "Docker"]
    types: [completed]

concurrency:
  group: post-ci-summary-${{ github.event.workflow_run.head_sha }}
  cancel-in-progress: true

permissions:
  pull-requests: write
  contents: read
  actions: read

jobs:
  summarize:
    if: >
      github.event.workflow_run.event == 'pull_request' &&
      github.event.workflow_run.pull_requests[0].number
    runs-on: ubuntu-latest
    env:
      WORKFLOW_TARGETS_JSON: |
        [
          {
            "key": "ci",
            "display_name": "CI",
            "workflow_path": ".github/workflows/pr-10-ci-python.yml"
          },
          {
            "key": "docker",
            "display_name": "Docker",
            "workflow_path": ".github/workflows/pr-12-docker-smoke.yml"
          }
        ]
      REQUIRED_JOB_GROUPS_JSON: |
        [
          {
            "label": "CI tests",
            "patterns": [
              "^main / tests(?: /|$)"
            ]
          },
          {
            "label": "CI workflow-automation",
            "patterns": [
              "^workflow / automation-tests(?: /|$)"
            ]
          },
          {
            "label": "CI style",
            "patterns": [
              "^main / style(?: /|$)"
            ]
          },
          {
            "label": "CI gate",
            "patterns": [
              "^gate / all-required-green$"
            ]
          }
        ]
    steps:
      - name: Gather workflow metadata
        id: gather
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const run = context.payload.workflow_run;
            const pr = (run.pull_requests || [])[0];

            if (!pr) {
              core.setOutput('should_skip', 'true');
              return;
            }

            const { owner, repo } = context.repo;
            const headSha = run.head_sha;

            const parseJsonInput = (raw, fallback) => {
              if (!raw) {
                return fallback;
              }
              try {
                return JSON.parse(raw);
              } catch (error) {
                core.warning(`Failed to parse JSON input: ${error}`);
                return fallback;
              }
            };

            const defaultWorkflowTargets = [
              { key: 'ci', display_name: 'CI', workflow_path: '.github/workflows/pr-10-ci-python.yml' },
              { key: 'docker', display_name: 'Docker', workflow_path: '.github/workflows/pr-12-docker-smoke.yml' },
            ];

            const workflowTargetsRaw = process.env.WORKFLOW_TARGETS_JSON;
            const workflowTargetsInput = parseJsonInput(workflowTargetsRaw, defaultWorkflowTargets);
            const workflowTargetsSource = Array.isArray(workflowTargetsInput) ? workflowTargetsInput : defaultWorkflowTargets;
            // Helper to normalize target properties
            function normalizeTargetProps(target) {
              return {
                key: target.key,
                displayName: target.display_name || target.displayName || target.key || 'workflow',
                workflowPath: target.workflow_path || target.workflowPath || '',
                workflowFile: target.workflow_file || target.workflowFile || target.workflow_id || target.workflowId || '',
                workflowName: target.workflow_name || target.workflowName || '',
                workflowIds: Array.isArray(target.workflow_ids) ? target.workflow_ids : (target.workflowIds && Array.isArray(target.workflowIds) ? target.workflowIds : []),
              };
            }

            const workflowTargets = workflowTargetsSource
              .map(normalizeTargetProps)
              .filter(target => target && target.key);

            const normalizePath = (value) => {
              if (!value) return '';
              return String(value).replace(/^\.\//, '').replace(/^\/+/, '');
            };

            const workflowMatchesRun = (target, candidate) => {
              if (!candidate) return false;
              const runPath = normalizePath(candidate.path);
              const runWorkflowId = candidate.workflow_id;
              const expectedPath = normalizePath(target.workflowPath);
              const expectedFile = normalizePath(target.workflowFile);
              if (expectedPath && runPath === normalizePath(expectedPath)) {
                return true;
              }
              if (expectedFile && runPath.endsWith(`/${expectedFile}`)) {
                return true;
              }
              if (target.workflowName && candidate.name === target.workflowName) {
                return true;
              }
              if (Array.isArray(target.workflowIds) && target.workflowIds.length) {
                return target.workflowIds.some(id => Number(id) === Number(runWorkflowId));
              }
              return false;
            };

            let repoRuns = [];
            try {
              repoRuns = await github.paginate(
                github.rest.actions.listWorkflowRunsForRepo,
                {
                  owner,
                  repo,
                  head_sha: headSha,
                  per_page: 100,
                },
                (response) => response.data.workflow_runs || [],
              );
            } catch (error) {
              core.warning(`Failed to list workflow runs for repo: ${error}`);
            }

            const runs = [];

            for (const wf of workflowTargets) {
              let latest = null;
              if (repoRuns.length) {
                const matches = repoRuns
                  .filter(candidate => workflowMatchesRun(wf, candidate))
                  .sort((a, b) => {
                    const aTime = a && a.created_at ? new Date(a.created_at).getTime() : 0;
                    const bTime = b && b.created_at ? new Date(b.created_at).getTime() : 0;
                    return bTime - aTime;
                  });
                latest = matches[0] || null;
              }

              if (!latest) {
                runs.push({ key: wf.key, displayName: wf.displayName, present: false });
                continue;
              }

              let jobs = [];
              try {
                jobs = await github.paginate(github.rest.actions.listJobsForWorkflowRun, {
                  owner,
                  repo,
                  run_id: latest.id,
                  per_page: 100,
                }, (response) => response.data.jobs || []);
              } catch (error) {
                core.warning(`Failed to list jobs for run ${latest.id}: ${error}`);
              }

              runs.push({
                key: wf.key,
                displayName: wf.displayName,
                present: true,
                id: latest.id,
                name: latest.name || wf.displayName,
                html_url: latest.html_url,
                status: latest.status,
                conclusion: latest.conclusion,
                run_attempt: latest.run_attempt,
                created_at: latest.created_at,
                updated_at: latest.updated_at,
                head_branch: latest.head_branch,
                jobs: jobs.map(job => ({
                  id: job.id,
                  name: job.name,
                  html_url: job.html_url,
                  status: job.status,
                  conclusion: job.conclusion,
                  started_at: job.started_at,
                  completed_at: job.completed_at,
                })),
              });
            }

            core.setOutput('pr_number', String(pr.number));
            core.setOutput('head_sha', headSha);
            core.setOutput('runs', JSON.stringify(runs));

            const ciRun = runs.find(r => r.key === 'ci' && r.present);
            const dockerRun = runs.find(r => r.key === 'docker' && r.present);
            core.setOutput('ci_run_id', ciRun ? String(ciRun.id) : '');
            core.setOutput('docker_run_id', dockerRun ? String(dockerRun.id) : '');
      - name: Check out repository
        uses: actions/checkout@v4
      - name: Download coverage summary
        if: ${{ steps.gather.outputs.ci_run_id }}
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: coverage-summary
          run-id: ${{ steps.gather.outputs.ci_run_id }}
          github-token: ${{ secrets.GITHUB_TOKEN }}
          path: artifacts/coverage-summary
      - name: Download coverage trend
        if: ${{ steps.gather.outputs.ci_run_id }}
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: coverage-trend
          run-id: ${{ steps.gather.outputs.ci_run_id }}
          github-token: ${{ secrets.GITHUB_TOKEN }}
          path: artifacts/coverage-trend
      - name: Download coverage trend history
        if: ${{ steps.gather.outputs.ci_run_id }}
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: coverage-trend-history
          run-id: ${{ steps.gather.outputs.ci_run_id }}
          github-token: ${{ secrets.GITHUB_TOKEN }}
          path: artifacts/coverage-trend-history
      - name: Capture coverage summary content
        id: coverage_summary
        shell: bash
        run: |
          set -euo pipefail
          FILE="artifacts/coverage-summary/coverage_summary.md"
          if [ -f "$FILE" ]; then
            {
              echo 'body<<EOF'
              cat "$FILE"
              echo 'EOF'
            } >> "$GITHUB_OUTPUT"
          fi
      - name: Derive coverage stats
        id: coverage_stats
        run: |
          python <<'PY'
          import json
          from pathlib import Path

          def load_json(path: Path):
              if not path.is_file():
                  return None
              try:
                  text = path.read_text(encoding='utf-8').strip()
              except Exception:
                  return None
              if not text:
                  return None
              try:
                  return json.loads(text)
              except json.JSONDecodeError:
                  return None

          def load_history(dir_path: Path):
              json_path = dir_path / 'coverage-trend-history.json'
              ndjson_path = dir_path / 'coverage-trend-history.ndjson'
              data = load_json(json_path)
              if isinstance(data, list):
                  return data
              if ndjson_path.is_file():
                  try:
                      lines = [ln for ln in ndjson_path.read_text(encoding='utf-8').splitlines() if ln.strip()]
                      parsed = []
                      for line in lines:
                          try:
                              parsed.append(json.loads(line))
                          except json.JSONDecodeError:
                              continue
                      return parsed
                  except Exception:
                      return []
              return []

          trend = load_json(Path('artifacts/coverage-trend/coverage-trend.json')) or {}
          history = load_history(Path('artifacts/coverage-trend-history'))

          avg_latest = trend.get('avg_coverage')
          worst_latest = trend.get('worst_job_coverage')

          avg_prev = None
          worst_prev = None
          if len(history) >= 2:
              prev = history[-2]
              if isinstance(prev, dict):
                  avg_prev = prev.get('avg_coverage')
                  worst_prev = prev.get('worst_job_coverage')

          def delta(latest, prev):
              try:
                  if latest is None or prev is None:
                      return None
                  return round(float(latest) - float(prev), 2)
              except Exception:
                  return None

          stats = {
              'avg_latest': avg_latest,
              'avg_previous': avg_prev,
              'avg_delta': delta(avg_latest, avg_prev),
              'worst_latest': worst_latest,
              'worst_previous': worst_prev,
              'worst_delta': delta(worst_latest, worst_prev),
              'history_len': len(history),
          }

          print(json.dumps(stats))
          with open('coverage-stats.json', 'w', encoding='utf-8') as handle:
              json.dump(stats, handle)
          PY
          if [ -f coverage-stats.json ]; then
            printf 'stats_json=%s\n' "$(cat coverage-stats.json)" >> "$GITHUB_OUTPUT"
          fi
      - name: Prepare summary body
        id: prep
        run: |
          python tools/post_ci_summary.py
        env:
          RUNS_JSON: ${{ steps.gather.outputs.runs }}
          HEAD_SHA: ${{ steps.gather.outputs.head_sha }}
          COVERAGE_SECTION: ${{ steps.coverage_summary.outputs.body }}
          COVERAGE_STATS: ${{ steps.coverage_stats.outputs.stats_json }}
          REQUIRED_JOB_GROUPS_JSON: ${{ env.REQUIRED_JOB_GROUPS_JSON }}
      - name: Upsert summary comment
        uses: actions/github-script@v7
        env:
          PR_NUMBER: ${{ steps.gather.outputs.pr_number }}
          BODY: ${{ steps.prep.outputs.body }}
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const prNumber = Number(process.env.PR_NUMBER);
            const body = process.env.BODY || '';
            const { owner, repo } = context.repo;
            if (!prNumber || !body) {
              core.info('No PR number or body computed; skipping comment update.');
              return;
            }

            const marker = 'Automated Status Summary';
            const { data: comments } = await github.rest.issues.listComments({
              owner,
              repo,
              issue_number: prNumber,
              per_page: 100,
            });

            const existing = comments.find(comment => comment.body && comment.body.includes(marker));

            if (existing) {
              await github.rest.issues.updateComment({
                owner,
                repo,
                comment_id: existing.id,
                body,
              });
            } else {
              await github.rest.issues.createComment({
                owner,
                repo,
                issue_number: prNumber,
                body,
              });
            }
