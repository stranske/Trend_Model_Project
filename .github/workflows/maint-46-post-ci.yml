name: Maint 46 Post CI

'on':
  workflow_run:
    workflows: ["Gate"]
    types: [completed]

concurrency:
  group: >-
    maint-46-post-ci-${{
      (
        github.event.workflow_run.pull_requests &&
        github.event.workflow_run.pull_requests[0] &&
        github.event.workflow_run.pull_requests[0].number
      ) ||
      github.event.workflow_run.head_sha ||
      github.event.workflow_run.id ||
      github.run_id
    }}
  cancel-in-progress: true

permissions:
  contents: write
  pull-requests: write
  checks: read
  actions: read
  issues: write

env:
  COMMIT_PREFIX: ${{ vars.AUTOFIX_COMMIT_PREFIX || 'chore(autofix):' }}

jobs:
  summarize:
    if: >
      github.event.workflow_run.event == 'pull_request' &&
      github.event.workflow_run.head_sha != ''
    runs-on: ubuntu-latest
    outputs:
      summary_body: ${{ steps.prep.outputs.body }}
      head_sha: ${{ steps.gather.outputs.head_sha }}
      runs: ${{ steps.gather.outputs.runs }}
      coverage_stats: ${{ steps.coverage_stats.outputs.stats_json }}
      coverage_section: ${{ steps.coverage_summary.outputs.body }}
      coverage_delta: ${{ steps.coverage_stats.outputs.delta_json }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Discover workflow runs for head SHA
        id: gather
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const { owner, repo } = context.repo;
            const workflowRun = context.payload.workflow_run || {};
            const prFromPayload = Array.isArray(workflowRun.pull_requests)
              ? workflowRun.pull_requests.find(item => item && item.head && item.head.sha)
              : null;
            const headSha = (prFromPayload?.head?.sha || workflowRun.head_sha || context.sha || '').trim();

            const parseJsonInput = (raw, fallback) => {
              if (!raw) {
                return fallback;
              }
              try {
                return JSON.parse(raw);
              } catch (error) {
                core.warning(`Failed to parse JSON input: ${error}`);
                return fallback;
              }
            };

            const defaultWorkflowTargets = [
              { key: 'gate', display_name: 'Gate', workflow_path: '.github/workflows/pr-00-gate.yml' },
            ];

            const workflowTargetsRaw = process.env.WORKFLOW_TARGETS_JSON;
            const workflowTargetsInput = parseJsonInput(workflowTargetsRaw, defaultWorkflowTargets);
            const workflowTargetsSource = Array.isArray(workflowTargetsInput) ? workflowTargetsInput : defaultWorkflowTargets;
            // Helper to normalize target properties
            function normalizeTargetProps(target) {
              return {
                key: target.key,
                displayName: target.display_name || target.displayName || target.key || 'workflow',
                workflowPath: target.workflow_path || target.workflowPath || '',
                workflowFile: target.workflow_file || target.workflowFile || target.workflow_id || target.workflowId || '',
                workflowName: target.workflow_name || target.workflowName || '',
                workflowIds: Array.isArray(target.workflow_ids) ? target.workflow_ids : (target.workflowIds && Array.isArray(target.workflowIds) ? target.workflowIds : []),
              };
            }

            const workflowTargets = workflowTargetsSource
              .map(normalizeTargetProps)
              .filter(target => target && target.key);

            const normalizePath = (value) => {
              if (!value) return '';
              return String(value).replace(/^\.\//, '').replace(/^\/+/, '');
            };

            async function loadWorkflowRun(identifier) {
              if (!identifier) {
                return null;
              }
              try {
                const response = await github.rest.actions.listWorkflowRuns({
                  owner,
                  repo,
                  workflow_id: identifier,
                  head_sha: headSha || undefined,
                  event: 'pull_request',
                  per_page: 10,
                });
                const runs = response.data.workflow_runs || [];
                if (!runs.length) {
                  return null;
                }
                if (!headSha) {
                  return runs[0];
                }
                const exact = runs.find(item => item.head_sha === headSha);
                return exact || runs[0];
              } catch (error) {
                core.warning(`Failed to query workflow runs for "${identifier}": ${error}`);
                return null;
              }
            }

            async function loadJobs(runId) {
              if (!runId) {
                return [];
              }
              try {
                const jobs = await github.paginate(
                  github.rest.actions.listJobsForWorkflowRun,
                  {
                    owner,
                    repo,
                    run_id: runId,
                    per_page: 100,
                  },
                );
                return jobs
                  .filter(job => job)
                  .map(job => ({
                    name: job.name,
                    conclusion: job.conclusion,
                    status: job.status,
                    html_url: job.html_url,
                  }));
              } catch (error) {
                core.warning(`Failed to query jobs for workflow run ${runId}: ${error}`);
                return [];
              }
            }

            async function resolveRun(target) {
              const candidates = [];
              if (Array.isArray(target.workflowIds) && target.workflowIds.length) {
                for (const id of target.workflowIds) {
                  if (id) {
                    candidates.push(id);
                  }
                }
              }
              if (target.workflowPath) {
                candidates.push(normalizePath(target.workflowPath));
              }
              if (target.workflowFile) {
                candidates.push(normalizePath(target.workflowFile));
              }
              if (target.workflowName) {
                candidates.push(target.workflowName);
              }
              if (!candidates.length) {
                candidates.push(target.key);
              }

              for (const identifier of candidates) {
                const run = await loadWorkflowRun(identifier);
                if (run) {
                  return run;
                }
              }
              return null;
            }

            const collected = [];
            for (const target of workflowTargets) {
              const run = await resolveRun(target);
              if (run) {
                const jobs = await loadJobs(run.id);
                collected.push({
                  key: target.key,
                  displayName: target.displayName,
                  present: true,
                  id: run.id,
                  run_attempt: run.run_attempt,
                  conclusion: run.conclusion,
                  status: run.status,
                  html_url: run.html_url,
                  jobs,
                });
              } else {
                collected.push({
                  key: target.key,
                  displayName: target.displayName,
                  present: false,
                  jobs: [],
                });
              }
            }

            const gateRun = collected.find(entry => entry.key === 'gate' && entry.present);
            const gateRunId = gateRun ? String(gateRun.id) : '';

            core.setOutput('runs', JSON.stringify(collected));
            core.setOutput('ci_run_id', gateRunId);
            core.setOutput('gate_run_id', gateRunId);
            core.setOutput('head_sha', headSha || '');
            core.notice(`Collected ${collected.filter(entry => entry.present).length} Gate workflow runs for head ${headSha}`);
      - name: Download coverage summary
        if: ${{ steps.gather.outputs.ci_run_id }}
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: coverage-summary
          run-id: ${{ steps.gather.outputs.ci_run_id }}
          github-token: ${{ secrets.GITHUB_TOKEN }}
          path: artifacts/coverage-summary
      - name: Download coverage trend
        if: ${{ steps.gather.outputs.ci_run_id }}
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          run-id: ${{ steps.gather.outputs.ci_run_id }}
          pattern: '*'
          merge-multiple: true
          path: summary_artifacts
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - name: Download coverage payloads
        if: ${{ steps.gather.outputs.ci_run_id }}
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          run-id: ${{ steps.gather.outputs.ci_run_id }}
          pattern: 'coverage-3.*'
          path: summary_artifacts/coverage-runtimes
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - name: Load coverage summary snippet
        id: coverage_summary
        run: |
          set -euo pipefail
          mkdir -p summary_artifacts
          python <<'PY'
          import os
          from pathlib import Path

          output_path = os.environ.get('GITHUB_OUTPUT')
          root = Path('summary_artifacts')
          search_roots = [root, Path('artifacts')]

          summary_path: Path | None = None
          candidates = []
          for base in search_roots:
              if not base.exists():
                  continue
              candidates.extend(base.rglob('coverage-summary.md'))
          if candidates:
              # Prefer files in earlier search_roots; if multiple, pick the first found
              summary_path = candidates[0]

          if summary_path is None:
              print('No coverage summary markdown found; continuing without it.')
          else:
              text = summary_path.read_text(encoding='utf-8')
              dest = root / 'coverage-summary.md'
              dest.write_text(text, encoding='utf-8')
              if output_path:
                  with Path(output_path).open('a', encoding='utf-8') as handle:
                      handle.write('body<<EOF\n')
                      handle.write(text)
                      if not text.endswith('\n'):
                          handle.write('\n')
                      handle.write('EOF\n')
              print(f'Embedded coverage summary from {summary_path}')
          PY

      - name: Compute coverage stats
        id: coverage_stats
        run: |
          set -euo pipefail
          python <<'PY'
          import json
          from pathlib import Path
          import xml.etree.ElementTree as ET

          root = Path('summary_artifacts')
          coverage_root = root / 'coverage-runtimes'

          def find_one(pattern: str) -> Path | None:
              if not root.exists():
                  return None
              for candidate in root.rglob(pattern):
                  if candidate.is_file():
                      return candidate
              return None

          def read_coverage(base: Path) -> float | None:
              xml_path = base / 'coverage.xml'
              if xml_path.is_file():
                  try:
                      rate = ET.parse(xml_path).getroot().get('line-rate')
                  except ET.ParseError:
                      rate = None
                  if rate is not None:
                      try:
                          return float(rate) * 100.0
                      except (TypeError, ValueError):
                          pass

              json_path = base / 'coverage.json'
              if json_path.is_file():
                  try:
                      data = json.loads(json_path.read_text(encoding='utf-8'))
                  except json.JSONDecodeError:
                      data = None
                  if isinstance(data, dict):
                      totals = data.get('totals')
                      if isinstance(totals, dict):
                          percent = totals.get('percent_covered')
                          if isinstance(percent, (int, float)):
                              return float(percent)
                          display = totals.get('percent_covered_display')
                          if isinstance(display, str):
                              try:
                                  return float(display)
                              except ValueError:
                                  pass
                          covered_candidates = (
                              totals.get('covered_lines'),
                              totals.get('covered_statements'),
                              totals.get('covered'),
                          )
                          total_candidates = (
                              totals.get('num_statements'),
                              totals.get('num_lines'),
                              totals.get('statements'),
                          )
                          covered_raw = next((value for value in covered_candidates if value is not None), 0)
                          total_raw = next((value for value in total_candidates if value is not None), None)
                          try:
                              covered_value = float(covered_raw)
                              total_value = float(total_raw) if total_raw is not None else None
                          except (TypeError, ValueError):
                              covered_value = None
                              total_value = None
                          if total_value is not None:
                              if total_value == 0:
                                  return 0.0
                              return covered_value / total_value * 100.0 if covered_value is not None else None
              return None

          def label_for(base: Path) -> str | None:
              parts = list(base.parts)
              # Prefer common runtime folder patterns (e.g. runtimes/<version>).
              for index, part in enumerate(parts):
                  if part == "runtimes" and index + 1 < len(parts):
                      return f"coverage-{parts[index + 1]}"
              # Fall back to explicit "coverage-<runtime>" directory names.
              for part in reversed(parts):
                  if part.startswith("coverage-"):
                      return part
              if parts:
                  return f"coverage-{parts[-1]}"
              return None

          def discover_payloads(base: Path) -> list[tuple[str, Path]]:
              if not base.exists():
                  return []
              discovered: list[tuple[str, Path]] = []
              seen_dirs: set[Path] = set()

              def handle(candidate: Path) -> None:
                  directory = candidate.parent
                  if directory in seen_dirs:
                      return
                  label = label_for(directory)
                  if label is None:
                      return
                  discovered.append((label, directory))
                  seen_dirs.add(directory)

              for xml_file in base.rglob("coverage.xml"):
                  handle(xml_file)
              for json_file in base.rglob("coverage.json"):
                  handle(json_file)
              return discovered

          job_coverages: dict[str, float] = {}
          for label, directory in discover_payloads(coverage_root):
              coverage_value = read_coverage(directory)
              if coverage_value is None:
                  continue
              job_coverages[label] = round(float(coverage_value), 2)

          sorted_jobs = sorted(job_coverages.items())
          job_rows: list[dict[str, object]] = []
          table_lines: list[str] = []
          diff_reference: str | None = None
          reference_value: float | None = None
          if sorted_jobs:
              reference_key, reference_value = sorted_jobs[0]
              diff_reference = reference_key[9:] if reference_key.startswith('coverage-') else reference_key
              table_lines = [
                  f"| Runtime | Coverage | Δ vs {diff_reference} |",
                  "| --- | --- | --- |",
              ]
              for index, (name, value) in enumerate(sorted_jobs):
                  label = name[9:] if name.startswith('coverage-') else name
                  if index == 0 or reference_value is None:
                      delta_value = None
                      delta_display = '—'
                  else:
                      delta_value = round(value - reference_value, 2)
                      delta_display = f"{delta_value:+.2f} pp"
                  job_rows.append(
                      {
                          'name': name,
                          'label': label,
                          'coverage': value,
                          'delta_vs_reference': delta_value,
                      }
                  )
                  table_lines.append(f"| {label} | {value:.2f}% | {delta_display} |")

          coverage_table = "\n".join(table_lines) if table_lines else ""
          computed_avg = round(sum(job_coverages.values()) / len(job_coverages), 2) if job_coverages else None
          computed_worst = round(min(job_coverages.values()), 2) if job_coverages else None

          record_path = find_one('coverage-trend.json')
          history_path = find_one('coverage-trend-history.ndjson')
          delta_path = find_one('coverage-delta.json')

          def load_record(path: Path | None) -> dict[str, object] | None:
              if not path:
                  return None
              try:
                  data = json.loads(path.read_text(encoding='utf-8'))
              except Exception:
                  return None
              return data if isinstance(data, dict) else None

          latest_record = load_record(record_path)

          history: list[dict[str, object]] = []
          if history_path and history_path.exists():
              for raw in history_path.read_text(encoding='utf-8').splitlines():
                  line = raw.strip()
                  if not line:
                      continue
                  try:
                      parsed = json.loads(line)
                  except json.JSONDecodeError:
                      continue
                  if isinstance(parsed, dict):
                      history.append(parsed)

          def run_identifier(record: dict[str, object] | None) -> tuple[object | None, object | None]:
              if not isinstance(record, dict):
                  return (None, None)
              return (record.get('run_id'), record.get('run_number'))

          latest_id = run_identifier(latest_record)

          if latest_record is None and history:
              latest_record = history[-1]
              latest_id = run_identifier(latest_record)

          previous_record: dict[str, object] | None = None
          if history:
              for candidate in reversed(history):
                  if run_identifier(candidate) == latest_id:
                      continue
                  previous_record = candidate
                  break
              if previous_record is None and len(history) > 1:
                  previous_record = history[-2]

          def extract(record: dict[str, object] | None, key: str) -> float | None:
              if not isinstance(record, dict):
                  return None
              value = record.get(key)
              try:
                  return float(value) if value is not None else None
              except (TypeError, ValueError):
                  return None

          history_avg_latest = extract(latest_record, 'avg_coverage')
          history_worst_latest = extract(latest_record, 'worst_job_coverage')
          avg_prev = extract(previous_record, 'avg_coverage')
          worst_prev = extract(previous_record, 'worst_job_coverage')

          def delta(latest: float | None, prev: float | None) -> float | None:
              try:
                  if latest is None or prev is None:
                      return None
                  return round(float(latest) - float(prev), 2)
              except Exception:
                  return None

          avg_latest_value = computed_avg if computed_avg is not None else history_avg_latest
          worst_latest_value = computed_worst if computed_worst is not None else history_worst_latest

          stats: dict[str, object] = {
              'avg_latest': avg_latest_value,
              'avg_previous': avg_prev,
              'avg_delta': delta(avg_latest_value, avg_prev),
              'worst_latest': worst_latest_value,
              'worst_previous': worst_prev,
              'worst_delta': delta(worst_latest_value, worst_prev),
              'history_len': len(history),
          }

          if job_rows:
              stats['job_coverages'] = job_rows
              stats['job_count'] = len(job_rows)
          if coverage_table:
              stats['coverage_table_markdown'] = coverage_table
          if diff_reference:
              stats['diff_reference'] = diff_reference

          print(json.dumps(stats))
          Path('coverage-stats.json').write_text(json.dumps(stats), encoding='utf-8')

          delta_payload = load_record(delta_path)
          if delta_payload is not None:
              Path('coverage-delta-output.json').write_text(
                  json.dumps(delta_payload),
                  encoding='utf-8',
              )
          PY
          if [ -f coverage-stats.json ]; then
            printf 'stats_json=%s\n' "$(cat coverage-stats.json)" >> "$GITHUB_OUTPUT"
          fi
          if [ -f coverage-delta-output.json ]; then
            printf 'delta_json=%s\n' "$(cat coverage-delta-output.json)" >> "$GITHUB_OUTPUT"
          fi
      - name: Prepare summary body
        id: prep
        run: |
          python tools/post_ci_summary.py
        env:
          RUNS_JSON: ${{ steps.gather.outputs.runs }}
          HEAD_SHA: ${{ steps.gather.outputs.head_sha }}
          COVERAGE_SECTION: ${{ steps.coverage_summary.outputs.body }}
          COVERAGE_STATS: ${{ steps.coverage_stats.outputs.stats_json }}
          COVERAGE_DELTA: ${{ steps.coverage_stats.outputs.delta_json }}
          REQUIRED_JOB_GROUPS_JSON: ${{ env.REQUIRED_JOB_GROUPS_JSON }}
      - name: Persist summary preview
        run: |
          mkdir -p summary_artifacts
          if [ -n "${SUMMARY_BODY}" ]; then
            printf '%s\n' "${SUMMARY_BODY}" > summary_artifacts/summary_preview.md
          else
            echo "Summary body empty; skipping preview persistence."
          fi
        env:
          SUMMARY_BODY: ${{ steps.prep.outputs.body }}
      - name: Upload coverage artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-summary
          path: |
            summary_artifacts/coverage-summary.md
            coverage-stats.json
            summary_artifacts/summary_preview.md
          if-no-files-found: ignore
      - name: Publish run summary
        run: |
          python <<'PY'
          import os
          import re

          summary_path = os.environ.get("GITHUB_STEP_SUMMARY")
          raw_body = os.environ.get("SUMMARY_BODY") or ""
          body = raw_body.strip()
          is_empty = False
          if not body:
              is_empty = True
              body = "## Automated Status Summary\nAutomated status summary was empty."

          if not summary_path:
              raise SystemExit("GITHUB_STEP_SUMMARY environment variable is not set")

          existing = ""
          if os.path.exists(summary_path):
              with open(summary_path, "r", encoding="utf-8") as handle:
                  existing = handle.read()

          marker = "## Automated Status Summary"
          pattern = re.compile(rf"\n?{re.escape(marker)}.*?(?=\n## |\Z)", re.DOTALL)
          cleaned = pattern.sub("", existing).strip()

          parts = [segment for segment in (cleaned, body) if segment]

          with open(summary_path, "w", encoding="utf-8") as handle:
              if parts:
                  handle.write("\n\n".join(parts))
                  handle.write("\n")

          if is_empty:
              print("Automated status summary was empty.")
          PY
        env:
          SUMMARY_BODY: ${{ steps.prep.outputs.body }}

  context:
    name: Gather context
    if: >
      github.event.workflow_run.event == 'pull_request' &&
      github.event.workflow_run.head_sha != ''
    runs-on: ubuntu-latest
    outputs:
      found: ${{ steps.info.outputs.found }}
      pr: ${{ steps.info.outputs.pr }}
      head_ref: ${{ steps.info.outputs.head_ref }}
      head_sha: ${{ steps.info.outputs.head_sha }}
      same_repo: ${{ steps.info.outputs.same_repo }}
      loop_skip: ${{ steps.info.outputs.loop_skip }}
      small_eligible: ${{ steps.info.outputs.small_eligible }}
      file_count: ${{ steps.info.outputs.file_count }}
      change_count: ${{ steps.info.outputs.change_count }}
      safe_paths: ${{ steps.info.outputs.safe_paths }}
      unsafe_paths: ${{ steps.info.outputs.unsafe_paths }}
      safe_file_count: ${{ steps.info.outputs.safe_file_count }}
      unsafe_file_count: ${{ steps.info.outputs.unsafe_file_count }}
      safe_change_count: ${{ steps.info.outputs.safe_change_count }}
      unsafe_change_count: ${{ steps.info.outputs.unsafe_change_count }}
      all_safe: ${{ steps.info.outputs.all_safe }}
      has_opt_in: ${{ steps.info.outputs.has_opt_in }}
      has_patch_label: ${{ steps.info.outputs.has_patch_label }}
      is_draft: ${{ steps.info.outputs.is_draft }}
      run_conclusion: ${{ steps.info.outputs.run_conclusion }}
      actor: ${{ steps.info.outputs.actor }}
      head_subject: ${{ steps.info.outputs.head_subject }}
      failure_tracker_skip: ${{ steps.info.outputs.failure_tracker_skip }}
      pat_available: ${{ steps.pat.outputs.available }}
      trivial_failure: ${{ steps.failure.outputs.trivial }}
      failing_jobs: ${{ steps.failure.outputs.names }}
      failing_count: ${{ steps.failure.outputs.count }}
      failure_incomplete: ${{ steps.failure.outputs.incomplete }}
      failure_has_jobs: ${{ steps.failure.outputs.has_jobs }}
    env:
      AUTOFIX_OPT_IN_LABEL: ${{ vars.AUTOFIX_OPT_IN_LABEL || 'autofix' }}
      AUTOFIX_PATCH_LABEL: ${{ vars.AUTOFIX_PATCH_LABEL || 'autofix:patch' }}
      AUTOFIX_TRIVIAL_KEYWORDS: ${{ vars.AUTOFIX_TRIVIAL_KEYWORDS || 'lint,format,style,doc,ruff,mypy,type,black,isort,label,test' }}
      AUTOFIX_MAX_FILES: ${{ vars.AUTOFIX_MAX_FILES || '40' }}
      AUTOFIX_MAX_CHANGES: ${{ vars.AUTOFIX_MAX_CHANGES || '800' }}
    steps:
      - name: Check PAT availability
        id: pat
        shell: bash
        run: |
          if [ -z "${{ secrets.SERVICE_BOT_PAT }}" ]; then
            echo "available=false" >> "$GITHUB_OUTPUT"
          else
            echo "available=true" >> "$GITHUB_OUTPUT"
          fi

      - name: Resolve workflow context
        id: info
        uses: actions/github-script@v7
        with:
          script: |
            const run = context.payload.workflow_run;
            const owner = context.repo.owner;
            const repo = context.repo.repo;
            const prefix = process.env.COMMIT_PREFIX || 'chore(autofix):';
            const payloadPr = run && Array.isArray(run.pull_requests)
              ? run.pull_requests.find(item => item && typeof item.number === 'number')
              : null;
            const branch = (payloadPr?.head?.ref || run?.head_branch || '').trim();
            const headSha = (payloadPr?.head?.sha || run?.head_sha || '').trim();

            const result = {
              found: 'false',
              pr: '',
              head_ref: branch || '',
              head_sha: headSha || '',
              same_repo: 'false',
              loop_skip: 'false',
              small_eligible: 'false',
              file_count: '0',
              change_count: '0',
              safe_paths: '',
              unsafe_paths: '',
              safe_file_count: '0',
              unsafe_file_count: '0',
              safe_change_count: '0',
              unsafe_change_count: '0',
              all_safe: 'false',
              has_opt_in: 'false',
              has_patch_label: 'false',
              is_draft: run.event === 'pull_request' && run.head_repository ? (run.pull_requests?.[0]?.draft ? 'true' : 'false') : 'false',
              run_conclusion: run.conclusion || '',
              actor: (run.triggering_actor?.login || run.actor?.login || '').toLowerCase(),
              head_subject: '',
              failure_tracker_skip: 'false',
            };

            if (!branch || !headSha) {
              core.info('Workflow run missing branch or head SHA; skipping.');
              for (const [key, value] of Object.entries(result)) {
                core.setOutput(key, value);
              }
              return;
            }

            const headShaLower = (headSha || '').toLowerCase();
            let pr = null;
            let prNumber = null;

            if (payloadPr) {
              prNumber = Number(payloadPr.number);
              if (!Number.isNaN(prNumber)) {
                try {
                  const prResponse = await github.rest.pulls.get({ owner, repo, pull_number: prNumber });
                  pr = prResponse.data;
                } catch (error) {
                  core.warning(`Failed to load PR #${prNumber} from workflow payload: ${error.message}`);
                }
              }
            }

            let openPrs = [];
            if (!pr) {
              openPrs = await github.paginate(github.rest.pulls.list, {
                owner,
                repo,
                state: 'open',
                per_page: 100,
              });

              if (headShaLower) {
                pr = openPrs.find(item => (item.head?.sha || '').toLowerCase() === headShaLower) || null;
              }

              if (!pr && branch) {
                const branchLower = branch.toLowerCase();
                pr = openPrs.find(item => (item.head?.ref || '').toLowerCase() === branchLower) || null;
              }

              if (!pr && openPrs.length) {
                pr = openPrs[0];
              }
            }

            if (!pr) {
              core.info(`Unable to locate an open PR for workflow run (head_sha=${headSha}, branch=${branch || 'n/a'})`);
              for (const [key, value] of Object.entries(result)) {
                core.setOutput(key, value);
              }
              return;
            }

            prNumber = Number(pr.number);
            result.found = 'true';
            result.pr = String(prNumber);
            result.head_ref = pr.head?.ref || branch;
            result.head_sha = pr.head?.sha || headSha;
            result.same_repo = pr.head?.repo?.full_name === `${owner}/${repo}` ? 'true' : 'false';
            result.is_draft = pr.draft ? 'true' : 'false';

            const resolvedHeadRef = result.head_ref || branch || 'unknown-ref';
            const resolvedHeadSha = result.head_sha || headSha || 'unknown-sha';
            const gateRunId = run?.id ? String(run.id) : 'unknown-run';
            core.notice(
              `Resolved PR #${result.pr} (${resolvedHeadRef} @ ${resolvedHeadSha}) for Gate run ${gateRunId}.`,
            );

            const failureTrackerSkipPrs = new Set([10, 12]);
            if (failureTrackerSkipPrs.has(prNumber)) {
              core.info(`PR #${prNumber} flagged to skip failure tracker updates (legacy duplicate).`);
              result.failure_tracker_skip = 'true';
            }

            const labels = Array.isArray(pr.labels)
              ? pr.labels
                  .filter(label => label && typeof label.name === 'string')
                  .map(label => label.name)
              : [];
            const optLabel = process.env.AUTOFIX_OPT_IN_LABEL || 'autofix';
            const patchLabel = process.env.AUTOFIX_PATCH_LABEL || 'autofix:patch';
            result.has_opt_in = labels.includes(optLabel) ? 'true' : 'false';
            result.has_patch_label = labels.includes(patchLabel) ? 'true' : 'false';

            try {
              const commit = await github.rest.repos.getCommit({ owner, repo, ref: result.head_sha });
              const subject = (commit.data.commit.message || '').split('\n')[0];
              result.head_subject = subject;
              const actor = result.actor;
              const isAutomation = actor === 'github-actions' || actor === 'github-actions[bot]';
              const subjectLower = subject.toLowerCase();
              const prefixLower = prefix.toLowerCase();
              if (isAutomation && prefixLower && subjectLower.startsWith(prefixLower)) {
                core.info(`Loop guard engaged for actor ${actor}: detected prior autofix commit.`);
                result.loop_skip = 'true';
              }
            } catch (error) {
              core.warning(`Unable to inspect commit message for loop guard: ${error.message}`);
            }

            if (result.found === 'true') {
              const files = await github.paginate(github.rest.pulls.listFiles, {
                owner,
                repo,
                pull_number: pr.number,
                per_page: 100,
              });
              const safeSuffixes = ['.py', '.pyi', '.toml', '.cfg', '.ini'];
              const safeBasenames = new Set([
                'pyproject.toml',
                'ruff.toml',
                '.ruff.toml',
                'mypy.ini',
                '.pre-commit-config.yaml',
                'pytest.ini',
                '.coveragerc',
              ].map(name => name.toLowerCase()));
              const isSafePath = (filepath) => {
                const lower = filepath.toLowerCase();
                if (safeSuffixes.some(suffix => lower.endsWith(suffix))) {
                  return true;
                }
                for (const name of safeBasenames) {
                  if (lower === name || lower.endsWith(`/${name}`)) {
                    return true;
                  }
                }
                return false;
              };
              const totalFiles = files.length;
              const totalChanges = files.reduce((acc, file) => acc + (file.changes || 0), 0);
              const safeFiles = files.filter(file => isSafePath(file.filename));
              const unsafeFiles = files.filter(file => !isSafePath(file.filename));
              const safeChanges = safeFiles.reduce((acc, file) => acc + (file.changes || 0), 0);
              const unsafeChanges = totalChanges - safeChanges;
              const allSafe = unsafeFiles.length === 0;
              const limitFiles = Number(process.env.AUTOFIX_MAX_FILES || 40);
              const limitChanges = Number(process.env.AUTOFIX_MAX_CHANGES || 800);
              const baseEligible = pr.draft ? labels.includes(optLabel) : true;
              const safeEligible = baseEligible && safeFiles.length > 0 && safeFiles.length <= limitFiles && safeChanges <= limitChanges;
              result.small_eligible = safeEligible ? 'true' : 'false';
              result.file_count = String(totalFiles);
              result.change_count = String(totalChanges);
              result.safe_paths = safeFiles.map(file => file.filename).join('\n');
              result.unsafe_paths = unsafeFiles.map(file => file.filename).join('\n');
              result.safe_file_count = String(safeFiles.length);
              result.unsafe_file_count = String(unsafeFiles.length);
              result.safe_change_count = String(safeChanges);
              result.unsafe_change_count = String(unsafeChanges);
              result.all_safe = allSafe ? 'true' : 'false';
            }

            for (const [key, value] of Object.entries(result)) {
              core.setOutput(key, value ?? '');
            }

      - name: Inspect failing jobs
        id: failure
        uses: actions/github-script@v7
        with:
          script: |
            const run = context.payload.workflow_run;
            const owner = context.repo.owner;
            const repo = context.repo.repo;
            const conclusion = (run.conclusion || '').toLowerCase();
            const setOutputs = ({
              trivial = 'false',
              names = '',
              count = '0',
              incomplete = 'false',
              hasJobs = 'false',
            } = {}) => {
              core.setOutput('trivial', trivial);
              core.setOutput('names', names);
              core.setOutput('count', count);
              core.setOutput('incomplete', incomplete);
              core.setOutput('has_jobs', hasJobs);
            };

            if (!run.id) {
              setOutputs({ incomplete: 'true' });
              return;
            }

            if (conclusion === 'success') {
              setOutputs();
              return;
            }

            if (conclusion && conclusion !== 'failure') {
              setOutputs({ incomplete: 'true' });
              return;
            }

            const keywords = (process.env.AUTOFIX_TRIVIAL_KEYWORDS || 'lint,format,style,doc,ruff,mypy,type,black,isort,label,test').split(',')
              .map(str => str.trim().toLowerCase())
              .filter(Boolean);

            const jobs = await github.paginate(github.rest.actions.listJobsForWorkflowRun, {
              owner,
              repo,
              run_id: run.id,
              per_page: 100,
            });

            const failing = jobs.filter(job => {
              const c = (job.conclusion || '').toLowerCase();
              return c && c !== 'success' && c !== 'skipped';
            });

            if (!failing.length) {
              setOutputs();
              return;
            }

            const actionableConclusions = new Set(['failure']);
            const incomplete = failing.some(job => !actionableConclusions.has((job.conclusion || '').toLowerCase()));
            const allTrivial = failing.every(job => {
              const name = (job.name || '').toLowerCase();
              return keywords.some(keyword => name.includes(keyword));
            });

            setOutputs({
              trivial: allTrivial ? 'true' : 'false',
              names: failing.map(job => job.name).join(', '),
              count: String(failing.length),
              incomplete: incomplete ? 'true' : 'false',
              hasJobs: 'true',
            });

  small-fixes:
    name: Small hygiene fixes
    needs: context
    if: |
      needs.context.outputs.found == 'true' &&
      needs.context.outputs.loop_skip != 'true' &&
      needs.context.outputs.small_eligible == 'true' &&
      (
        github.event.workflow_run.conclusion == 'success' ||
        (
          github.event.workflow_run.conclusion == 'failure' &&
          needs.context.outputs.trivial_failure == 'true' &&
          needs.context.outputs.failure_incomplete != 'true' &&
          needs.context.outputs.failure_has_jobs == 'true'
        )
      ) &&
      github.event.workflow_run.conclusion != 'cancelled' &&
      github.event.workflow_run.conclusion != 'timed_out' &&
      (needs.context.outputs.same_repo != 'true' || needs.context.outputs.pat_available == 'true')
    runs-on: ubuntu-latest
    env:
      APPLIED_LABEL: ${{ vars.AUTOFIX_APPLIED_LABEL || 'autofix:applied' }}
      PATCH_LABEL: ${{ vars.AUTOFIX_PATCH_LABEL || 'autofix:patch' }}
      AUTOFIX_TRIGGER_CONCLUSION: ${{ github.event.workflow_run.conclusion }}
      AUTOFIX_TRIGGER_CLASS: ${{ github.event.workflow_run.conclusion == 'failure' && needs.context.outputs.trivial_failure == 'true' && 'trivial-failure' || github.event.workflow_run.conclusion }}
      AUTOFIX_TRIGGER_PR_HEAD: ${{ needs.context.outputs.head_sha }}
      AUTOFIX_TRIGGER_REASON: ${{ github.event.workflow_run.conclusion == 'failure' && needs.context.outputs.trivial_failure == 'true' && 'trivial-failure' || github.event.workflow_run.conclusion }}
    outputs:
      ran: ${{ steps.capture.outputs.ran }}
      changed: ${{ steps.capture.outputs.changed }}
      remaining: ${{ steps.capture.outputs.remaining }}
      new: ${{ steps.capture.outputs.new }}
      patch_available: ${{ steps.capture.outputs.patch_available }}
      skip_reason: ${{ steps.capture.outputs.skip_reason }}
      trigger_conclusion: ${{ steps.capture.outputs.trigger_conclusion }}
      trigger_class: ${{ steps.capture.outputs.trigger_class }}
      trigger_reason: ${{ steps.capture.outputs.trigger_reason }}
      trigger_head: ${{ steps.capture.outputs.trigger_head }}
    steps:
      - name: Autofix path summary
        shell: bash
        env:
          SAFE_COUNT: ${{ needs.context.outputs.safe_file_count }}
          UNSAFE_COUNT: ${{ needs.context.outputs.unsafe_file_count }}
          SAFE_PATHS: ${{ needs.context.outputs.safe_paths }}
          UNSAFE_PATHS: ${{ needs.context.outputs.unsafe_paths }}
        run: |
          set -euo pipefail
          echo "Safe autofix file count: ${SAFE_COUNT:-0}"
          echo "Unsafe file count: ${UNSAFE_COUNT:-0}"
          echo "--- Safe paths ---"
          if [ -n "${SAFE_PATHS}" ]; then
            printf '%s\n' "${SAFE_PATHS}"
          else
            echo "(none)"
          fi
          echo "--- Skipped paths ---"
          if [ -n "${UNSAFE_PATHS}" ]; then
            printf '%s\n' "${UNSAFE_PATHS}"
          else
            echo "(none)"
          fi

      - name: Determine rerun eligibility
        id: rerun
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.SERVICE_BOT_PAT || github.token }}
          script: |
            const prNumber = Number('${{ needs.context.outputs.pr }}');
            const headSha = ('${{ needs.context.outputs.head_sha }}' || '').toLowerCase();
            const sameRepo = '${{ needs.context.outputs.same_repo }}' === 'true';
            const hasPatchLabel = '${{ needs.context.outputs.has_patch_label }}' === 'true';
            const markerPrefix = '<!-- autofix-meta:';

            const setSkip = (reason) => {
              core.setOutput('skip', 'true');
              if (reason) {
                core.exportVariable('AUTOFIX_SKIP_REASON', reason);
              }
            };

            core.exportVariable('AUTOFIX_SKIP_REASON', '');

            if (!prNumber || !headSha) {
              core.setOutput('skip', 'false');
              return;
            }

            if (sameRepo || !hasPatchLabel) {
              core.setOutput('skip', 'false');
              return;
            }

            const comments = await github.paginate(github.rest.issues.listComments, {
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: prNumber,
              per_page: 100,
            });

            let storedHead = '';
            for (const comment of comments) {
              const body = comment.body || '';
              if (!body.includes(markerPrefix)) {
                continue;
              }
              const match = body.match(/<!--\s*autofix-meta:[^>]*head=([0-9a-f]+)/i);
              if (match) {
                storedHead = match[1].toLowerCase();
                break;
              }
            }

            if (storedHead && storedHead === headSha) {
              core.info(`Autofix patch already generated for commit ${headSha}; skipping rerun.`);
              setSkip('duplicate-patch');
            } else {
              core.setOutput('skip', 'false');
            }

      - name: Checkout workflow repository
        if: steps.rerun.outputs.skip != 'true'
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Load formatter pins
        if: steps.rerun.outputs.skip != 'true'
        run: |
          set -euo pipefail
          pin_file=".github/workflows/autofix-versions.env"
          if [ ! -f "$pin_file" ]; then
            echo "::error::Missing $pin_file" >&2
            exit 1
          fi
          # shellcheck source=/dev/null
          source "$pin_file"
          for var in BLACK_VERSION RUFF_VERSION ISORT_VERSION DOCFORMATTER_VERSION MYPY_VERSION; do
            if [ -z "${!var:-}" ]; then
              echo "::error::Missing ${var} in $pin_file" >&2
              exit 1
            fi
          done
          cat "$pin_file" >> "$GITHUB_ENV"
      - name: Apply autofix and deliver changes
        id: apply
        if: steps.rerun.outputs.skip != 'true'
        uses: ./.github/actions/apply-autofix
        with:
          head_ref: ${{ needs.context.outputs.head_ref }}
          same_repo: ${{ needs.context.outputs.same_repo }}
          commit_prefix: ${{ env.COMMIT_PREFIX }}
          pr: ${{ needs.context.outputs.pr }}
          safe_paths: ${{ needs.context.outputs.safe_paths }}
          service_bot_pat: ${{ secrets.SERVICE_BOT_PAT }}
          github_token: ${{ github.token }}

      - name: Label PR (autofix applied)
        if: steps.rerun.outputs.skip != 'true' && steps.apply.outputs.changed == 'true' && needs.context.outputs.same_repo == 'true'
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.SERVICE_BOT_PAT || github.token }}
          script: |
            const pr = Number('${{ needs.context.outputs.pr }}');
            const label = process.env.APPLIED_LABEL || 'autofix:applied';
            try {
              await github.rest.issues.addLabels({ owner: context.repo.owner, repo: context.repo.repo, issue_number: pr, labels: [label] });
            } catch (error) {
              core.warning(`Failed to add label: ${error.message}`);
            }
      - name: Label PR (autofix patch available)
        if: steps.rerun.outputs.skip != 'true' && steps.apply.outputs.changed == 'true' && needs.context.outputs.same_repo != 'true'
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.SERVICE_BOT_PAT || github.token }}
          script: |
            const pr = Number('${{ needs.context.outputs.pr }}');
            const label = process.env.PATCH_LABEL || 'autofix:patch';
            try {
              await github.rest.issues.addLabels({ owner: context.repo.owner, repo: context.repo.repo, issue_number: pr, labels: [label] });
            } catch (error) {
              core.warning(`Failed to add patch label: ${error.message}`);
            }
      - name: Manage clean/debt labels
        if: steps.rerun.outputs.skip != 'true' && needs.context.outputs.same_repo == 'true'
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.SERVICE_BOT_PAT || github.token }}
          script: |
            const pr = Number('${{ needs.context.outputs.pr }}');
            const remaining = Number("${{ steps.apply.outputs.remaining_issues || '0' }}") || 0;
            const cleanLabel = 'autofix:clean';
            const debtLabel = 'autofix:debt';
            const patchLabel = process.env.PATCH_LABEL || 'autofix:patch';
            const want = remaining === 0 ? cleanLabel : debtLabel;
            const drop = remaining === 0 ? debtLabel : cleanLabel;
            try {
              await github.rest.issues.removeLabel({ owner: context.repo.owner, repo: context.repo.repo, issue_number: pr, name: drop }).catch(() => {});
              await github.rest.issues.addLabels({ owner: context.repo.owner, repo: context.repo.repo, issue_number: pr, labels: [want] });
              await github.rest.issues.removeLabel({ owner: context.repo.owner, repo: context.repo.repo, issue_number: pr, name: patchLabel }).catch(() => {});
            } catch (error) {
              core.warning(`Label management warning: ${error.message}`);
            }

      - name: Update residual history (same-repo)
        if: |
          steps.rerun.outputs.skip != 'true' &&
          needs.context.outputs.same_repo == 'true' &&
          hashFiles('.github/actions/update-residual-history/action.yml') != ''
        uses: ./.github/actions/update-residual-history

      - name: Upload autofix history artifact (same-repo)
        if: steps.rerun.outputs.skip != 'true' && needs.context.outputs.same_repo == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: autofix-history-pr-${{ needs.context.outputs.pr }}
          path: ci/autofix/history.json
          if-no-files-found: ignore

      - name: Generate trend sparkline (same-repo)
        if: steps.rerun.outputs.skip != 'true' && needs.context.outputs.same_repo == 'true'
        shell: bash
        run: |
          if [ -f scripts/generate_residual_trend.py ]; then python scripts/generate_residual_trend.py || true; fi
          if [ -f ci/autofix/trend.json ]; then echo "Trend:"; cat ci/autofix/trend.json; fi

      - name: Emit JSON report
        if: always()
        shell: bash
        env:
          PR_NUMBER: ${{ needs.context.outputs.pr }}
          CHANGED: ${{ steps.apply.outputs.changed }}
          REMAINING: ${{ steps.apply.outputs.remaining_issues }}
          NEW_ISSUES: ${{ steps.apply.outputs.new_issues }}
        run: |
          set -euo pipefail
          ts=$(date -u +%Y-%m-%dT%H:%M:%SZ)
          if [ -f autofix_report_enriched.json ]; then
            if python scripts/merge_autofix_report.py \
              --input autofix_report_enriched.json \
              --output autofix_report.json \
              --pr-number "${PR_NUMBER}" \
              --timestamp "${ts}"; then
              :
            else
              printf '{\n  "changed": "%s",\n  "remaining_issues": "%s",\n  "new_issues": "%s",\n  "pull_request": "%s",\n  "timestamp_utc": "%s"\n}\n' \
                "${CHANGED}" \
                "${REMAINING}" \
                "${NEW_ISSUES}" \
                "${PR_NUMBER}" \
                "${ts}" > autofix_report.json
            fi
          else
            printf '{\n  "changed": "%s",\n  "remaining_issues": "%s",\n  "new_issues": "%s",\n  "pull_request": "%s",\n  "timestamp_utc": "%s"\n}\n' \
              "${CHANGED}" \
              "${REMAINING}" \
              "${NEW_ISSUES}" \
              "${PR_NUMBER}" \
              "${ts}" > autofix_report.json
          fi
          echo "Enriched report ready."

      - name: Upload JSON report
        uses: actions/upload-artifact@v4
        with:
          name: autofix-report-pr-${{ needs.context.outputs.pr }}
          path: autofix_report.json

      - name: Summary
        if: always()
        run: |
          {
            echo "### Small fixes"
            echo "PR: #${{ needs.context.outputs.pr }}"
            echo "Files considered: ${{ needs.context.outputs.file_count }} (safe paths: ${{ needs.context.outputs.safe_paths }})"
            echo "Changes applied: ${{ steps.apply.outputs.changed }}"
            echo "Remaining ruff issues: ${{ steps.apply.outputs.remaining_issues }}"
            if [ "${{ steps.apply.outputs.changed }}" = "true" ] && [ "${{ needs.context.outputs.same_repo }}" != "true" ]; then
              patch_url="${GITHUB_SERVER_URL:-https://github.com}/${GITHUB_REPOSITORY}/actions/runs/${GITHUB_RUN_ID}?check_suite_focus=true#artifacts"
              echo "Patch artifact: ${patch_url}"
            fi
          } >> "$GITHUB_STEP_SUMMARY"

      - name: Capture job summary
        id: capture
        if: always()
        run: |
          set -euo pipefail
          changed="${APPLY_CHANGED:-false}"
          remaining="${APPLY_REMAINING:-0}"
          new_issues="${APPLY_NEW:-0}"
          skip_flag="${RERUN_SKIP:-}"
          same_repo="${SAME_REPO:-false}"
          ran="false"
          if [ "${skip_flag}" != "true" ]; then
            ran="true"
          fi
          patch="false"
          if [ "${ran}" = "true" ] && [ "${changed}" = "true" ] && [ "${same_repo}" != "true" ]; then
            patch="true"
          fi
          {
            echo "ran=${ran}"
            echo "changed=${changed:-false}"
            echo "remaining=${remaining:-0}"
            echo "new=${new_issues:-0}"
            echo "patch_available=${patch}"
            echo "skip_reason=${AUTOFIX_SKIP_REASON:-}"
            echo "trigger_conclusion=${AUTOFIX_TRIGGER_CONCLUSION:-}"
            echo "trigger_class=${AUTOFIX_TRIGGER_CLASS:-}"
            echo "trigger_reason=${AUTOFIX_TRIGGER_REASON:-}"
            echo "trigger_head=${AUTOFIX_TRIGGER_PR_HEAD:-}"
          } >> "$GITHUB_OUTPUT"
        env:
          APPLY_CHANGED: ${{ steps.apply.outputs.changed }}
          APPLY_REMAINING: ${{ steps.apply.outputs.remaining_issues }}
          APPLY_NEW: ${{ steps.apply.outputs.new_issues }}
          RERUN_SKIP: ${{ steps.rerun.outputs.skip }}
          SAME_REPO: ${{ needs.context.outputs.same_repo }}
          AUTOFIX_SKIP_REASON: ${{ env.AUTOFIX_SKIP_REASON }}
          AUTOFIX_TRIGGER_CONCLUSION: ${{ env.AUTOFIX_TRIGGER_CONCLUSION }}
          AUTOFIX_TRIGGER_CLASS: ${{ env.AUTOFIX_TRIGGER_CLASS }}
          AUTOFIX_TRIGGER_REASON: ${{ env.AUTOFIX_TRIGGER_REASON }}
          AUTOFIX_TRIGGER_PR_HEAD: ${{ env.AUTOFIX_TRIGGER_PR_HEAD }}

  fix-failing-checks:
    name: Fix failing checks
    needs: context
    if: |
      needs.context.outputs.found == 'true' &&
      needs.context.outputs.loop_skip != 'true' &&
      needs.context.outputs.trivial_failure == 'true' &&
      (needs.context.outputs.same_repo != 'true' || needs.context.outputs.pat_available == 'true')
    runs-on: ubuntu-latest
    env:
      APPLIED_LABEL: ${{ vars.AUTOFIX_APPLIED_LABEL || 'autofix:applied' }}
      AUTOFIX_TRIGGER_CONCLUSION: ${{ github.event.workflow_run.conclusion }}
      AUTOFIX_TRIGGER_CLASS: ${{ github.event.workflow_run.conclusion == 'failure' && needs.context.outputs.trivial_failure == 'true' && 'trivial-failure' || github.event.workflow_run.conclusion }}
      AUTOFIX_TRIGGER_PR_HEAD: ${{ needs.context.outputs.head_sha }}
      AUTOFIX_TRIGGER_REASON: ${{ github.event.workflow_run.conclusion == 'failure' && needs.context.outputs.trivial_failure == 'true' && 'trivial-failure' || github.event.workflow_run.conclusion }}
    outputs:
      ran: ${{ steps.capture.outputs.ran }}
      changed: ${{ steps.capture.outputs.changed }}
      remaining: ${{ steps.capture.outputs.remaining }}
      new: ${{ steps.capture.outputs.new }}
      patch_available: ${{ steps.capture.outputs.patch_available }}
      skip_reason: ${{ steps.capture.outputs.skip_reason }}
      trigger_conclusion: ${{ steps.capture.outputs.trigger_conclusion }}
      trigger_class: ${{ steps.capture.outputs.trigger_class }}
      trigger_reason: ${{ steps.capture.outputs.trigger_reason }}
      trigger_head: ${{ steps.capture.outputs.trigger_head }}
    steps:
      - name: Checkout workflow repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - name: Load formatter pins
        run: |
          set -euo pipefail
          pin_file=".github/workflows/autofix-versions.env"
          if [ ! -f "$pin_file" ]; then
            echo "::error::Missing $pin_file" >&2
            exit 1
          fi
          # shellcheck source=/dev/null
          source "$pin_file"
          for var in BLACK_VERSION RUFF_VERSION ISORT_VERSION DOCFORMATTER_VERSION MYPY_VERSION; do
            if [ -z "${!var:-}" ]; then
              echo "::error::Missing ${var} in $pin_file" >&2
              exit 1
            fi
          done
          cat "$pin_file" >> "$GITHUB_ENV"
      - name: Autofix, commit, and push (composite)
        id: autofix
        uses: ./.github/actions/autofix-commit-push
        with:
          head_ref: ${{ needs.context.outputs.head_ref }}
          token: ${{ needs.context.outputs.same_repo == 'true' && secrets.SERVICE_BOT_PAT || github.token }}
          same_repo: ${{ needs.context.outputs.same_repo }}
          commit_message: "${COMMIT_PREFIX} fix failing checks"
          pr: ${{ needs.context.outputs.pr }}
      - name: Label PR (autofix applied)
        if: steps.autofix.outputs.changed == 'true' && needs.context.outputs.same_repo == 'true'
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.SERVICE_BOT_PAT || github.token }}
          script: |
            const pr = Number('${{ needs.context.outputs.pr }}');
            const label = process.env.APPLIED_LABEL || 'autofix:applied';
            try {
              await github.rest.issues.addLabels({ owner: context.repo.owner, repo: context.repo.repo, issue_number: pr, labels: [label] });
            } catch (error) {
              core.warning(`Failed to add label: ${error.message}`);
            }

      - name: Tag PR for manual review
        if: steps.autofix.outputs.changed != 'true'
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.SERVICE_BOT_PAT || github.token }}
          script: |
            const pr = Number('${{ needs.context.outputs.pr }}');
            const label = 'needs-autofix-review';
            try {
              await github.rest.issues.addLabels({ owner: context.repo.owner, repo: context.repo.repo, issue_number: pr, labels: [label] });
            } catch (error) {
              core.warning(`Failed to add label: ${error.message}`);
            }

      - name: Update residual history (same-repo)
        if: |
          needs.context.outputs.same_repo == 'true' &&
          hashFiles('.github/actions/update-residual-history/action.yml') != ''
        uses: ./.github/actions/update-residual-history

      - name: Upload autofix history artifact (same-repo)
        if: needs.context.outputs.same_repo == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: autofix-history-pr-${{ needs.context.outputs.pr }}
          path: ci/autofix/history.json
          if-no-files-found: ignore

      - name: Generate trend sparkline (same-repo)
        if: needs.context.outputs.same_repo == 'true'
        shell: bash
        run: |
          if [ -f scripts/generate_residual_trend.py ]; then python scripts/generate_residual_trend.py || true; fi
          if [ -f ci/autofix/trend.json ]; then echo "Trend:"; cat ci/autofix/trend.json; fi

      - name: Emit JSON report
        if: always()
        shell: bash
        env:
          PR_NUMBER: ${{ needs.context.outputs.pr }}
          CHANGED: ${{ steps.autofix.outputs.changed }}
          REMAINING: ${{ steps.autofix.outputs.remaining_issues }}
          NEW_ISSUES: ${{ steps.autofix.outputs.new_issues }}
        run: |
          set -euo pipefail
          ts=$(date -u +%Y-%m-%dT%H:%M:%SZ)
          if [ -f autofix_report_enriched.json ]; then
            if python scripts/merge_autofix_report.py \
              --input autofix_report_enriched.json \
              --output autofix_report.json \
              --pr-number "${PR_NUMBER}" \
              --timestamp "${ts}"; then
              :
            else
              printf '{\n  "changed": "%s",\n  "remaining_issues": "%s",\n  "new_issues": "%s",\n  "pull_request": "%s",\n  "timestamp_utc": "%s"\n}\n' \
                "${CHANGED}" \
                "${REMAINING}" \
                "${NEW_ISSUES}" \
                "${PR_NUMBER}" \
                "${ts}" > autofix_report.json
            fi
          else
            printf '{\n  "changed": "%s",\n  "remaining_issues": "%s",\n  "new_issues": "%s",\n  "pull_request": "%s",\n  "timestamp_utc": "%s"\n}\n' \
              "${CHANGED}" \
              "${REMAINING}" \
              "${NEW_ISSUES}" \
              "${PR_NUMBER}" \
              "${ts}" > autofix_report.json
          fi
          echo "Enriched report ready."

      - name: Upload JSON report
        uses: actions/upload-artifact@v4
        with:
          name: autofix-report-pr-${{ needs.context.outputs.pr }}
          path: autofix_report.json

      - name: Summary
        if: always()
        run: |
          {
            echo "### Fix failing checks"
            echo "PR: #${{ needs.context.outputs.pr }}"
            echo "Changes applied: ${{ steps.autofix.outputs.changed }}"
            echo "Remaining ruff issues: ${{ steps.autofix.outputs.remaining_issues }}"
            echo "New ruff issues: ${{ steps.autofix.outputs.new_issues }}"
          } >> "$GITHUB_STEP_SUMMARY"

      - name: Capture job summary
        id: capture
        if: always()
        run: |
          set -euo pipefail
          changed="${AUTOFIX_CHANGED:-false}"
          remaining="${AUTOFIX_REMAINING:-0}"
          new_issues="${AUTOFIX_NEW:-0}"
          same_repo="${SAME_REPO:-false}"
          ran="true"
          patch="false"
          if [ "${changed}" = "true" ] && [ "${same_repo}" != "true" ]; then
            patch="true"
          fi
          {
            echo "ran=${ran}"
            echo "changed=${changed:-false}"
            echo "remaining=${remaining:-0}"
            echo "new=${new_issues:-0}"
            echo "patch_available=${patch}"
            echo "skip_reason="
            echo "trigger_conclusion=${AUTOFIX_TRIGGER_CONCLUSION:-}"
            echo "trigger_class=${AUTOFIX_TRIGGER_CLASS:-}"
            echo "trigger_reason=${AUTOFIX_TRIGGER_REASON:-}"
            echo "trigger_head=${AUTOFIX_TRIGGER_PR_HEAD:-}"
          } >> "$GITHUB_OUTPUT"
        env:
          AUTOFIX_CHANGED: ${{ steps.autofix.outputs.changed }}
          AUTOFIX_REMAINING: ${{ steps.autofix.outputs.remaining_issues }}
          AUTOFIX_NEW: ${{ steps.autofix.outputs.new_issues }}
          SAME_REPO: ${{ needs.context.outputs.same_repo }}
          AUTOFIX_TRIGGER_CONCLUSION: ${{ env.AUTOFIX_TRIGGER_CONCLUSION }}
          AUTOFIX_TRIGGER_CLASS: ${{ env.AUTOFIX_TRIGGER_CLASS }}
          AUTOFIX_TRIGGER_REASON: ${{ env.AUTOFIX_TRIGGER_REASON }}
          AUTOFIX_TRIGGER_PR_HEAD: ${{ env.AUTOFIX_TRIGGER_PR_HEAD }}

  post-comment:
    name: Publish consolidated status
    needs:
      - summarize
      - context
      - small-fixes
      - fix-failing-checks
    if: >
      always() &&
      needs.context.result == 'success' &&
      needs.context.outputs.found == 'true' &&
      needs.context.outputs.failure_tracker_skip != 'true' &&
      needs.context.outputs.failure_incomplete != 'true'
    runs-on: ubuntu-latest
    steps:
      - name: Write summary body
        run: |
          cat <<'EOF' > summary_body.md
          ${{ needs.summarize.outputs.summary_body }}
          EOF

      - name: Locate patch artifact
        id: patch_artifact
        if: >-
          needs.small-fixes.outputs.patch_available == 'true' ||
          needs.fix-failing-checks.outputs.patch_available == 'true'
        uses: actions/github-script@v7
        env:
          RUN_ID: ${{ github.run_id }}
          PR_NUMBER: ${{ needs.context.outputs.pr }}
        with:
          github-token: ${{ secrets.SERVICE_BOT_PAT || github.token }}
          result-encoding: string
          script: |
            const runId = process.env.RUN_ID;
            const prNumber = process.env.PR_NUMBER;
            if (!runId || !prNumber) {
              let missing = [];
              if (!runId) missing.push('run ID');
              if (!prNumber) missing.push('PR number');
              core.info(`Missing ${missing.join(' and ')} for patch lookup.`);
              return '';
            }
            const targetName = `autofix-patch-pr-${prNumber}`;
            const artifacts = await github.paginate(
              github.rest.actions.listWorkflowRunArtifacts,
              {
                owner: context.repo.owner,
                repo: context.repo.repo,
                run_id: Number(runId),
                per_page: 100,
              },
            );
            const match = artifacts.find(artifact => artifact && artifact.name === targetName);
            if (!match) {
              core.info(`No patch artifact found for ${targetName}. This is expected if no patch was generated for this PR/run. If a patch was expected, this may indicate an error or misconfiguration.`);
              return '';
            }
            return `https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${runId}/artifacts/${match.id}`;

      - name: Prepare consolidated comment
        id: prepare
        env:
          SUMMARY_PATH: summary_body.md
          HEAD_SHA: ${{ needs.summarize.outputs.head_sha }}
          PR_NUMBER: ${{ needs.context.outputs.pr }}
          PATCH_URL: ${{ steps.patch_artifact.outputs.result }}
          SMALL_RESULT: ${{ needs.small-fixes.result }}
          SMALL_RAN: ${{ needs.small-fixes.outputs.ran }}
          SMALL_CHANGED: ${{ needs.small-fixes.outputs.changed }}
          SMALL_REMAINING: ${{ needs.small-fixes.outputs.remaining }}
          SMALL_NEW: ${{ needs.small-fixes.outputs.new }}
          SMALL_PATCH: ${{ needs.small-fixes.outputs.patch_available }}
          SMALL_SKIP: ${{ needs.small-fixes.outputs.skip_reason }}
          SMALL_TRIGGER_CONCLUSION: ${{ needs.small-fixes.outputs.trigger_conclusion }}
          SMALL_TRIGGER_CLASS: ${{ needs.small-fixes.outputs.trigger_class }}
          SMALL_TRIGGER_REASON: ${{ needs.small-fixes.outputs.trigger_reason }}
          SMALL_TRIGGER_HEAD: ${{ needs.small-fixes.outputs.trigger_head }}
          FIX_RESULT: ${{ needs.fix-failing-checks.result }}
          FIX_RAN: ${{ needs.fix-failing-checks.outputs.ran }}
          FIX_CHANGED: ${{ needs.fix-failing-checks.outputs.changed }}
          FIX_REMAINING: ${{ needs.fix-failing-checks.outputs.remaining }}
          FIX_NEW: ${{ needs.fix-failing-checks.outputs.new }}
          FIX_PATCH: ${{ needs.fix-failing-checks.outputs.patch_available }}
          FIX_SKIP: ${{ needs.fix-failing-checks.outputs.skip_reason }}
          FIX_TRIGGER_CONCLUSION: ${{ needs.fix-failing-checks.outputs.trigger_conclusion }}
          FIX_TRIGGER_CLASS: ${{ needs.fix-failing-checks.outputs.trigger_class }}
          FIX_TRIGGER_REASON: ${{ needs.fix-failing-checks.outputs.trigger_reason }}
          FIX_TRIGGER_HEAD: ${{ needs.fix-failing-checks.outputs.trigger_head }}
        run: |
          python <<'PY'
          from __future__ import annotations

          import os
          import pathlib
          import re
          from typing import Any, Dict, List

          summary_path = pathlib.Path(os.environ.get("SUMMARY_PATH", "summary_body.md"))
          summary_text = summary_path.read_text(encoding="utf-8") if summary_path.exists() else ""
          summary_text = summary_text.strip()

          footer = "_Updated automatically; will refresh on subsequent CI/Docker completions._"
          prefix = summary_text
          suffix = ""
          if footer and footer in summary_text:
              parts = summary_text.split(footer, 1)
              prefix = parts[0].rstrip()
              suffix = (footer + parts[1]).strip()

          def parse_bool(value: str | None) -> bool:
              if value is None:
                  return False
              return value.strip().lower() == "true"

          def parse_int(value: str | None) -> int:
              if value is None or not value.strip():
                  return 0
              try:
                  return int(float(value.strip()))
              except ValueError:
                  return 0

          def clean_reason(reason: str | None) -> str:
              if not reason:
                  return ""
              value = reason.strip()
              if not value:
                  return ""
              mapping = {
                  "duplicate-patch": "duplicate patch",
                  "not-applicable": "not applicable",
              }
              return mapping.get(value.lower(), re.sub(r"[-_]+", " ", value))

          server = os.environ.get("GITHUB_SERVER_URL", "https://github.com").rstrip("/")
          repository = os.environ.get("GITHUB_REPOSITORY", "").strip("/")
          run_id = os.environ.get("GITHUB_RUN_ID", "").strip()
          artifact_url = ""
          if repository and run_id:
              artifact_url = f"{server}/{repository}/actions/runs/{run_id}?check_suite_focus=true#artifacts"

          pr_number = os.environ.get("PR_NUMBER", "").strip()
          head_sha = os.environ.get("HEAD_SHA", "").strip()
          head_token = head_sha[:12] if head_sha else ""
          anchor_fields: list[str] = []
          if pr_number:
              anchor_fields.append(f"pr={pr_number}")
          if head_token:
              anchor_fields.append(f"head={head_token}")
          if not anchor_fields:
              anchor_fields.append("anchor")
          anchor_marker = f"<!-- maint-46-post-ci: {' '.join(anchor_fields)} -->"
          legacy_marker = "<!-- maint-46-post-ci: DO NOT EDIT -->"

          def build_entry(prefix_key: str, title: str, result: str | None) -> Dict[str, Any]:
              env = os.environ
              ran = parse_bool(env.get(f"{prefix_key}_RAN"))
              changed = parse_bool(env.get(f"{prefix_key}_CHANGED"))
              remaining = parse_int(env.get(f"{prefix_key}_REMAINING"))
              new = parse_int(env.get(f"{prefix_key}_NEW"))
              patch = parse_bool(env.get(f"{prefix_key}_PATCH"))
              skip_raw = env.get(f"{prefix_key}_SKIP")
              trigger_conclusion = (env.get(f"{prefix_key}_TRIGGER_CONCLUSION") or "").strip()
              trigger_class = (env.get(f"{prefix_key}_TRIGGER_CLASS") or "").strip()
              trigger_reason = (env.get(f"{prefix_key}_TRIGGER_REASON") or "").strip()
              trigger_head = (env.get(f"{prefix_key}_TRIGGER_HEAD") or "").strip()
              skip_reason = clean_reason(skip_raw)
              job_result = (result or "").lower()
              if job_result == "skipped" and not skip_reason and not ran:
                  skip_reason = "not applicable"
              status: str
              if ran:
                  status = "✅ completed" if changed else "✅ no changes"
              else:
                  status = "⏭️ skipped"
                  if skip_reason:
                      status = f"{status} ({skip_reason})"
              notes: List[str] = []
              if patch and pr_number:
                  if artifact_url:
                      notes.append(
                          f"Patch artifact: [autofix-patch-pr-{pr_number}]({artifact_url})"
                      )
                  else:
                      notes.append(f"Patch artifact: `autofix-patch-pr-{pr_number}`")
              if trigger_conclusion:
                  trigger_label = trigger_conclusion
                  if trigger_class and trigger_class != trigger_conclusion:
                      trigger_label = f"{trigger_conclusion} ({trigger_class})"
                  if trigger_reason and trigger_reason not in {trigger_conclusion, trigger_class}:
                      trigger_label = f"{trigger_label}; reason={trigger_reason}"
                  notes.append(f"Trigger: {trigger_label}")
              if trigger_head:
                  notes.append(f"Head: `{trigger_head}`")
              if not notes:
                  notes.append("—")
              return {
                  "title": title,
                  "status": status,
                  "changed": "Yes" if changed else "No",
                  "remaining": remaining,
                  "new": new,
                  "notes": "<br/>".join(notes),
                  "ran": ran,
                  "patch": patch,
              }

          entries = [
              build_entry("SMALL", "Small hygiene fixes", os.environ.get("SMALL_RESULT")),
              build_entry("FIX", "Fix failing checks", os.environ.get("FIX_RESULT")),
          ]

          autopatch_present = any(entry["patch"] for entry in entries)

          table_lines: List[str] = [
              "### Autofix Summary",
              "",
              "| Flow | Status | Changed | Remaining | New | Notes |",
              "|------|--------|---------|-----------|-----|-------|",
          ]
          for entry in entries:
              table_lines.append(
                  f"| {entry['title']} | {entry['status']} | {entry['changed']} | {entry['remaining']} | {entry['new']} | {entry['notes']} |"
              )

          if autopatch_present:
              table_lines.extend(
                  [
                      "",
                      "Fork contributors: download the patch artifact above and apply it locally:",
                      "```bash",
                      "git am < autofix.patch",
                      "```",
                  ]
              )

          sections: List[str] = [anchor_marker, legacy_marker]
          if prefix:
              sections.append(prefix.strip())
          sections.append("")
          sections.extend(table_lines)
          if suffix:
              sections.append("")
              sections.append(suffix)

          comment_body = "\n".join(part for part in sections if part is not None)
          pathlib.Path("maint_post_ci_comment.md").write_text(comment_body.strip() + "\n", encoding="utf-8")
          PY

      - name: Upsert consolidated PR comment
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.SERVICE_BOT_PAT || github.token }}
          script: |
            const fs = require('fs');
            const body = fs.readFileSync('maint_post_ci_comment.md', 'utf8');
            const pr = Number('${{ needs.context.outputs.pr }}');
            if (!pr) {
              core.warning('PR number missing; skipping comment update.');
              return;
            }
            const anchorPattern = /<!--\s*maint-46-post-ci:([^>]*)-->/i;
            const extractAnchor = (text) => {
              if (!text) {
                return null;
              }
              const match = text.match(anchorPattern);
              if (!match) {
                return null;
              }
              const content = match[1] || '';
              const prMatch = content.match(/pr=([0-9]+)/i);
              const headMatch = content.match(/head=([0-9a-f]+)/i);
              return {
                raw: match[0],
                pr: prMatch ? prMatch[1] : null,
                head: headMatch ? headMatch[1] : null,
              };
            };
            const targetAnchor = extractAnchor(body);
            const comments = await github.paginate(github.rest.issues.listComments, {
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: pr,
              per_page: 100,
            });
            let existing = null;
            if (targetAnchor) {
              existing = comments.find(comment => {
                const info = extractAnchor(comment.body || '');
                if (!info) {
                  return false;
                }
                if (targetAnchor.pr && info.pr && info.pr !== targetAnchor.pr) {
                  return false;
                }
                if (targetAnchor.head && info.head && info.head !== targetAnchor.head) {
                  return false;
                }
                return true;
              });
            }
            if (!existing) {
              existing = comments.find(comment => (comment.body || '').includes('<!-- maint-46-post-ci:'));
            }
            if (existing) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existing.id,
                body,
              });
              core.info('Updated existing consolidated status comment.');
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: pr,
                body,
              });
              core.info('Created consolidated status comment.');
            }
  failure-tracker:
    name: Update failure tracker
    needs: context
    if: >
      needs.context.result == 'success' &&
      needs.context.outputs.found == 'true' &&
      needs.context.outputs.failure_incomplete != 'true' &&
      needs.context.outputs.failure_tracker_skip != 'true' &&
      github.event.workflow_run.event == 'pull_request'
    runs-on: ubuntu-latest
    env:
      PR_NUMBER: ${{ needs.context.outputs.pr }}
      RATE_LIMIT_MINUTES: 15
      STACK_TOKENS_ENABLED: 'true'
      STACK_TOKEN_MAX_LEN: 160
      AUTO_HEAL_INACTIVITY_HOURS: 24
      FAILURE_INACTIVITY_HEAL_HOURS: 0
      NEW_ISSUE_COOLDOWN_HOURS: 12
      DISABLE_FAILURE_ISSUES: 'false'
      COOLDOWN_RETRY_MS: 3000
      STACK_TOKEN_RAW: 'false'
      COOLDOWN_SCOPE: signature
      OCCURRENCE_ESCALATE_THRESHOLD: 3
      ESCALATE_LABEL: 'priority: high'
      ESCALATE_COMMENT: ''
    steps:
      - name: Derive failure signature & update tracking issue
        if: github.event.workflow_run.conclusion == 'failure'
        id: tracker
        uses: actions/github-script@v7
        with:
          script: |
            const run = context.payload.workflow_run;
            const { owner, repo } = context.repo;
            const runId = run.id;
            const prNumberParsed = parseInt(process.env.PR_NUMBER || '', 10);
            const prNumber = Number.isFinite(prNumberParsed) && prNumberParsed > 0 ? prNumberParsed : null;
            const prTag = prNumber ? `<!-- tracked-pr: ${prNumber} -->` : null;
            const prLine = prNumber ? `Tracked PR: #${prNumber}` : null;

            const slugify = (value) => {
              if (!value) {
                return 'unknown-workflow';
              }
              const slug = String(value)
                .toLowerCase()
                .replace(/[^a-z0-9]+/g, '-')
                .replace(/^-+|-+$/g, '')
                .replace(/--+/g, '-')
                .trim();
              return slug ? slug.slice(0, 80) : 'unknown-workflow';
            };

            const RATE_LIMIT_MINUTES = parseInt(process.env.RATE_LIMIT_MINUTES || '15', 10);
            const STACK_TOKENS_ENABLED = /^true$/i.test(process.env.STACK_TOKENS_ENABLED || 'true');
            const STACK_TOKEN_MAX_LEN = parseInt(process.env.STACK_TOKEN_MAX_LEN || '160', 10);
            const FAILURE_INACTIVITY_HEAL_HOURS = parseFloat(process.env.FAILURE_INACTIVITY_HEAL_HOURS || '0');
            const HEAL_THRESHOLD_DESC = `Auto-heal after ${process.env.AUTO_HEAL_INACTIVITY_HOURS || '24'}h stability (success path)`;

            const jobsResp = await github.rest.actions.listJobsForWorkflowRun({ owner, repo, run_id: runId, per_page: 100 });
            const failedJobs = jobsResp.data.jobs.filter(j => (j.conclusion || '').toLowerCase() !== 'success');
            if (!failedJobs.length) {
              core.info('No failed jobs found despite run-level failure — aborting.');
              return;
            }

            let stackTokenNote = 'Stack tokens disabled';
            if (STACK_TOKENS_ENABLED) {
              const zlib = require('zlib');
              const STACK_TOKEN_RAW = /^true$/i.test(process.env.STACK_TOKEN_RAW || 'false');
              function normalizeToken(raw, maxLen) {
                if (STACK_TOKEN_RAW) return (raw || 'no-stack').slice(0, maxLen);
                if (!raw) return 'no-stack';
                let t = raw;
                const ISO_TIMESTAMP_START_REGEX = /^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(?:\.\d+)?Z\s*/;
                t = t.replace(ISO_TIMESTAMP_START_REGEX, '');
                t = t.replace(/\s+\[[0-9]{1,3}%\]\s*/g, ' ');
                t = t.replace(/\s+/g, ' ').trim();
                const m = t.match(/^[^:]+: [^:]+/);
                if (m) t = m[0];
                if (!t) t = 'no-stack';
                return t.slice(0, maxLen);
              }
              async function extractStackToken(job) {
                try {
                  const logResp = await github.request('GET /repos/{owner}/{repo}/actions/jobs/{job_id}/logs', {
                    owner, repo, job_id: job.id,
                    request: { decompress: false }
                  });
                  const gz = Buffer.from(logResp.data);
                  let text;
                  try { text = zlib.gunzipSync(gz).toString('utf8'); }
                  catch { text = gz.toString('utf8'); }
                  const lines = text.split(/\r?\n/);
                  const idxTraceback = lines.findIndex(l => /Traceback (most recent call last)/i.test(l));
                  if (idxTraceback !== -1) {
                    for (let k = idxTraceback + 1; k < Math.min(lines.length, idxTraceback + 15); k++) {
                      if (/^[A-Za-z_][A-Za-z0-9_\.]*: /.test(lines[k])) {
                        return normalizeToken(lines[k].trim(), STACK_TOKEN_MAX_LEN);
                      }
                    }
                  }
                  const errLine = lines.find(l => /error/i.test(l));
                  if (errLine) return normalizeToken(errLine.trim(), STACK_TOKEN_MAX_LEN);
                } catch (e) {
                  core.info(`Log parse skipped for job ${job.name}: ${e.message}`);
                }
                return normalizeToken('no-stack', STACK_TOKEN_MAX_LEN);
              }
              for (const job of failedJobs) {
                job.__stackToken = await extractStackToken(job);
              }
              stackTokenNote = 'Stack token = first exception or error line (fallback no-stack).';
            } else {
              for (const job of failedJobs) job.__stackToken = 'stacks-off';
            }

            const sigParts = failedJobs.map(j => {
              const failingStep = (j.steps || []).find(s => (s.conclusion || '').toLowerCase() !== 'success');
              return `${j.name}::${failingStep ? failingStep.name : 'no-step'}::${j.__stackToken}`;
            }).sort();
            const crypto = require('crypto');
            const sigHash = crypto.createHash('sha256').update(sigParts.join('|')).digest('hex').slice(0, 12);
            const rawWorkflowName = run.name || run.display_title || 'Workflow';
            const workflowName = rawWorkflowName;
            // Each fallback in this chain yields a defined string, so no nullish values reach the slug source.
            const workflowPath = (run.path || '').trim();
            const workflowFile = (workflowPath.split('/').pop() || '').trim();
            const workflowStem = workflowFile.replace(/\.ya?ml$/i, '') || workflowFile;
            const workflowIdRaw = workflowStem || workflowPath || rawWorkflowName || '';
            const workflowId = slugify(workflowIdRaw);
            const fallbackId = run.workflow_id ? `workflow-${run.workflow_id}` : (run.id ? `run-${run.id}` : 'workflow');
            const workflowKey = workflowId || fallbackId;
            const signature = `${workflowKey}|${sigHash}`;
            const title = `Workflow Failure (${signature})`;
            const labels = ['ci-failure', 'ci', 'devops', 'priority: medium'];
            const COOLDOWN_SCOPE = (process.env.COOLDOWN_SCOPE || 'global').toLowerCase();
            const ESC_THRESHOLD = parseInt(process.env.OCCURRENCE_ESCALATE_THRESHOLD || '0', 10);
            const ESC_LABEL = process.env.ESCALATE_LABEL || 'priority: high';
            const ESC_COMMENT_DEFAULT = `Escalation: occurrences reached threshold (>= ${ESC_THRESHOLD}).`;
            const ESC_COMMENT = (process.env.ESCALATE_COMMENT && process.env.ESCALATE_COMMENT.trim()) || ESC_COMMENT_DEFAULT;

            for (const lb of labels) {
              try { await github.rest.issues.getLabel({ owner, repo, name: lb }); }
              catch { try { await github.rest.issues.createLabel({ owner, repo, name: lb, color: 'BFDADC' }); } catch {} }
            }

            const rows = failedJobs.map(j => `| ${j.name} | ${(j.conclusion || j.status)} | [logs](${j.html_url}) |`);
            const table = ['| Job | Result | Logs |', '|---|---|---|', ...rows].join('\n');
            const runUrl = run.html_url;
            const stackRows = failedJobs.map(j => `| ${j.name} | ${j.__stackToken.replace(/\|/g,'\\|')} |`).join('\n');
            const stackTable = ['| Job | Stack Token |','|---|---|', stackRows].join('\n');

            const bodyParts = [
              `Workflow: **${workflowName}**`,
              `Workflow slug: \`${workflowId}\`,`
              `Run: ${runUrl}`,
            ];
            if (prLine) bodyParts.push(prLine);
            if (prTag) bodyParts.push(prTag);
            bodyParts.push(
              '',
              `**Failure signature:** \`${signature}\` (hash over failed job + first failing step names + first stack line token)`,
              '',
              '### Failed Jobs',
              table,
              '',
              '### Stack Tokens (Phase2)',
              stackTable,
              '',
              `> NOTE: ${stackTokenNote}`
            );
            const bodyBlock = bodyParts.join('\n');

            if (/^true$/i.test(process.env.DISABLE_FAILURE_ISSUES || 'false')) {
              core.info('Failure issue tracking disabled via DISABLE_FAILURE_ISSUES.');
              core.summary.addHeading('Failure (tracking disabled)');
              core.summary.addRaw('Issue creation disabled; diagnostics available in logs.');
              await core.summary.write();
              return;
            }

            const qOpen = `repo:${owner}/${repo} is:issue is:open in:title "${signature}" label:ci-failure`;
            const searchOpen = await github.rest.search.issuesAndPullRequests({ q: qOpen, per_page: 1 });
            let issue_number = null;
            let reopened = false;
            if (searchOpen.data.items.length) {
              issue_number = searchOpen.data.items[0].number;
            } else {
              const qClosed = `repo:${owner}/${repo} is:issue is:closed in:title "${signature}" label:ci-failure`;
              const searchClosed = await github.rest.search.issuesAndPullRequests({ q: qClosed, per_page: 1 });
              if (searchClosed.data.items.length) {
                issue_number = searchClosed.data.items[0].number;
                try {
                  await github.rest.issues.update({ owner, repo, issue_number, state: 'open' });
                  reopened = true;
                  core.info(`Reopened failure issue #${issue_number} for signature ${signature}.`);
                } catch (e) {
                  core.warning(`Failed to reopen prior failure issue #${issue_number}: ${e.message}`);
                  issue_number = null;
                }
              }
            }
            if (issue_number) {
              const existing = await github.rest.issues.get({ owner, repo, issue_number });
              let body = existing.data.body || '';
              const nowIso = new Date().toISOString();
              const occMatch = body.match(/Occurrences:\s*(\d+)/i);
              let occ = occMatch ? parseInt(occMatch[1], 10) : 0;
              occ += 1;
              if (/Occurrences:/i.test(body)) {
                body = body.replace(/Occurrences:\s*\d+/i, `Occurrences: ${occ}`);
              } else {
                body = `Occurrences: ${occ}\n${body}`;
              }
              if (/Last seen:/i.test(body)) {
                body = body.replace(/Last seen:.*/i, `Last seen: ${nowIso}`);
              } else {
                body = `Last seen: ${nowIso}\n${body}`;
              }
              if (!/Healing threshold:/i.test(body)) {
                body = `Healing threshold: ${HEAL_THRESHOLD_DESC}\n${body}`;
              }

              const escapedPr = String(prNumber).replace(/[.*+?^${}()|[\]\\]/g, '\\$&');
              if (prLine && !new RegExp(`Tracked PR:\\s*#${escapedPr}`, 'i').test(body)) {
                if (/Healing threshold:/i.test(body)) {
                  body = body.replace(/Healing threshold:.*?(?:\n|$)/i, (match) => {
                    const trimmed = match.replace(/\s+$/, '');
                    const suffix = match.endsWith('\n') ? '\n' : '';
                    return `${trimmed}\n${prLine}${suffix}`;
                  });
                } else {
                  body = `${prLine}\n${body}`;
                }
              }
              if (prTag && !body.includes(prTag)) {
                body = `${prTag}\n${body}`;
              }

              const HISTORY_START = '<!-- occurrence-history-start -->';
              const HISTORY_END = '<!-- occurrence-history-end -->';
              const MAX_HISTORY = 10;
              const runUrlHist = run.html_url;
              const newRow = `| ${nowIso} | [run](${runUrlHist}) | ${sigHash} | ${failedJobs.length} |`;
              let historySection = `${HISTORY_START}\n| Timestamp | Run | Sig Hash | Failed Jobs |\n|---|---|---|---|\n${newRow}\n${HISTORY_END}`;
              if (body.includes(HISTORY_START) && body.includes(HISTORY_END)) {
                const pre = body.split(HISTORY_START)[0];
                const midPlus = body.split(HISTORY_START)[1];
                const after = midPlus.split(HISTORY_END)[1];
                const existingBlock = midPlus.split(HISTORY_END)[0];
                const lines = existingBlock.split(/\n/).filter(l => l.trim() && !l.startsWith('| Timestamp'));
                const rowsHistory = lines.filter(l => l.startsWith('|')).map(l => l.trim());
                const filtered = [newRow, ...rowsHistory.filter(r => r !== newRow)].slice(0, MAX_HISTORY);
                historySection = `${HISTORY_START}\n| Timestamp | Run | Sig Hash | Failed Jobs |\n|---|---|---|---|\n${filtered.join('\n')}\n${HISTORY_END}`;
                body = pre + historySection + after;
              } else {
                body = historySection + '\n' + body;
              }
              await github.rest.issues.update({ owner, repo, issue_number, title, body });

              if (ESC_THRESHOLD > 0 && occ >= ESC_THRESHOLD) {
                try {
                  const existingLabels = (existing.data.labels || []).map(l => (typeof l === 'string' ? l : l.name));
                  if (!existingLabels.includes(ESC_LABEL)) {
                    try { await github.rest.issues.getLabel({ owner, repo, name: ESC_LABEL }); }
                    catch {
                      try { await github.rest.issues.createLabel({ owner, repo, name: ESC_LABEL, color: 'D93F0B' }); }
                      catch (err) { core.info('Failed to create label: ' + (err && err.message ? err.message : err)); }
                    }
                    await github.rest.issues.addLabels({ owner, repo, issue_number, labels: [ESC_LABEL] });
                    const commentsEsc = await github.rest.issues.listComments({ owner, repo, issue_number, per_page: 50 });
                    const hasEsc = commentsEsc.data.some(c => c.body && c.body.includes(ESC_COMMENT.substring(0, 25)));
                    if (!hasEsc) {
                      await github.rest.issues.createComment({ owner, repo, issue_number, body: ESC_COMMENT });
                    }
                    core.info(`Escalated issue #${issue_number} (occ=${occ}) with label '${ESC_LABEL}'.`);
                  }
                } catch (e) {
                  core.info(`Escalation label step failed: ${e.message}`);
                }
              }

              const comments = await github.rest.issues.listComments({ owner, repo, issue_number, per_page: 50 });
              const alreadyCommented = comments.data.some(c => c.body && c.body.includes(runUrl));
              let postComment = !alreadyCommented;
              if (postComment && comments.data.length) {
                const last = comments.data[comments.data.length - 1];
                const lastTs = Date.parse(last.created_at);
                if (!isNaN(lastTs)) {
                  const minutesAgo = (Date.now() - lastTs) / 60000;
                  if (minutesAgo < RATE_LIMIT_MINUTES) {
                    postComment = false;
                    core.info(`Skipping comment due to rate limit (${minutesAgo.toFixed(1)} < ${RATE_LIMIT_MINUTES}m).`);
                  }
                }
              }
              const commentPayload = reopened ? `Failure reoccurred after auto-heal; issue reopened.\n\n${bodyBlock}` : bodyBlock;
              if (postComment) {
                await github.rest.issues.createComment({ owner, repo, issue_number, body: commentPayload });
                core.info(`Appended failure instance comment to #${issue_number}`);
              }
              core.info(`Updated existing failure issue #${issue_number} (occurrence ${occ}).`);
            } else {
              const cooldownHours = parseFloat(process.env.NEW_ISSUE_COOLDOWN_HOURS || '0');
              let appendedViaCooldown = false;
              const retryMs = parseInt(process.env.COOLDOWN_RETRY_MS || '3000', 10);
              async function attemptCooldownAppend(stage) {
                if (cooldownHours <= 0) return false;
                const cutoff = Date.now() - cooldownHours * 3600_000;
                try {
                  const issuesResp = await github.rest.issues.listForRepo({ owner, repo, labels: 'ci-failure', state: 'open', per_page: 30, sort: 'created', direction: 'desc' });
                  const items = issuesResp.data.filter(it => !it.pull_request && Date.parse(it.created_at) >= cutoff);
                  let recent;
                  if (COOLDOWN_SCOPE === 'workflow') {
                    recent = items.find(it => it.title.startsWith(`Workflow Failure (${workflowName}|`));
                  } else if (COOLDOWN_SCOPE === 'signature') {
                    recent = items.find(it => it.title.includes(`(${signature})`));
                  } else {
                    recent = items[0];
                  }
                  if (recent) {
                    await github.rest.issues.createComment({ owner, repo, issue_number: recent.number, body: `Additional failure during cooldown (${stage}; no new issue).\n\n${bodyBlock}` });
                    core.info(`Cooldown active (${stage}); appended to #${recent.number}`);
                    return true;
                  }
                } catch (e) {
                  core.info(`Cooldown list retrieval failed (${stage}): ${e.message}`);
                }
                return false;
              }
              appendedViaCooldown = await attemptCooldownAppend('initial');
              if (!appendedViaCooldown && cooldownHours > 0 && retryMs > 0) {
                await new Promise(r => setTimeout(r, retryMs));
                appendedViaCooldown = await attemptCooldownAppend('retry');
              }
              if (appendedViaCooldown) return;

              const nowIso = new Date().toISOString();
              const headerMeta = [
                'Occurrences: 1',
                `Last seen: ${nowIso}`,
                `Healing threshold: ${HEAL_THRESHOLD_DESC}`,
                ''
              ].join('\n');
              const created = await github.rest.issues.create({ owner, repo, title, body: headerMeta + bodyBlock, labels });
              core.info(`Created new failure issue #${created.data.number}`);
            }
      - name: Label pull request as ci-failure
        if: >
          github.event.workflow_run.conclusion == 'failure' &&
          needs.context.outputs.pr
        uses: actions/github-script@v7
        env:
          PR_NUMBER: ${{ needs.context.outputs.pr }}
        with:
          script: |
            const pr = Number(process.env.PR_NUMBER || 0);
            if (!pr) {
              core.info('No PR number detected; skipping label.');
              return;
            }
            try {
              await github.rest.issues.addLabels({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: pr,
                labels: ['ci-failure'],
              });
              core.info(`Applied ci-failure label to PR #${pr}.`);
            } catch (error) {
              if (error.status === 422) {
                core.info(`ci-failure label already present on PR #${pr}.`);
              } else {
                throw error;
              }
            }
      - name: Emit failure snapshot artifact
        if: github.event.workflow_run.conclusion == 'failure'
        run: |
          set -euo pipefail
          mkdir -p artifacts
          python - <<'PY'
          import json
          import os
          from pathlib import Path

          payload = {
              "workflow": os.environ.get("WORKFLOW_NAME"),
              "run_id": os.environ.get("RUN_ID"),
              "run_url": os.environ.get("RUN_URL"),
              "pr_number": os.environ.get("PR_NUMBER"),
              "run_conclusion": os.environ.get("RUN_CONCLUSION"),
              "generated_at": os.environ.get("GENERATED_AT"),
          }

          Path("artifacts/ci_failures_snapshot.json").write_text(
              json.dumps(payload, indent=2),
              encoding="utf-8",
          )
          PY
        env:
          WORKFLOW_NAME: ${{ github.event.workflow_run.name }}
          RUN_ID: ${{ github.event.workflow_run.id }}
          RUN_URL: ${{ github.event.workflow_run.html_url }}
          PR_NUMBER: ${{ needs.context.outputs.pr }}
          RUN_CONCLUSION: ${{ github.event.workflow_run.conclusion }}
          GENERATED_AT: ${{ github.event.workflow_run.updated_at }}
      - name: Upload failure snapshot artifact
        if: github.event.workflow_run.conclusion == 'failure'
        uses: actions/upload-artifact@v4
        with:
          name: ci-failures-snapshot
          path: artifacts/ci_failures_snapshot.json
          retention-days: 7
      - name: Emit failure summary
        if: github.event.workflow_run.conclusion == 'failure'
        run: echo "Failure tracking step completed." >> "$GITHUB_STEP_SUMMARY"

      - name: Resolve failure issue for recovered PR
        if: >
          github.event.workflow_run.conclusion == 'success' &&
          needs.context.outputs.pr
        uses: actions/github-script@v7
        env:
          PR_NUMBER: ${{ needs.context.outputs.pr }}
          RUN_URL: ${{ github.event.workflow_run.html_url }}
        with:
          script: |
            const pr = parseInt(process.env.PR_NUMBER || '', 10);
            if (!Number.isFinite(pr) || pr <= 0) {
              core.info('No PR number detected; skipping failure issue resolution.');
              return;
            }
            const { owner, repo } = context.repo;
            const tag = `<!-- tracked-pr: ${pr} -->`;
            const query = `repo:${owner}/${repo} is:issue is:open label:ci-failure "${tag}"`;
            const search = await github.rest.search.issuesAndPullRequests({ q: query, per_page: 10 });
            if (!search.data.items.length) {
              core.info(`No open failure issues tagged for PR #${pr}.`);
              return;
            }
            const runUrl = process.env.RUN_URL || (context.payload.workflow_run && context.payload.workflow_run.html_url) || '';
            const nowIso = new Date().toISOString();
            for (const item of search.data.items) {
              const issue_number = item.number;
              const issue = await github.rest.issues.get({ owner, repo, issue_number });
              let body = issue.data.body || '';
              body = body
                .replace(/^Resolved:.*$/gim, '')
                .replace(/\n{3,}/g, '\n\n')
                .replace(/^\n+/, '')
                .replace(/\s+$/, '');
              body = `Resolved: ${nowIso}\n${body}`.replace(/\n{3,}/g, '\n\n').replace(/\s+$/, '');
              if (body) {
                body = `${body}\n`;
              }

              const commentLines = [
                `Resolution: Gate run succeeded for PR #${pr}.`,
                runUrl ? `Success run: ${runUrl}` : null,
                `Timestamp: ${nowIso}`,
              ].filter(Boolean);
              if (commentLines.length) {
                await github.rest.issues.createComment({
                  owner,
                  repo,
                  issue_number,
                  body: commentLines.join('\n'),
                });
              }
              await github.rest.issues.update({ owner, repo, issue_number, state: 'closed', body });
              core.info(`Closed failure issue #${issue_number} for PR #${pr}.`);
            }

      - name: Auto-heal stale failure issues & note success
        if: github.event.workflow_run.conclusion == 'success'
        uses: actions/github-script@v7
        with:
          script: |
            const { owner, repo } = context.repo;
            const INACTIVITY_HOURS = parseFloat(process.env.AUTO_HEAL_INACTIVITY_HOURS || '24');
            const now = Date.now();
            const q = `repo:${owner}/${repo} is:issue is:open label:ci-failure`;
            const search = await github.rest.search.issuesAndPullRequests({ q, per_page: 100 });
            for (const item of search.data.items) {
              const issue_number = item.number;
              const issue = await github.rest.issues.get({ owner, repo, issue_number });
              const body = issue.data.body || '';
              const m = body.match(/Last seen:\s*(.+)/i);
              if (!m) continue;
              const lastSeenTs = Date.parse(m[1].trim());
              if (isNaN(lastSeenTs)) continue;
              const hours = (now - lastSeenTs) / 3_600_000;
              if (hours >= INACTIVITY_HOURS) {
                const comment = `Auto-heal: no reoccurrence for ${hours.toFixed(1)}h (>= ${INACTIVITY_HOURS}h). Closing.`;
                await github.rest.issues.createComment({ owner, repo, issue_number, body: comment });
                await github.rest.issues.update({ owner, repo, issue_number, state: 'closed' });
                core.info(`Closed healed failure issue #${issue_number}`);
              }
            }
            core.summary.addHeading('Success Run Summary');
            core.summary.addRaw('Checked for stale failure issues and applied auto-heal where applicable.');
            await core.summary.write();
      - name: Remove ci-failure label from pull request
        if: >
          github.event.workflow_run.conclusion == 'success' &&
          needs.context.outputs.pr
        uses: actions/github-script@v7
        env:
          PR_NUMBER: ${{ needs.context.outputs.pr }}
        with:
          script: |
            const pr = Number(process.env.PR_NUMBER || 0);
            if (!pr) {
              core.info('No PR number detected; skipping label removal.');
              return;
            }
            try {
              await github.rest.issues.removeLabel({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: pr,
                name: 'ci-failure',
              });
              core.info(`Removed ci-failure label from PR #${pr}.`);
            } catch (error) {
              if (error.status === 404) {
                core.info(`ci-failure label not present on PR #${pr}.`);
              } else {
                throw error;
              }
            }
      - name: Snapshot open failure issues (JSON)
        if: github.event.workflow_run.conclusion == 'success'
        uses: actions/github-script@v7
        with:
          script: |
            const { owner, repo } = context.repo;
            const q = `repo:${owner}/${repo} is:issue is:open label:ci-failure`;
            const search = await github.rest.search.issuesAndPullRequests({ q, per_page: 100 });
            const issues = [];
            for (const item of search.data.items) {
              const issue = await github.rest.issues.get({ owner, repo, issue_number: item.number });
              const body = issue.data.body || '';
              const occ = (body.match(/Occurrences:\s*(\d+)/i)||[])[1] || null;
              const lastSeen = (body.match(/Last seen:\s*(.*)/i)||[])[1] || null;
              issues.push({
                number: issue.data.number,
                title: issue.data.title,
                occurrences: occ ? parseInt(occ,10) : null,
                last_seen: lastSeen,
                url: issue.data.html_url,
                created_at: issue.data.created_at,
                updated_at: issue.data.updated_at
              });
            }
            const fs = require('fs');
            fs.mkdirSync('artifacts', { recursive: true });
            fs.writeFileSync('artifacts/ci_failures_snapshot.json', JSON.stringify({ generated_at: new Date().toISOString(), issues }, null, 2));
            core.info(`Snapshot written with ${issues.length} open failure issues.`);
      - name: Upload snapshot artifact
        if: github.event.workflow_run.conclusion == 'success'
        uses: actions/upload-artifact@v4
        with:
          name: ci-failures-snapshot
          path: artifacts/ci_failures_snapshot.json
      - name: Emit success summary
        if: github.event.workflow_run.conclusion == 'success'
        run: echo "Run succeeded — failure issues scanned for auto-heal." >> "$GITHUB_STEP_SUMMARY"

