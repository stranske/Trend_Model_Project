 (cd "$(git rev-parse --show-toplevel)" && git apply --3way <<'EOF' 
diff --git a/data_utils.py b/data_utils.py
index 7ddd3f420ad36f4edd862bcfe26ed8749a07a5c1..bf8e44649dfbab1d1aa902827ce23b20d051355e 100644
--- a/data_utils.py
+++ b/data_utils.py
@@ -4,41 +4,41 @@ import pandas as pd
 
 logger = logging.getLogger(__name__)
 
 
 def load_csv(path: str) -> Optional[pd.DataFrame]:
     """Load a CSV expecting a 'Date' column.
 
     Parameters
     ----------
     path : str
         Path to the CSV file.
 
     Returns
     -------
     pandas.DataFrame or None
         The loaded DataFrame if successful, otherwise ``None``.
     """
     try:
         df = pd.read_csv(path, parse_dates=["Date"])
     except FileNotFoundError:
         logger.error(f"File not found: {path}")
         return None
     except pd.errors.EmptyDataError:
         logger.error(f"No data in file: {path}")
         return None
-    except pd.errors.ParserError as e:
-        logger.error(f"Parsing error in {path}: {e}")
+    except pd.errors.ParserError as exc:
+        logger.error(f"Parsing error in {path}: {exc}")
         return None
-    except ValueError as e:
+    except ValueError:
         # Raised when parse_dates references a missing column
         logger.error(f"Missing 'Date' column in {path}")
         return None
 
     if "Date" not in df.columns:
         logger.error(f"Validation failed ({path}): missing 'Date' column")
         return None
 
     if df["Date"].isnull().any():
         logger.warning(f"Null values found in 'Date' column of {path}")
 
     return df
diff --git a/pyproject.toml b/pyproject.toml
new file mode 100644
index 0000000000000000000000000000000000000000..46a9afb1ca45e568b88860da87aa790dd7c933b0
--- /dev/null
+++ b/pyproject.toml
@@ -0,0 +1,9 @@
+[tool.black]
+line-length = 88
+
+[tool.ruff]
+exclude = ["Old/*", "notebooks/*", "*.ipynb"]
+line-length = 88
+
+[tool.mypy]
+ignore_missing_imports = true
diff --git a/run_analysis.py b/run_analysis.py
new file mode 100644
index 0000000000000000000000000000000000000000..19542a6764ee1b2a36eba13a24d0aa5dcbeb4071
--- /dev/null
+++ b/run_analysis.py
@@ -0,0 +1,231 @@
+"""Core analysis functions for the trend model project."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Dict, Optional
+import numpy as np
+import pandas as pd
+
+
+@dataclass
+class Stats:
+    """Container for performance metrics."""
+
+    cagr: float
+    vol: float
+    sharpe: float
+    sortino: float
+    max_drawdown: float
+
+
+def annualize_return(series: pd.Series, periods_per_year: int = 12) -> float:
+    """Annualised compounded return for a series of periodic returns."""
+    if series.empty:
+        return float("nan")
+    compounded = (1 + series).prod() ** (periods_per_year / len(series)) - 1
+    return float(compounded)
+
+
+def annualize_volatility(series: pd.Series, periods_per_year: int = 12) -> float:
+    """Annualised volatility."""
+    if series.empty:
+        return float("nan")
+    return float(series.std(ddof=0) * np.sqrt(periods_per_year))
+
+
+def sharpe_ratio(
+    returns: pd.Series, rf: pd.Series, periods_per_year: int = 12
+) -> float:
+    """Annualised Sharpe ratio."""
+    if returns.empty:
+        return float("nan")
+    excess = returns - rf.reindex_like(returns)
+    if excess.std(ddof=0) == 0:
+        return float("nan")
+    return float(excess.mean() / excess.std(ddof=0) * np.sqrt(periods_per_year))
+
+
+def sortino_ratio(
+    returns: pd.Series, rf: pd.Series, periods_per_year: int = 12
+) -> float:
+    """Annualised Sortino ratio."""
+    if returns.empty:
+        return float("nan")
+    excess = returns - rf.reindex_like(returns)
+    downside = np.minimum(excess, 0)
+    downside_vol = np.sqrt((downside**2).mean()) * np.sqrt(periods_per_year)
+    if downside_vol == 0:
+        return float("nan")
+    return float(excess.mean() / downside_vol)
+
+
+def max_drawdown(returns: pd.Series) -> float:
+    """Maximum drawdown of a return series."""
+    if returns.empty:
+        return float("nan")
+    cumulative = (1 + returns).cumprod()
+    running_max = cumulative.cummax()
+    drawdowns = cumulative / running_max - 1
+    return float(drawdowns.min())
+
+
+def calc_portfolio_returns(weights: np.ndarray, returns_df: pd.DataFrame) -> pd.Series:
+    """Calculate weighted portfolio returns."""
+    return returns_df.mul(weights, axis=1).sum(axis=1)
+
+
+def _compute_stats(df: pd.DataFrame, rf: pd.Series) -> Dict[str, Stats]:
+    stats = {}
+    for col in df:
+        stats[col] = Stats(
+            cagr=annualize_return(df[col]),
+            vol=annualize_volatility(df[col]),
+            sharpe=sharpe_ratio(df[col], rf),
+            sortino=sortino_ratio(df[col], rf),
+            max_drawdown=max_drawdown(df[col]),
+        )
+    return stats
+
+
+def run_analysis(
+    df: pd.DataFrame,
+    in_start: str,
+    in_end: str,
+    out_start: str,
+    out_end: str,
+    target_vol: float,
+    monthly_cost: float,
+    selection_mode: str = "all",
+    random_n: int = 8,
+    custom_weights: Optional[Dict[str, float]] = None,
+    seed: int = 42,
+) -> Optional[Dict[str, object]]:
+    """Run the trend-following analysis on ``df``.
+
+    Parameters
+    ----------
+    df:
+        DataFrame with a ``Date`` column and return columns.
+    in_start, in_end, out_start, out_end:
+        Analysis date range in ``YYYY-MM`` format.
+    target_vol:
+        Target annualised volatility for scaling returns.
+    monthly_cost:
+        Cost deducted from scaled returns each period.
+    selection_mode:
+        ``"all"`` to use every fund or ``"random"`` for a random subset.
+    random_n:
+        Number of funds if ``selection_mode='random'``.
+    custom_weights:
+        Optional mapping of fund â†’ weight percentage for the user portfolio.
+    seed:
+        Random seed when sampling funds.
+
+    Returns
+    -------
+    dict or None
+        Analysis results dictionary, or ``None`` if no funds are selected.
+
+    Example
+    -------
+    >>> df = pd.DataFrame({"Date": pd.date_range("2020-01-31", periods=3, freq="M"),
+    ...                    "RF": 0.0, "A": 0.01, "B": 0.02})
+    >>> run_analysis(df, "2020-01", "2020-02", "2020-03", "2020-03", 0.1, 0.0)
+    {'selected_funds': ['A', 'B'], ...}
+    """
+    if df is None:
+        return None
+
+    date_col = "Date"
+    if date_col not in df.columns:
+        raise ValueError("DataFrame must contain a 'Date' column")
+
+    df = df.copy()
+    if not np.issubdtype(df[date_col].dtype, np.datetime64):
+        df[date_col] = pd.to_datetime(df[date_col])
+    df.sort_values(date_col, inplace=True)
+
+    def _parse_month(s: str) -> pd.Timestamp:
+        return pd.to_datetime(f"{s}-01") + pd.offsets.MonthEnd(0)
+
+    in_sdate, in_edate = _parse_month(in_start), _parse_month(in_end)
+    out_sdate, out_edate = _parse_month(out_start), _parse_month(out_end)
+
+    in_df = df[(df[date_col] >= in_sdate) & (df[date_col] <= in_edate)]
+    out_df = df[(df[date_col] >= out_sdate) & (df[date_col] <= out_edate)]
+
+    if in_df.empty or out_df.empty:
+        return None
+
+    ret_cols = [c for c in df.columns if c != date_col]
+    rf_col = min(ret_cols, key=lambda c: df[c].std())
+    fund_cols = [c for c in ret_cols if c != rf_col]
+
+    if selection_mode == "random" and len(fund_cols) > random_n:
+        rng = np.random.default_rng(seed)
+        fund_cols = rng.choice(fund_cols, size=random_n, replace=False).tolist()
+
+    if not fund_cols:
+        return None
+
+    vols = in_df[fund_cols].std() * np.sqrt(12)
+    scale_factors = (
+        pd.Series(target_vol / vols, index=fund_cols)
+        .replace([np.inf, -np.inf], 1.0)
+        .fillna(1.0)
+    )
+
+    in_scaled = in_df[fund_cols].mul(scale_factors, axis=1) - monthly_cost
+    out_scaled = out_df[fund_cols].mul(scale_factors, axis=1) - monthly_cost
+    in_scaled = in_scaled.clip(lower=-1.0)
+    out_scaled = out_scaled.clip(lower=-1.0)
+
+    rf_in = in_df[rf_col]
+    rf_out = out_df[rf_col]
+
+    in_stats = _compute_stats(in_scaled, rf_in)
+    out_stats = _compute_stats(out_scaled, rf_out)
+    out_stats_raw = _compute_stats(out_df[fund_cols], rf_out)
+
+    ew_weights = np.repeat(1.0 / len(fund_cols), len(fund_cols))
+    ew_w_dict = {c: w for c, w in zip(fund_cols, ew_weights)}
+    in_ew = calc_portfolio_returns(ew_weights, in_scaled)
+    out_ew = calc_portfolio_returns(ew_weights, out_scaled)
+    out_ew_raw = calc_portfolio_returns(ew_weights, out_df[fund_cols])
+
+    in_ew_stats = _compute_stats(pd.DataFrame({"ew": in_ew}), rf_in)["ew"]
+    out_ew_stats = _compute_stats(pd.DataFrame({"ew": out_ew}), rf_out)["ew"]
+    out_ew_stats_raw = _compute_stats(pd.DataFrame({"ew": out_ew_raw}), rf_out)["ew"]
+
+    if custom_weights is None:
+        custom_weights = {c: 100 / len(fund_cols) for c in fund_cols}
+    user_w = np.array([custom_weights.get(c, 0) / 100 for c in fund_cols])
+    user_w_dict = {c: w for c, w in zip(fund_cols, user_w)}
+
+    in_user = calc_portfolio_returns(user_w, in_scaled)
+    out_user = calc_portfolio_returns(user_w, out_scaled)
+    out_user_raw = calc_portfolio_returns(user_w, out_df[fund_cols])
+
+    in_user_stats = _compute_stats(pd.DataFrame({"user": in_user}), rf_in)["user"]
+    out_user_stats = _compute_stats(pd.DataFrame({"user": out_user}), rf_out)["user"]
+    out_user_stats_raw = _compute_stats(pd.DataFrame({"user": out_user_raw}), rf_out)[
+        "user"
+    ]
+
+    return {
+        "selected_funds": fund_cols,
+        "in_sample_scaled": in_scaled,
+        "out_sample_scaled": out_scaled,
+        "in_sample_stats": in_stats,
+        "out_sample_stats": out_stats,
+        "out_sample_stats_raw": out_stats_raw,
+        "in_ew_stats": in_ew_stats,
+        "out_ew_stats": out_ew_stats,
+        "out_ew_stats_raw": out_ew_stats_raw,
+        "in_user_stats": in_user_stats,
+        "out_user_stats": out_user_stats,
+        "out_user_stats_raw": out_user_stats_raw,
+        "ew_weights": ew_w_dict,
+        "fund_weights": user_w_dict,
+    }
diff --git a/tests/test_analysis.py b/tests/test_analysis.py
deleted file mode 100644
index 22a692738008d5da439885129b24fcd6abbcb685..0000000000000000000000000000000000000000
--- a/tests/test_analysis.py
+++ /dev/null
@@ -1,17 +0,0 @@
-import importlib.util
-import pathlib
-import pytest
-
-# Dynamically load the cleanup module from its file path
-module_path = pathlib.Path(__file__).resolve().parents[1] / 'Old' / 'Vol_Adj_Trend_Analysis_Cleanup.py'
-spec = importlib.util.spec_from_file_location('cleanup', module_path)
-cleanup = importlib.util.module_from_spec(spec)
-spec.loader.exec_module(cleanup)
-
-
-def test_run_analysis_returns_none():
-    assert cleanup.run_analysis(None, None, None, None, None, None, None) is None
-
-
-def test_prepare_weights_missing():
-    assert not hasattr(cleanup, 'prepare_weights')
diff --git a/tests/test_run_analysis.py b/tests/test_run_analysis.py
new file mode 100644
index 0000000000000000000000000000000000000000..78da503b2c75fe317691ea4825e957706c5e6e52
--- /dev/null
+++ b/tests/test_run_analysis.py
@@ -0,0 +1,65 @@
+import pandas as pd
+import numpy as np
+from run_analysis import (
+    run_analysis,
+    Stats,
+    annualize_return,
+    annualize_volatility,
+    sharpe_ratio,
+    sortino_ratio,
+    max_drawdown,
+    calc_portfolio_returns,
+)
+
+
+def make_df():
+    dates = pd.date_range("2020-01-31", periods=6, freq="M")
+    data = {
+        "Date": dates,
+        "RF": 0.0,
+        "A": [0.02, 0.03, -0.01, 0.04, 0.02, 0.01],
+        "B": [0.01, 0.02, -0.02, 0.03, 0.02, 0.0],
+    }
+    return pd.DataFrame(data)
+
+
+def test_metrics_roundtrip():
+    df = make_df()
+    series = df["A"]
+    rf = df["RF"]
+    r = annualize_return(series)
+    v = annualize_volatility(series)
+    s = sharpe_ratio(series, rf)
+    so = sortino_ratio(series, rf)
+    mdd = max_drawdown(series)
+    port = calc_portfolio_returns(np.array([0.5, 0.5]), df[["A", "B"]])
+    assert isinstance(r, float) and isinstance(v, float)
+    assert isinstance(s, float) and isinstance(so, float)
+    assert isinstance(mdd, float)
+    assert port.shape[0] == series.shape[0]
+
+
+def test_run_analysis_basic():
+    df = make_df()
+    res = run_analysis(df, "2020-01", "2020-03", "2020-04", "2020-06", 0.1, 0.0)
+    assert res is not None
+    assert set(res["selected_funds"]) == {"A", "B"}
+    assert "in_sample_stats" in res
+    assert isinstance(res["in_ew_stats"], Stats)
+
+
+def test_run_analysis_random_selection():
+    df = make_df()
+    res = run_analysis(
+        df,
+        "2020-01",
+        "2020-03",
+        "2020-04",
+        "2020-06",
+        0.1,
+        0.0,
+        selection_mode="random",
+        random_n=1,
+        seed=1,
+    )
+    assert len(res["selected_funds"]) == 1
 
EOF
)