# User Guide

This document introduces the main features of the Trend Model Project and explains how to use them. It assumes no prior knowledge of the system.

## 1. Setup

1. Install the dependencies into a virtual environment:
   ```bash
   ./scripts/setup_env.sh
   ```
   This creates `.venv/` and installs packages from `requirements.txt`.
2. Optional: run the test suite to verify everything works:
   ```bash
   ./scripts/run_tests.sh
   ```

## 2. Running the analysis from the command line

Invoke the pipeline with an optional YAML configuration file:
```bash
python -m trend_analysis.run_analysis -c path/to/config.yml
```
If `-c` is omitted, the tool loads `config/defaults.yml` or the file specified by the `TREND_CFG` environment variable.

The metrics output is printed to the console and can also be written to Excel, CSV or JSON depending on `output.format` in the config.

## 3. Interactive GUI

A graphical interface is available in Jupyter. Launch it by running:
```python
from trend_analysis.gui import launch
launch()
```

The GUI allows you to edit the configuration, select the portfolio mode, tweak ranking options and choose a weighting method. A dark‑mode toggle controls the theme. All settings persist to `~/.trend_gui_state.yml` so the next session picks up where you left off.

After editing the parameters click **Run** to execute the pipeline. Results are exported according to the chosen format. Weighting plug‑ins discovered via entry points appear automatically in the interface.

## 4. Selection modes and ranking

`portfolio.selection_mode` supports `all`, `random`, `manual` and `rank` values. In rank mode you can keep the top funds by score or apply a threshold. Scores come from metrics defined under `metrics.registry` and may be combined with z‑scored weights.

Manual mode displays a table of candidates so you can override fund inclusion and weights directly.

## 5. Weighting options

Several weighting strategies are built in:

- **EqualWeight** – simple 1/N allocation.
- **ScorePropSimple** – weights proportional to positive scores.
- **ScorePropBayesian** – shrinks scores toward the mean before weighting.
- **AdaptiveBayesWeighting** – updates weights over multiple periods and persists its state between runs.

When the GUI is used, the state of AdaptiveBayesWeighting is saved alongside the main config so that repeated runs refine the posterior.

## 6. Output formats

Set `output.format` to `excel`, `csv` or `json`. Excel exports include a formatted summary sheet generated by `trend_analysis.export.make_summary_formatter` and can be inspected with any spreadsheet program.

## 7. Benchmarks and information ratio

Add a `benchmarks` mapping in the config to compute information ratios against one or more indices. The results appear as `ir_<label>` columns in the metrics output and on the summary sheet.

## 8. Inspecting the score frame

`run_analysis()` returns a dictionary containing a `score_frame` DataFrame with one column per metric and attributes describing the analysed period. Advanced users can examine this table to build custom selection logic.

## 9. Multi-period demo

Generate the synthetic dataset and run the helper script to exercise the Phase 2 engine:

```bash
python scripts/generate_demo.py
python scripts/run_multi_demo.py
```

The script ensures multiple periods are processed and that adaptive weights evolve over time.

## 10. Further help

See `README.md` for a short overview of the repository structure and the example notebooks for end‑to‑end demonstrations.

