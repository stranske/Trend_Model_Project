# User Guide

This document introduces the main features of the Trend Model Project and explains how to use them. It assumes no prior knowledge of the system.

## 1. Setup

1. Install the dependencies into a virtual environment:
   ```bash
   ./scripts/setup_env.sh
   ```
   This creates `.venv/` and installs packages from `requirements.txt`.
2. Optional: run the test suite to verify everything works:
   ```bash
   ./scripts/run_tests.sh
   ```

## 2. Running the analysis from the command line

Invoke the pipeline with an optional YAML configuration file:
```bash
python -m trend_analysis.run_analysis -c path/to/config.yml
```
If `-c` is omitted, the tool loads `config/defaults.yml` or the file specified by the `TREND_CFG` environment variable.

The metrics output is printed to the console and can also be written to Excel, CSV or JSON depending on `output.format` in the config.

## 3. Interactive GUI

A graphical interface is available in Jupyter. Launch it by running:
```python
from trend_analysis.gui import launch
launch()
```

The GUI allows you to edit the configuration, select the portfolio mode, tweak ranking options and choose a weighting method. A dark‑mode toggle controls the theme. All settings persist to `~/.trend_gui_state.yml` so the next session picks up where you left off.

After editing the parameters click **Run** to execute the pipeline. Results are exported according to the chosen format. Weighting plug‑ins discovered via entry points appear automatically in the interface.

## 4. Configuration presets

Ready-made presets live in `config/presets/` and provide sensible defaults for common risk profiles.
Select a preset on the Streamlit **Configure** page or load it from the command line:

```bash
python -m trend_analysis.run_analysis -c config/presets/balanced.yml
```

Replace `balanced` with `conservative` or `aggressive` as needed. See
[PresetStrategies.md](PresetStrategies.md) for a summary of each option.

## 5. Selection modes and ranking

`portfolio.selection_mode` supports `all`, `random`, `manual` and `rank` values. In rank mode you can keep the top funds by score or apply a threshold. Scores come from metrics defined under `metrics.registry` and may be combined with z‑scored weights.

Manual mode displays a table of candidates so you can override fund inclusion and weights directly.

## 6. Weighting options

Several weighting strategies are built in:

- **EqualWeight** – simple 1/N allocation.
- **ScorePropSimple** – weights proportional to positive scores.
- **ScorePropBayesian** – shrinks scores toward the mean before weighting.
- **AdaptiveBayesWeighting** – updates weights over multiple periods and persists its state between runs.

When the GUI is used, the state of AdaptiveBayesWeighting is saved alongside the main config so that repeated runs refine the posterior.

## 7. Output formats

Set `output.format` to `excel`, `csv` or `json`. Excel exports include a formatted summary sheet generated by `trend_analysis.export.make_summary_formatter` and can be inspected with any spreadsheet program.

## 8. Benchmarks and information ratio

Add a `benchmarks` mapping in the config to compute information ratios against one or more indices. The results appear as `ir_<label>` columns in the metrics output and on the summary sheet.

## 9. Inspecting the score frame

`run_analysis()` returns a dictionary containing a `score_frame` DataFrame with one column per metric and attributes describing the analysed period. Advanced users can examine this table to build custom selection logic.

## 10. Multi-period demo

Generate the synthetic dataset and run the helper script to exercise the Phase 2 engine. The demo now starts by bootstrapping a clean environment and then cycles through several selector and weighting strategies while exporting results in multiple formats. `config/demo.yml` enables the rank-based selector and registers an `SPX` benchmark so information ratios are computed:

```bash
./scripts/setup_env.sh
python scripts/generate_demo.py [--no-xlsx]
python scripts/run_multi_demo.py
```
Use `--no-xlsx` to skip generating the Excel workbook if binary files should be
avoided.

`run_multi_demo.py` calls ``export.export_data`` so CSV, Excel, JSON **and TXT** reports are produced in one go. It also runs the full test suite, ensuring multiple periods are processed and that adaptive weights evolve over time.
The script exercises the CLI wrappers as part of these checks.

## 11. Demo pipeline (maintenance / CI)

Whenever the exporter or pipeline behaviour changes, re-run the demo steps below
and adjust `config/demo.yml` or `scripts/run_multi_demo.py` so the demo
exercises every new code path.

1. **Bootstrap the environment**
   ```bash
   ./scripts/setup_env.sh
   ```
2. **Generate the demo dataset**
   ```bash
   python scripts/generate_demo.py
   ```
3. **Run the full demo pipeline and export checks**
   ```bash
   python scripts/run_multi_demo.py
   ```
4. **Run the test suite**
   ```bash
   ./scripts/run_tests.sh
   ```

## 12. Reproducibility and Testing

For reproducible results across different runs and environments, see [ReproducibilityGuide.md](ReproducibilityGuide.md). This is especially important when using random selection modes or when running tests.

## 12.1 Walk-forward (rolling OOS) analysis

The Streamlit Results page now includes a Walk-forward analysis expander to aggregate metrics over rolling out-of-sample windows and, optionally, by regime. See [Walkforward.md](Walkforward.md) for a quick guide and a CLI example.

## 13. Further help

See `README.md` for a short overview of the repository structure and the example notebooks for end‑to‑end demonstrations.

