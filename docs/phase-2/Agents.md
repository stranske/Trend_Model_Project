# agents.md
See [PhaseÂ 1 docs](../phase-1/Agents.md) for earlier context.
"""
YOU ARE CODEX.  EXTEND THE VOL_ADJ_TREND_ANALYSIS PROJECT AS FOLLOWS
--------------------------------------------------------------------

Highâ€‘level goal
~~~~~~~~~~~~~~~
Add a **performanceâ€‘based managerâ€‘selection mode** that works alongside the
existing 'all', 'random', and 'manual' modes.  Make the pipeline fully
configâ€‘driven and keep everything vectorised.

Functional spec
~~~~~~~~~~~~~~~
1.  New selection mode keyword:  `rank`.
    â€¢ Works on the *inâ€‘sample* window **after** the usual dataâ€‘quality filters.  
    â€¢ Supported inclusion approaches:
         - `'top_n'`       â€“ keep the N best funds.
         - `'top_pct'`     â€“ keep the top Pâ€¯percent.  
         - `'threshold'`   â€“ keep funds whose score â‰¥ user threshold
           (this is the â€œuseful extraâ€ beyond N and percentile).

2.  Rank criteria (`score_by`):
    â€¢ Any single metric registered in `METRIC_REGISTRY`
      (e.g. 'Sharpe', 'AnnualReturn', 'MaxDrawdown', â€¦).  
    â€¢ Special value `'blended'` that combines up to three metrics with
      userâ€‘supplied *positive* weights (weights will be normalised to 1.0).

3.  Directionâ€‘ofâ€‘merit:
    â€¢ Metrics where â€œlarger is betterâ€  â†’ rank descending.
    â€¢ Metrics where â€œsmaller is betterâ€ (currently **only** MaxDrawdown)  
      â†’ rank ascending.  Future metrics can extend `ASCENDING_METRICS`.

4.  Config file (YAML) drives everything â€“ sample below.

5.  UI flow (ipywidgets, no external deps):
<!-- STEP 0 START -->
### Stepâ€¯0â€¯â€“â€¯Config Loader & Editor Â ğŸ“

| Purpose | Controls | Behaviour |
|---------|----------|-----------|
| **Load existing config** | `FileUpload(accept=".yml")` | Parse YAML â†’ populate `ParamStore` â†’ refresh downstream widgets. |
| **Template picker** | `Dropdown(options=list_builtin_cfgs())` | Selecting a template triggers the same refresh. |
| **Grid editor** | If **ipydatagrid** present: render editable grid of the current YAML.  Else show a disabled grid stub plus a warning banner. | Edits propagate to `ParamStore` in real time via the `on_cell_change` event; invalid edits revert and flash red. |
| **Save/Download** | â€œğŸ’¾â€¯Save configâ€ button â†’ writes YAML to disk; â€œâ¬‡ï¸â€¯Downloadâ€ â†’ triggers browser download. | Uses `yaml.safe_dump(param_store.to_dict())`. |

> *Rationale*: power users often arrive with a ready config; making this the very first step shortâ€‘circuits half the clicks.

<!-- STEP 0 END -->
    Step 1  â€“ Mode (â€˜allâ€™, â€˜randomâ€™, â€˜manualâ€™, **â€˜rankâ€™**),
               checkboxes for â€œvolâ€‘adjâ€ and â€œuse rankingâ€.  
    Step 2  â€“ If mode == 'rank' **or** user ticked â€œuse rankingâ€
               â†’ reveal controls for `inclusion_approach`,
               `score_by`, `N / Pct / Threshold`, and (if blended)
               three sliders for weights + metric pickers.  
    Step 3  â€“ If mode == 'manual'  
               â†’ display an interactive DataFrame of the IS scores so the
               user can override selection and set weights.
    Step 4  â€“ Output format picker (csv / xlsx / json) then fire
               `run_analysis()` and `export_to_*`.

6.  No broken changes:
    â€¢ Default behaviour (config absent) must be identical to current build.
    â€¢ All heavy computation stays in NumPy / pandas vector land.

7.  Unitâ€‘test hooks:
    â€¢ New pure functions must be importâ€‘safe and testable without widgets.
      (e.g. `rank_select_funds()`).
8.  Selector cache reuse:
    â€¢ Window bundles are keyed by `(start, end, universe_hash, stats_cfg_hash)` via
      `make_window_key()` and surfaced through `get_window_metric_bundle()`.
    â€¢ Instrument `selector_cache_stats()["selector_cache_hits"]` to verify that
      repeated selector runs on identical windows reuse the cached metrics and
      covariance payloads.

Sample YAML
~~~~~~~~~~~
selection:
  mode: rank               # all | random | manual | rank
  random_n: 12             # only if mode == random
  use_vol_adjust: true
rank:
  inclusion_approach: top_n     # top_n | top_pct | threshold
  n: 8                          # for top_n
  pct: 0.10                     # for top_pct  (decimal, not %)
  threshold: 1.5                # ignored unless approach == threshold
  score_by: blended             # Sharpe | AnnualReturn | â€¦ | blended
  blended_weights:
    Sharpe: 0.5
    AnnualReturn: 0.3
    MaxDrawdown: 0.2
output:
  format: excel                 # csv | excel | json
<!-- â€¦ existing Stepsâ€¯1â€‘10 remain unchanged â€¦ -->

<!-- locate STEPâ€¯11 and replace its body with the following â€¦ -->

### Stepâ€¯11â€¯â€“â€¯GUI implementation (ipywidgetsÂ Â±â€¯ipydatagrid) Â ğŸš€

> **Scope additions since v2**: Configâ€‘first flow, true grid editing, `ParamStore`, debounce wrapper, state persistence, darkâ€‘mode toggle, plugâ€‘in registry.

| GUI Step | Mandatory Controls | Behaviour | Pureâ€‘function hooks |
|----------|-------------------|-----------|---------------------|
| **0â€¯â€“â€¯Config I/O** | See **Stepâ€¯0** table above. | Valid edits update `ParamStore`; invalid edits rollback with toast. | `build_config_dict()` |
| **1â€¯â€“â€¯Mode & global flags** | unchanged | unchanged | â€“ |
| **2â€¯â€“â€¯Ranking options** | unchanged | **New**: metric/weight sliders wrapped in 300â€¯ms debounce decorator. | `rank_select_funds()` |
| **3â€¯â€“â€¯Manual override** | **Primary**: `ipydatagrid.DataGrid` (editable include/weight columns).  <br>**Fallback**: previous SelectMultiple layout + warning. | Grid emits `cell_edited`; keeps weights numeric & â‰¥0. | â€“ |
| **4â€¯â€“â€¯Output & run** | + â€œğŸŒ—Â Theme:â€ `ToggleButtons(["system","light","dark"])` | Darkâ€‘mode switch toggles a CSS variable on the root DOM node. | â€“ |
| **Status / logs** | unchanged | unchanged | â€“ |

**ParamStore dataclass**

```python
@dataclass
class ParamStore:
    """Mutable GUI state shared across view layers."""
    cfg: dict[str, Any] = field(default_factory=dict)
    theme: str = "system"          # light | dark
    dirty: bool = False            # unsaved edits flag
    weight_state: dict[str, Any] | None = None  # AdaptiveBayes posterior

    def to_dict(self) -> dict[str, Any]:
        return self.cfg

    @classmethod
    def from_yaml(cls, path: Path) -> "ParamStore":
        return cls(cfg=yaml.safe_load(path.read_text()))

All widget callbacks accept (change, *, store: ParamStore) and mutate store
only; pipeline ingest path is run(build_config_from_store(store)).
Debounce helper
def debounce(wait_ms: int = 300):
    def decorator(fn):
        last_call = 0
        async def wrapper(*args, **kwargs):
            nonlocal last_call
            last_call = time.time()
            await asyncio.sleep(wait_ms / 1000)
            if time.time() - last_call >= wait_ms / 1000:
                return fn(*args, **kwargs)
        return wrapper
    return decorator

State persistence

On successful Run, call
yaml.safe_dump(store.to_dict(), Path.home()/".trend_gui_state.yml").

If ``store.weight_state`` is not ``None`` also pickle it to
``~/.trend_gui_state.pkl`` as ``{"adaptive_bayes_posteriors": store.weight_state}``.

On GUI launch, attempt to load the file; if malformed, ignore with a warning.

Plugâ€‘in registry
for ep in importlib.metadata.entry_points(group="trend_analysis.gui_plugins"):
    plugin_cls = ep.load()
    register_plugin(plugin_cls)       # adds controls dynamically

Tests must assert that enumerating plugâ€‘ins requires no widget edits.

<!-- STEPÂ 11Â END -->





ğŸ”„ 2025â€‘06â€‘15 UPDATE â€” PHASEâ€‘1 ENHANCEMENTS
------------------------------------------
â€¢ Blended ranking **must** use *zâ€‘scores* (meanâ€‘0, stdevâ€‘1) before the
  weighted sum so metrics on different scales are commensurable.
â€¢ MaxDrawdown is currently the only â€œsmallerâ€‘isâ€‘betterâ€ metric; the
  ASCENDING_METRICS set remains {"MaxDrawdown"} until further notice.
â€¢ Config format stays YAML.
"""# agents.md
## Mission
Converge the scattered modules into one fullyâ€‘testâ€‘covered, vectorised pipeline that can be invoked from a single CLI entryâ€‘point.
Never touch notebooks living under any directory whose name ends in old/.

---

### 2025-06-27 UPDATE â€” RISK-METRICS EXPORT (SERIOUSLY, LEAVE THIS IN)

Codex removed the pretty reporting layer once; it shall not happen again.  
Follow these guard-rails whenever you touch export logic.

1. **Call the canonical exporters**  
   After `pipeline.run()` completes, pipe the returned `Mapping[str, pd.DataFrame]`
   into **exactly one** of  
   `trend_analysis.export.export_to_excel | export_to_csv | export_to_json`.

2. **Sheet / file keys**  

3. **Excel format contract**  
* Generate the summary sheet formatter via
  `trend_analysis.export.make_summary_formatter(...)`.
* Register any other sheet formatter with `@register_formatter_excel`
  so `export_to_excel` auto-hooks it.
* Required cosmetics:  
  - bold title row,  
  - `0.00%` for CAGR & Vol,  
  - `0.00` for Sharpe & Sortino,  
  - red numerals for MaxDD,  
  - freeze panes on header,  
  - auto-filter,  
  - column width = `max(len(header)) + 2`.

4. **Column order = law**  
Tests must fail if this order mutates.

5. **Config switches**  
`output.format` = `excel | csv | json`  
`output.path`   = prefix used by exporter (Excel auto-appends `.xlsx`).

6. **Tests**  
* In-memory smoke test: write to `BytesIO`, assert two sheets and cell
  `A1 == "Vol-Adj Trend Analysis"`.
* Regression test: `assert list(df.columns) == EXPECTED_COLUMNS`.

7. **Back-compat**  
Silent config = drop the fully formatted Excel workbook into `outputs/`
exactly as v1.0 did. Breaking that throws `ExportError`.

> ğŸ›¡ï¸  If you rip out these formatters again, CI will chaperone you with a failing gate and a stern commit message.


## | Layer / concern                      | **Canonical location**                                                     | Everything else is **deprecated**                         |
| ------------------------------------ | -------------------------------------------------------------------------- | --------------------------------------------------------- |
| **Data ingest & cleaning**           | `trend_analysis/data.py` <br>â€¯(alias exported as `trend_analysis.data`)    | `data_utils.py`, helper code in notebooks or `scripts/`   |
| **Portfolio logicâ€¯& metrics**        | `trend_analysis/metrics.py` (vectorised)                                   | loops inside `run_analysis.py`, adâ€‘hoc calcs in notebooks |
| **Export / I/O**                     | `trend_analysis/export.py`                                                 | the rootâ€‘level `exports.py`, snippets inside notebooks    |
| **Domain kernels (fast primitives)** | `trend_analysis/core/` package                                             | standâ€‘alone modules under the topâ€‘level `core/` directory |
| **Pipeline orchestration**           | `trend_analysis/pipeline.py` (pure)                                        | any duplicated control flow elsewhere                     |
| **CLI entryâ€‘point**                  | `run_analysis.py` **only** (thin wrapper around `trend_analysis.cli:main`) | bespoke `scripts/*.py` entry points                       |
| **Config**                           | `config/defaults.yml` loaded through `trend_analysis.config.load()`        | hardâ€‘coded constants, magic numbers in notebooks          |
| **Tests**                            | `tests/` (pytest; 100â€¯% branchâ€‘aware coverage gate)                        |    â€”                                                      |
One concern â†’ one module.
Replacements must delete or commentâ€‘out whatever they obsolete in the same PR.

Immediate Refactor Tasks
Flatten duplications

Rename data_utils.py â†’ trend_analysis/data.py, adjust imports, delete the original.

Migrate the contents of the topâ€‘level exports.py into trend_analysis/export.py; keep only a reâ€‘export stub for one minor release.

Turn the stray core/ directory into an importable subâ€‘package:
core/indicator.py â†’ trend_analysis/core/indicator.py, etc.

Single pipeline

Implement trend_analysis/pipeline.py exposing a pure function
run(config: Config) -> pd.DataFrame.

run_analysis.py should parse CLI args, build a Config, pass it to pipeline.run, then handle pretty printing / file output only.

Config resolution

# trend_analysis/config.py
from pydantic import BaseModel
class Config(BaseModel):
    defaults: str = Path(__file__).with_name("..").joinpath("config/defaults.yml")
    # ...other validated fields...
def load(path: str | None = None) -> Config: ...

Envâ€‘var override: TREND_CFG=/path/to/override.yml run_analysis ...

Dependency hygiene

Heavy imports (numpy, pandas, scipy) at top of each module are fine.

Keep formatter/test tool versions in lock-step by running `python -m scripts.sync_tool_versions --check` (use `--apply` if drift is detected) before committing config or workflow changes.

No circular imports. pipeline.py orchestrates; nothing imports it.

Tests

NOTE: Test fixtures must be text-serialised (CSV/JSON); no binary formats in PRs.

Require 100â€¯% branch coverage on trend_analysis/* via pytestâ€‘cov in CI.

Conventions & Guardâ€‘rails
Vectorise first.
Falling back to forâ€‘loops requires a comment justifying why vectorisation is impossible or harmful.

Public API (exported in __all__) uses USâ€‘English snakeâ€‘case; private helpers are prefixed with _.

Notebook hygiene: any new exploratory notebook must start with the header
# ğŸ”¬ scratchpad â€“ may be deleted at any time.

CI (GitHub Actions) stages to add:

lint  (ruff + black â€“â€‘check)

typeâ€‘check (mypy, strict)

test (pytest â€‘â€‘cov trend_analysis â€‘â€‘covâ€‘branch)

buildâ€‘wheel (tags only)

##NEW

### âœ¨ Task: Integrate `information_ratio` endâ€‘toâ€‘end  (#metricsâ€‘IR)

**Motivation**  
Phaseâ€‘1 now includes a vectorised `information_ratio` metric.  
It is fully unitâ€‘tested but not yet surfaced in the CLI / Excel export or
multiâ€‘benchmark workflows.

---

#### 1.  Pipeline / Statistics

* [x] Extend `_Stats` dataclass with `information_ratio: float`.
* [x] In `_compute_stats()` compute `information_ratio(df[col], rf_series)`.
* [x] Ensure `out_stats_df` includes the new field.

#### 2.  Multiâ€‘benchmark support

* [x] Accept `benchmarks:` mapping in YAML cfg, e.g.

```yaml
benchmarks:
  spx: SPX
  tsx: TSX

### 2025â€‘07â€‘03 UPDATEÂ â€” STEPâ€¯4: surface a real `score_frame`

* **Add** `single_period_run()` to **`trend_analysis/pipeline.py`**  
  * **Signature**  
    ```python
    def single_period_run(
        df: pd.DataFrame,
        start: str,
        end: str,
        *,
        stats_cfg: "RiskStatsConfig" | None = None
    ) -> pd.DataFrame:
        ...
    ```
  * **Behaviour**
    1. Slice *df* to `[start,â€¯end]` (inclusive) and drop the Date column into the index.  
    2. For every metric listed in `stats_cfg.metrics_to_run` **call the public registry** (`core.rank_selection._compute_metric_series`) to obtain a vectorised series.  
    3. **Concatenate** those series columnâ€‘wise into **`score_frame`** (`index = fund code`, `columns = metric names`, dtypeÂ `float64`).  
    4. Attach metadata  
       ```python
       score_frame.attrs["insample_len"] = len(window)        # number of bars
       score_frame.attrs["period"] = (start, end)             # optional helper
       ```
    5. Return `score_frame` â€“ *no side effects, no I/O*.

* **Update callers**
  * `pipeline._run_analysis()` should call `single_period_run()` once, stash the resulting frame in the returned dict under key `"score_frame"`, but **must not** change existing outputs or CLI flags.
  * Existing metricsâ€‘export logic stays exactly as is.

* **Tests**
  1. **Goldenâ€‘master**: compare the new `score_frame` against a preâ€‘generated CSV fixture for a small sample set.  
  2. **Metadata**: `assert sf.attrs["insample_len"] == expected_len`.  
  3. **Column order** equals the order of `stats_cfg.metrics_to_run`; failing this should raise.

* **Performance / style guardâ€‘rails**
  * Remain fully vectorisedâ€”no perâ€‘fund Python loops.
  * Keep `single_period_run()` *pure* (no global writes, no prints).
  * Do **not** introduce extra dependencies; stick to `numpy`â€¯+â€¯`pandas`.

> Once the test suite passes with the new `score_frame`, proceed to stepsâ€¯5â€‘7 (Selector & Weighting classes).


### Stepâ€¯5Â â€“Â Selector classes

| Class | Purpose | Key Inputs |
| ----- | ------- | ---------- |
| `RankSelector` | Pure rankâ€‘based inclusion identical to Phaseâ€¯1 behaviour, but exposed as a plugâ€‘in. | `score_frame`, `top_n`, `rank_column` |
| `ZScoreSelector` | Filters candidates whose zâ€‘score >Â `threshold`; supports negative screening by passing `direction=-1`. | `score_frame`, `threshold`, `direction` |

Both selectors must return **two** DataFrames:  
1. `selected` â€“ rows kept for the rebalancing date  
2. `log` â€“ diagnostic table (candidate, metric, reason) used by UI & tests.

---

### Stepâ€¯6Â â€“Â Weighting classes

| Class (inherits `BaseWeighting`) | Logic | YAML Config Stub |
| --- | --- | --- |
| `EqualWeight` | 1/N allocation across `selected`; rounds to nearest bps to avoid float dust. | `portfolio.weighting: {method: equal}` |
| `ScorePropSimple` | Weight âˆ positive score; rescales to 100â€¯%. | `portfolio.weighting: {method: score_prop}` |
| `ScorePropBayesian` | Same, but shrinks extreme scores toward the crossâ€‘sectional mean using a conjugateâ€‘normal update. | `portfolio.weighting: {method: score_prop_bayes, shrink_tau: 0.25}` |

---

### Stepâ€¯7Â â€“Â Engine wiring

Minimal loop in `multi_period/engine.py`:

```python
for date in schedule:
    candidates  = selector.select(score_frames[date])
    weights     = weighting.weight(candidates)
    portfolio.rebalance(date, weights)
```
Stepâ€¯8Â â€“Â Config schema delta

```yaml
metrics:
  registry: [annual_return, downside_deviation, sharpe_ratio]

portfolio:
  selector:
    name: zscore            # or 'rank'
    params:
      threshold: 1.0        # Ïƒ
  weighting:
    name: score_prop_bayes
    params:
      shrink_tau: 0.25
```

Stepâ€¯9Â â€“Â Unitâ€‘test skeletons

```
tests/
â””â”€ test_selector_weighting.py
   â”œâ”€ fixtures/score_frame_2025â€‘06â€‘30.csv
   â”œâ”€ test_rank_selector()
   â”œâ”€ test_zscore_selector_edge()
   â”œâ”€ test_equal_weighting_sum_to_one()
   â””â”€ test_bayesian_shrinkage_monotonic()
```

Goldenâ€‘master strategy identical to Phaseâ€‘1 metrics: pickle one known score_frame
and compare selector/weighting outputs bitâ€‘forâ€‘bit (tolerances <â€¯1eâ€‘9).

Stepâ€¯10Â â€“Â Docs housekeeping

Phaseâ€‘1 docs stay at docs/phase-1/Agents.md.
Phaseâ€‘2 docs live in docs/phase-2/Agents.md (this file).
Crossâ€‘link at the top.

<!-- STEPÂ 12Â START -->
---

##â€¯Stepâ€¯12â€¯â€“â€¯Adaptive Bayesian Crossâ€‘Period Weighting â€¯ğŸ¯

> **Problem**  
> Current weighting schemes (`equal`, `score_prop_*`) â€œresetâ€ every rebalance;
> persistent skill is ignored.  
> **Solution**â€¯â€”â€¯`AdaptiveBayesWeighting`: a conjugateâ€‘normal, exponentially
> decayed posterior that tilts capital toward managers with *sustained*
> high scores while letting laggards meanâ€‘revert.

### 12.1â€¯Â Class contract

```python
class AdaptiveBayesWeighting(BaseWeighting):
    """Stateâ€‘ful, crossâ€‘period Bayesian reâ€‘weighting.

    Posterior mean := capital share per fund.
    Posterior Ï„     := confidence (inverse variance) updated via scores.
    """

    def __init__(
        self,
        *,
        half_life: int = 90,          # days â€“ exponential decay of Ï„
        obs_sigma: float = 0.25,      # Ïƒ of score observation noise
        max_w: float | None = 0.20,   # optional hard cap on any single fund
        prior_mean: Literal["equal"] | ndarray = "equal",
        prior_tau: float = 1.0,
    ):
        ...

    def update(
        self,
        scores: pd.Series,            # indexÂ = fund, float64
        days: int                     # # calendar days since last rebalance
    ) -> None:
        """Bayesâ€‘update posteriors inâ€‘place (no return)."""

    def weight(self, candidates: pd.DataFrame) -> pd.Series:
        """Return weights **for this period**, sum ==Â 1.0, respects `max_w`."""

12.2â€¯Â Engine wiring
selector   = build_selector(cfg)          # unchanged
weighting  = build_weighting(cfg)         # may be AdaptiveBayesWeighting

for date, sf in score_frames.items():
    selected = selector.select(sf)[0]     # DataFrame
    weights  = weighting.weight(selected) # uses *current* posterior
    portfolio.rebalance(date, weights)

    # --- NEW: feed realised scores back in ---
    weighting.update(
        scores = sf.loc[weights.index, cfg.rank_column],
        days   = (date - prev_date).days
    )
    prev_date = date

12.3â€¯Â Config schema delta
portfolio:
  weighting:
    name: adaptive_bayes           # new
    params:
      half_life: 90                # intÂ days
      obs_sigma: 0.25              # score Ïƒ
      max_w: 0.20                  # clip (optional)
      prior_tau: 1.0               # prior precision

Backâ€‘compat: absence of these keys defaults to score_prop_simple.

12.4â€¯Â GUI additions
Weighting method dropdown now enumerates via the plugâ€‘in registry and
autoâ€‘discovers AdaptiveBayesWeighting.

If chosen, reveal controls:

half_lifeÂ â†’ IntSlider(30â€‘365)

obs_sigmaÂ â†’ FloatSlider(0â€‘1)

max_wÂ Â Â Â Â â†’ FloatSlider(0â€‘0.5)

prior_tauÂ â†’ FloatSlider(0â€‘5)

All four controls inherit the 300â€¯ms debounce wrapper.

12.5â€¯Â State & reset rules
Posterior state lives inside the weighting instance only.

Loading a new YAML (Stepâ€¯0) or clicking â€œâ†»â€¯Resetâ€ in the GUI rebuilds
the weighting object â†’ posteriors reset.

Periodic state can be serialised alongside ~/.trend_gui_state.yml
using pickle under the key adaptive_bayes_posteriors; safe to ignore
if absent.

12.6â€¯Â Tests (newâ€¯tests/test_adaptive_bayes.py)
Drift toward winners
Simulate three periods; fundâ€¯A topâ€‘scores each time.
assert weights_A_3 > weights_A_2 > weights_A_1.

Sumâ€‘toâ€‘one
numpy.testing.assert_allclose(weights.sum(), 1.0, rtol=1eâ€‘12).

Clip respects max_w
Force outlier posterior, assert weights.max() <= max_w + 1eâ€‘9.

Halfâ€‘life zero â†’ simple weighting
With half_life = 0, compare against ScorePropSimple.

12.7â€¯Â Open parameters for Phaseâ€¯2 signâ€‘off
Parameter	Default	Rationale	Can be tuned later?
half_life	90â€¯d	echo typical quarterly review cycle	âœ…
obs_sigma	0.25	reasonable dispersion of zâ€‘scores	âœ…
max_w	20â€¯%	avoid concentration risk	âœ…
prior_tau	1.0	uninformative prior	âœ…


---

**Status**

The `AdaptiveBayesWeighting` implementation and accompanying unit tests are
now merged. The GUI enumerates all weighting plugâ€‘ins automatically and the
defaults listed above match the shipped configuration. Persistent skill can
therefore be modelled outâ€‘ofâ€‘theâ€‘box.

## Demo pipeline (maintenance / CI)

Keep the demo scripts in sync with exporter and pipeline changes. Whenever a new
feature lands, run the sequence below and update `config/demo.yml` or
`scripts/run_multi_demo.py` so the demo exercises every code path. See
[../DemoMaintenance.md](../DemoMaintenance.md) for a concise checklist.

1. **Bootstrap the environment**
   ```bash
   ./scripts/setup_env.sh
   ```
2. **Generate the demo dataset**
   ```bash
   python scripts/generate_demo.py [--no-xlsx]
   ```
   The optional flag skips the Excel copy when binary artefacts are not needed.
3. **Run the full demo pipeline and export checks**
   ```bash
   python scripts/run_multi_demo.py
   ```
   The script must call `export.export_data()` so CSV, Excel, JSON **and TXT**
   outputs are produced in one go. Extend the script and config whenever new
   exporter options are introduced.
4. **Run the test suite**
   ```bash
   ./scripts/run_tests.sh
   ```
5. **Keep demo config current**
   - Update `config/demo.yml` and demo scripts whenever export or pipeline
     behaviour changes so the demo covers all features.



