{"type":"thread.started","thread_id":"019b8201-356a-74d0-a5de-59f2673c3c1c"}
{"type":"turn.started"}
{"type":"item.completed","item":{"id":"item_0","type":"reasoning","text":"**Planning initial inspection**"}}
{"type":"item.started","item":{"id":"item_1","type":"command_execution","command":"/bin/bash -lc ls","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_1","type":"command_execution","command":"/bin/bash -lc ls","aggregated_output":"Agents.md\nCHANGELOG.md\nCLAUDE.md\nCODE_OF_CONDUCT.md\nCONTRIBUTING.md\nDEPENDENCY_QUICKSTART.md\nDOCKER_QUICKSTART.md\nDockerfile\nIssues.txt\nLICENSE\nMANIFEST.in\nMakefile\nREADME.md\nREADME_APP.md\nREADME_DATA.md\nSECURITY.md\nTrend Universe Data.csv\nagents\nanalysis\narchives\nassets\ncliff.toml\ncodex-prompt-4138.md\ncodex-session-4138.jsonl\nconfig\ncoverage-summary.md\ndata\ndemo\ndocker-compose.yml\ndocs\nexamples\ngate-summary.md\nkeepalive_status.md\nman\nnotebooks\npackage-lock.json\npackage.json\nperf\npr_body.md\npyproject.toml\npytest.ini\nreports\nrequirements.lock\nretired\nscripts\nsrc\nstreamlit_app\ntests\ntools\ntrend_model.egg-info\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_2","type":"reasoning","text":"**Checking AGENTS.md for instructions**"}}
{"type":"item.started","item":{"id":"item_3","type":"command_execution","command":"/bin/bash -lc 'cat AGENTS.md'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_3","type":"command_execution","command":"/bin/bash -lc 'cat AGENTS.md'","aggregated_output":"cat: AGENTS.md: No such file or directory\n","exit_code":1,"status":"failed"}}
{"type":"item.completed","item":{"id":"item_4","type":"reasoning","text":"**Checking for Agents.md file**"}}
{"type":"item.started","item":{"id":"item_5","type":"command_execution","command":"/bin/bash -lc 'cat Agents.md'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_5","type":"command_execution","command":"/bin/bash -lc 'cat Agents.md'","aggregated_output":"# Agents.md\n\n> **Canonical implementation roadmap**: See [docs/phase-2/Agents.md](docs/phase-2/Agents.md) for the complete Phase-2 implementation spec and step-by-step guidance.\n\n## Mission\n\nConverge the scattered modules into one fully test-covered, vectorised pipeline that can be invoked from a single CLI entry-point. Never touch notebooks living under any directory whose name ends in `old/`.\n\n---\n\n## Quick Reference\n\n### Demo Pipeline\n\n```bash\n# 1. Bootstrap environment\n./scripts/setup_env.sh\n\n# 2. Generate demo dataset\npython scripts/generate_demo.py\n\n# 3. Run full demo pipeline\npython scripts/run_multi_demo.py\n\n# 4. Run test suite\n./scripts/run_tests.sh\n```\n\nSee [docs/DemoMaintenance.md](docs/DemoMaintenance.md) for the full checklist.\n\n### Key Entry Points\n\n| Purpose | Command |\n|---------|---------|\n| CLI analysis | `PYTHONPATH=\"./src\" python -m trend_analysis.run_analysis -c config/demo.yml` |\n| Streamlit app | `./scripts/run_streamlit.sh` |\n| Tests | `./scripts/run_tests.sh` |\n| Fast validation | `./scripts/dev_check.sh --fix` |\n\n---\n\n## Automation Entry Points\n\n### Agents 70 Orchestrator\n- **File**: `.github/workflows/agents-70-orchestrator.yml`\n- **Role**: Single automation front door for all agent operations\n- **Triggers**: 20-minute schedule sweep plus manual `workflow_dispatch`\n\n### Agents 63 Issue Intake\n- **File**: `.github/workflows/agents-63-issue-intake.yml`\n- **Role**: Bootstrap Codex PRs on `agent:codex` labeled issues\n- **Triggers**: Issue events (`opened`, `labeled`, `reopened`) plus manual dispatch\n\n### Agents 64 Verify Agent Assignment\n- **File**: `.github/workflows/agents-64-verify-agent-assignment.yml`\n- **Role**: Validates agent assignment before orchestrator proceeds\n\nLegacy consumer wrappers were retired. See [docs/archive/ARCHIVE_WORKFLOWS.md](docs/archive/ARCHIVE_WORKFLOWS.md).\n\n---\n\n## Canonical Module Locations\n\n| Layer / Concern | Canonical Location | Deprecated |\n|-----------------|-------------------|------------|\n| Data ingest & cleaning | `trend_analysis/data.py` | `data_utils.py`, notebook helpers |\n| Portfolio logic & metrics | `trend_analysis/metrics.py` | loops in `run_analysis.py` |\n| Export / I/O | `trend_analysis/export/` | root-level `exports.py` |\n| Domain kernels | `trend_analysis/core/` | standalone `core/` directory |\n| Pipeline orchestration | `trend_analysis/pipeline.py` | duplicated control flow |\n| CLI entry-point | `trend_analysis/cli.py` | bespoke `scripts/*.py` |\n| Config | `config/defaults.yml` via `trend_analysis.config.load()` | hard-coded constants |\n| Tests | `tests/` (pytest with coverage gate) | â€” |\n\n**Rule**: One concern â†’ one module. Replacements must delete or comment-out whatever they obsolete in the same PR.\n\n---\n\n## Guard-Rails\n\n### Communication\n- When a request contains multiple tasks, explicitly recap which items are complete and which remain before handing control back.\n- Call out any suggestion that is only a partial fix, and list the follow-up steps required for it to succeed.\n- Highlight assumptions about workflow or automation status and offer to pull the live data when needed.\n\n### Code Quality\n- **Vectorise first**: Falling back to for-loops requires a comment justifying why vectorisation is impossible or harmful.\n- **Public API**: Uses US-English snake_case; private helpers prefixed with `_`.\n- **No circular imports**: `pipeline.py` orchestrates; nothing imports it.\n- **Test fixtures**: Must be text-serialised (CSV/JSON); no binary formats in PRs.\n\n### Git Workflow\nBefore pushing, verify the target PR is still open:\n```bash\ngh pr view --json state,mergedAt,closed\n```\n- âŒ Never push to merged/closed PRs\n- âœ… Create a new branch and PR if the original is closed\n\n---\n\n## Export Guard-Rails\n\n> ðŸ›¡ï¸ Codex removed the pretty reporting layer once; it shall not happen again.\n\n1. **Call the canonical exporters**: After `pipeline.run()` completes, pipe results into exactly one of `export_to_excel`, `export_to_csv`, or `export_to_json`.\n\n2. **Excel format contract**:\n   - Bold title row\n   - `0.00%` for CAGR & Vol, `0.00` for Sharpe & Sortino\n   - Red numerals for MaxDD\n   - Freeze panes on header, auto-filter\n   - Column width = `max(len(header)) + 2`\n\n3. **Column order is law**: Tests must fail if column order mutates.\n\n4. **Back-compat**: Silent config = drop fully formatted Excel workbook into `outputs/` exactly as v1.0 did.\n\n---\n\n## Multi-Period Export (Phase-2)\n\nâœ… **Implemented**: Multi-period runs now emit Phase-1 style exports via:\n- `export_phase1_workbook()` - Excel with one sheet per period + summary tab\n- `export_phase1_multi_metrics()` - CSV/JSON with `*_periods.*` and `*_summary.*` files\n- `export_multi_period_metrics()` - General multi-period export helper\n\nHelper functions:\n- `workbook_frames_from_results()` - Builds sheet mapping\n- `period_frames_from_results()` - Converts result sequence to export format\n- `combined_summary_result()` - Aggregates portfolio returns across periods\n\n---\n\n## Feature Status\n\n### Implemented âœ…\n- Rank-based manager selection mode (`mode: rank`)\n- Blended scoring with z-score normalization\n- Scalar metric memoization (opt-in via `performance.cache.metrics: true`)\n- PR draft toggle for Codex bootstrap (`codex_pr_draft` input)\n- Multi-period Phase-1 style exports\n- Selector and weighting plugin classes\n\n### Backlog ðŸ“‹\n- Preview score frame in UI\n- Weight heatmap visualization\n- Expected shortfall metric\n- Diversification value metric\n- Export commit hash in outputs\n\n---\n\n## Debugging\n\n### Fund Selection Issues\n\nUse the debug script in `examples/`:\n```bash\npython examples/debug_fund_selection.py\n```\n\nThis reveals:\n- Which managers get filtered due to missing data\n- Available manager pool for selection\n- Actual ranking results\n\n### Common Pitfalls\n- Don't assume ranking is wrong without checking data completeness first\n- Both in-sample AND out-of-sample periods must have complete data\n- Verify configuration parameters match intended behavior\n\n---\n\n## Related Documentation\n\n- [docs/phase-2/Agents.md](docs/phase-2/Agents.md) - Complete implementation spec\n- [docs/DemoMaintenance.md](docs/DemoMaintenance.md) - Demo pipeline checklist\n- [docs/archive/ARCHIVE_WORKFLOWS.md](docs/archive/ARCHIVE_WORKFLOWS.md) - Retired workflows\n- [docs/metric_cache.md](docs/metric_cache.md) - Metric memoization details\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_6","type":"reasoning","text":"**Searching for tasks with rg max_turnover**"}}
{"type":"item.started","item":{"id":"item_7","type":"command_execution","command":"/bin/bash -lc 'rg -n \"max_turnover|turnover\" src tests scripts'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_7","type":"command_execution","command":"/bin/bash -lc 'rg -n \"max_turnover|turnover\" src tests scripts'","aggregated_output":"scripts/evaluate_settings_effectiveness.py:84:    \"max_turnover\": 0.3,\nscripts/evaluate_settings_effectiveness.py:446:    turnover = analysis.turnover if analysis is not None else pd.Series(dtype=float)\nscripts/evaluate_settings_effectiveness.py:466:        \"turnover\": turnover,\nscripts/evaluate_settings_effectiveness.py:483:def _mean_turnover(turnover: pd.Series) -> float:\nscripts/evaluate_settings_effectiveness.py:484:    if turnover is None or turnover.empty:\nscripts/evaluate_settings_effectiveness.py:486:    return float(turnover.abs().mean())\nscripts/evaluate_settings_effectiveness.py:601:    turnover_diff = float(\nscripts/evaluate_settings_effectiveness.py:602:        _mean_turnover(test_summary[\"turnover\"])\nscripts/evaluate_settings_effectiveness.py:603:        - _mean_turnover(base_summary[\"turnover\"])\nscripts/evaluate_settings_effectiveness.py:606:        test_summary.get(\"costs\", {}).get(\"turnover_applied\", 0.0)\nscripts/evaluate_settings_effectiveness.py:607:        - base_summary.get(\"costs\", {}).get(\"turnover_applied\", 0.0)\nscripts/evaluate_settings_effectiveness.py:632:            not math.isnan(turnover_diff) and abs(turnover_diff) > 1.0e-4,\nscripts/evaluate_settings_effectiveness.py:677:            \"turnover_diff\": turnover_diff,\nsrc/trend_analysis/multi_period/engine.py:208:def _compute_turnover_state(\nsrc/trend_analysis/multi_period/engine.py:213:    \"\"\"Vectorised turnover computation used by ``run_schedule``.\nsrc/trend_analysis/multi_period/engine.py:227:        Total turnover together with the index/value arrays to persist for the\nsrc/trend_analysis/multi_period/engine.py:249:    turnover = float(np.abs(new_aligned - prev_aligned).sum())\nsrc/trend_analysis/multi_period/engine.py:250:    return turnover, nidx, nvals\nsrc/trend_analysis/multi_period/engine.py:390:def _apply_turnover_penalty(\nsrc/trend_analysis/multi_period/engine.py:397:    \"\"\"Shrink trades toward the previous allocation to damp turnover.\nsrc/trend_analysis/multi_period/engine.py:400:    reduces total turnover when ``lambda_tc`` is in ``(0, 1]``. Bounds are\nsrc/trend_analysis/multi_period/engine.py:413:    \"\"\"Minimal container for weight, turnover and cost history.\"\"\"\nsrc/trend_analysis/multi_period/engine.py:416:    turnover: Dict[str, float] = field(default_factory=dict)\nsrc/trend_analysis/multi_period/engine.py:424:        turnover: float = 0.0,\nsrc/trend_analysis/multi_period/engine.py:442:        self.turnover[key] = float(turnover)\nsrc/trend_analysis/multi_period/engine.py:496:    # Fast turnover state (index array + values array)\nsrc/trend_analysis/multi_period/engine.py:504:    def _fast_turnover(\nsrc/trend_analysis/multi_period/engine.py:509:        \"\"\"Compute turnover between previous and new weights using NumPy.\nsrc/trend_analysis/multi_period/engine.py:522:            turnover : float\nsrc/trend_analysis/multi_period/engine.py:527:        # First period: turnover = sum(abs(new_w))\nsrc/trend_analysis/multi_period/engine.py:557:        turnover = float(np.abs(new_aligned - prev_aligned).sum())\nsrc/trend_analysis/multi_period/engine.py:558:        return turnover, nidx, nvals\nsrc/trend_analysis/multi_period/engine.py:623:            # Fast turnover computation\nsrc/trend_analysis/multi_period/engine.py:624:            turnover, prev_tidx, prev_tvals = _fast_turnover(\nsrc/trend_analysis/multi_period/engine.py:633:            turnover, prev_tidx, prev_tvals = _compute_turnover_state(\nsrc/trend_analysis/multi_period/engine.py:638:        pf.rebalance(date, weights, turnover, cost)\nsrc/trend_analysis/multi_period/engine.py:654:    # Optional debug validation: recompute turnover from stored history and compare\nsrc/trend_analysis/multi_period/engine.py:669:                got = pf.turnover[d]\nsrc/trend_analysis/multi_period/engine.py:1042:                max_turnover=cfg.portfolio.get(\"max_turnover\"),\nsrc/trend_analysis/multi_period/engine.py:1663:    # Transaction cost and turnover-cap controls (Issue #429)\nsrc/trend_analysis/multi_period/engine.py:1666:    max_turnover_cap = float(cfg.portfolio.get(\"max_turnover\", 1.0))\nsrc/trend_analysis/multi_period/engine.py:2109:                        \"turnover\": 0.0,\nsrc/trend_analysis/multi_period/engine.py:3099:            max_changes_raw = cfg.portfolio.get(\"turnover_budget_max_changes\")\nsrc/trend_analysis/multi_period/engine.py:3122:                    # Exits/drops are always honoured; turnover budget limits\nsrc/trend_analysis/multi_period/engine.py:3134:                                \"reason\": \"turnover_budget\",\nsrc/trend_analysis/multi_period/engine.py:3162:                                \"reason\": \"turnover_budget\",\nsrc/trend_analysis/multi_period/engine.py:3396:        # Subsequent turnover alignment (union with previous holdings) may\nsrc/trend_analysis/multi_period/engine.py:3401:        # Enforce optional turnover cap by scaling trades towards target\nsrc/trend_analysis/multi_period/engine.py:3415:            target_w = _apply_turnover_penalty(\nsrc/trend_analysis/multi_period/engine.py:3419:        # Forced exits must not be diluted by turnover-penalty shrinkage.\nsrc/trend_analysis/multi_period/engine.py:3428:        desired_turnover = float(desired_trades.abs().sum())\nsrc/trend_analysis/multi_period/engine.py:3431:            max_turnover_cap < 1.0 - NUMERICAL_TOLERANCE_HIGH\nsrc/trend_analysis/multi_period/engine.py:3432:            and desired_turnover > max_turnover_cap + NUMERICAL_TOLERANCE_HIGH\nsrc/trend_analysis/multi_period/engine.py:3434:            # Respect turnover cap, but prioritise forced exits (soft/hard z exits).\nsrc/trend_analysis/multi_period/engine.py:3436:            # solely because turnover is capped.\nsrc/trend_analysis/multi_period/engine.py:3447:            mandatory_turnover = float(mandatory.abs().sum())\nsrc/trend_analysis/multi_period/engine.py:3449:            optional_turnover = float(optional.abs().sum())\nsrc/trend_analysis/multi_period/engine.py:3451:            if mandatory_turnover >= max_turnover_cap - NUMERICAL_TOLERANCE_HIGH:\nsrc/trend_analysis/multi_period/engine.py:3456:                remaining_turnover = max_turnover_cap - mandatory_turnover\nsrc/trend_analysis/multi_period/engine.py:3458:                    remaining_turnover / optional_turnover\nsrc/trend_analysis/multi_period/engine.py:3459:                    if optional_turnover > 0\nsrc/trend_analysis/multi_period/engine.py:3471:        # We keep the internal turnover-cap/bounds logic here, but reconcile the\nsrc/trend_analysis/multi_period/engine.py:3491:        # Construct previous weights dict for pipeline (turnover tracking)\nsrc/trend_analysis/multi_period/engine.py:3517:            # Pass turnover parameters for pipeline-level enforcement.\nsrc/trend_analysis/multi_period/engine.py:3518:            # The pipeline applies vol-scaling after which turnover may exceed\nsrc/trend_analysis/multi_period/engine.py:3520:            # the final weights respect the turnover constraint.\nsrc/trend_analysis/multi_period/engine.py:3522:            max_turnover=max_turnover_cap if max_turnover_cap < 1.0 else None,\nsrc/trend_analysis/multi_period/engine.py:3637:        # Compute turnover/cost from the realised weights, not the intended ones.\nsrc/trend_analysis/multi_period/engine.py:3638:        # Convention: report one-sided turnover (sum of buys or sells). For\nsrc/trend_analysis/multi_period/engine.py:3644:            period_turnover = abs_diff\nsrc/trend_analysis/multi_period/engine.py:3650:            period_turnover = 0.5 * abs_diff\nsrc/trend_analysis/multi_period/engine.py:3652:        period_cost = period_turnover * ((tc_bps + slippage_bps) / 10000.0)\nsrc/trend_analysis/multi_period/engine.py:3875:        res_dict[\"turnover\"] = period_turnover\nsrc/trend_analysis/multi_period/engine.py:3884:        # Persist realised weights for next-period turnover logic.\nsrc/trend_analysis/multi_period/engine.py:3886:        # union-alignment used for turnover computations.\nscripts/reproduce_ui_run.py:532:                    \"turnover\": item.get(\"turnover\"),\nscripts/reproduce_ui_run.py:566:        max_changes = int(cfg.portfolio.get(\"turnover_budget_max_changes\") or 0)\nsrc/trend_analysis/backtesting/harness.py:32:    \"\"\"Linear transaction cost model applied to turnover events.\"\"\"\nsrc/trend_analysis/backtesting/harness.py:66:    def apply(self, turnover: float) -> float:\nsrc/trend_analysis/backtesting/harness.py:67:        if turnover <= 0:\nsrc/trend_analysis/backtesting/harness.py:72:        return float(turnover) * multiplier\nsrc/trend_analysis/backtesting/harness.py:90:    turnover: pd.Series\nsrc/trend_analysis/backtesting/harness.py:91:    per_period_turnover: pd.Series\nsrc/trend_analysis/backtesting/harness.py:117:            \"turnover\": _series_to_dict(self.turnover),\nsrc/trend_analysis/backtesting/harness.py:118:            \"per_period_turnover\": _series_to_dict(self.per_period_turnover),\nsrc/trend_analysis/backtesting/harness.py:147:    turnover_cap: float | None = None,\nsrc/trend_analysis/backtesting/harness.py:160:        turnover_cap: Optional per-rebalance cap on total absolute turnover.\nsrc/trend_analysis/backtesting/harness.py:162:            weights so the executed turnover does not exceed the limit.\nsrc/trend_analysis/backtesting/harness.py:183:    if turnover_cap is not None and turnover_cap < 0:\nsrc/trend_analysis/backtesting/harness.py:184:        raise ValueError(\"turnover_cap must be non-negative when provided\")\nsrc/trend_analysis/backtesting/harness.py:217:    turnover = pd.Series(dtype=float)\nsrc/trend_analysis/backtesting/harness.py:218:    per_period_turnover = pd.Series(0.0, index=data.index, dtype=float)\nsrc/trend_analysis/backtesting/harness.py:282:            turnover.loc[date] = 0.0\nsrc/trend_analysis/backtesting/harness.py:298:        desired_turnover = float(trades.abs().sum())\nsrc/trend_analysis/backtesting/harness.py:299:        execute = desired_turnover >= float(min_trade)\nsrc/trend_analysis/backtesting/harness.py:300:        applied_turnover = desired_turnover if execute else 0.0\nsrc/trend_analysis/backtesting/harness.py:302:        if execute and turnover_cap is not None and turnover_cap >= 0:\nsrc/trend_analysis/backtesting/harness.py:303:            if desired_turnover > turnover_cap:\nsrc/trend_analysis/backtesting/harness.py:304:                scale = turnover_cap / desired_turnover if desired_turnover > 0 else 0.0\nsrc/trend_analysis/backtesting/harness.py:306:                applied_turnover = float((new_weights - prev_weights).abs().sum())\nsrc/trend_analysis/backtesting/harness.py:307:        cost = model.apply(applied_turnover)\nsrc/trend_analysis/backtesting/harness.py:310:        turnover.loc[date] = applied_turnover\nsrc/trend_analysis/backtesting/harness.py:316:        if 0 <= apply_slice.start < len(per_period_turnover):\nsrc/trend_analysis/backtesting/harness.py:317:            per_period_turnover.iloc[apply_slice.start] = applied_turnover\nsrc/trend_analysis/backtesting/harness.py:351:    turnover = turnover.reindex(calendar_index, fill_value=0.0)\nsrc/trend_analysis/backtesting/harness.py:358:        turnover=turnover.sort_index(),\nsrc/trend_analysis/backtesting/harness.py:359:        per_period_turnover=per_period_turnover,\nsrc/trend_analysis/backtesting/harness.py:428:        # rebalance with zero turnover. This preserves backward-compatibility\nscripts/benchmark_performance.py:32:from trend_analysis.multi_period.engine import _compute_turnover_state\nscripts/benchmark_performance.py:44:def _python_turnover_state(\nscripts/benchmark_performance.py:75:    turnover = float(np.abs(new_aligned - prev_aligned).sum())\nscripts/benchmark_performance.py:76:    return turnover, nidx, nvals\nscripts/benchmark_performance.py:93:def _benchmark_turnover_alignment(rows: int, cols: int, runs: int) -> dict:\nscripts/benchmark_performance.py:106:            turnover, prev_idx_py, prev_vals_py = _python_turnover_state(\nscripts/benchmark_performance.py:109:            total_py += turnover\nscripts/benchmark_performance.py:117:            turnover, prev_idx_vec, prev_vals_vec = _compute_turnover_state(\nscripts/benchmark_performance.py:120:            total_vec += turnover\nscripts/benchmark_performance.py:138:def _python_turnover_cap(\nscripts/benchmark_performance.py:149:    if total_desired <= strategy.max_turnover:\nscripts/benchmark_performance.py:150:        actual_turnover = float(total_desired)\nscripts/benchmark_performance.py:164:        remaining = strategy.max_turnover\nscripts/benchmark_performance.py:178:        actual_turnover = float(executed.abs().sum())\nscripts/benchmark_performance.py:180:    cost = strategy._calculate_cost(actual_turnover)\nscripts/benchmark_performance.py:203:def _benchmark_turnover_cap(cols: int, scenarios: int, runs: int) -> dict:\nscripts/benchmark_performance.py:213:            params = {\"max_turnover\": 0.25, \"cost_bps\": 10, \"priority\": priority}\nscripts/benchmark_performance.py:220:                _python_turnover_cap(strat_python, current, target, score_input)\nscripts/benchmark_performance.py:262:    turnover_alignment = _benchmark_turnover_alignment(rows, cols, runs)\nscripts/benchmark_performance.py:263:    turnover_cap = _benchmark_turnover_cap(cols, rows, runs)\nscripts/benchmark_performance.py:279:        \"turnover_vectorization\": turnover_alignment,\nscripts/benchmark_performance.py:280:        \"turnover_cap_vectorization\": turnover_cap,\nsrc/trend_analysis/config/models.py:326:            \"\"\"Validate and normalise turnover / transaction cost controls.\nsrc/trend_analysis/config/models.py:333:            # Transaction cost (basis points per 1 unit turnover)\nsrc/trend_analysis/config/models.py:343:            # Max turnover cap (fraction of portfolio; 1.0 = effectively uncapped)\nsrc/trend_analysis/config/models.py:344:            if \"max_turnover\" in v:\nsrc/trend_analysis/config/models.py:345:                raw = v[\"max_turnover\"]\nsrc/trend_analysis/config/models.py:349:                    raise ValueError(\"max_turnover must be numeric\") from exc\nsrc/trend_analysis/config/models.py:351:                    raise ValueError(\"max_turnover must be >= 0\")\nsrc/trend_analysis/config/models.py:354:                    raise ValueError(\"max_turnover must be <= 2.0\")\nsrc/trend_analysis/config/models.py:355:                v[\"max_turnover\"] = mt\nsrc/trend_analysis/config/models.py:508:            # Light-weight validation for turnover / cost controls\nsrc/trend_analysis/config/models.py:521:                if \"max_turnover\" in port:\nsrc/trend_analysis/config/models.py:523:                        mt = float(port[\"max_turnover\"])\nsrc/trend_analysis/config/models.py:525:                        raise ValueError(\"max_turnover must be numeric\") from exc\nsrc/trend_analysis/config/models.py:527:                        raise ValueError(\"max_turnover must be >= 0\")\nsrc/trend_analysis/config/models.py:529:                        raise ValueError(\"max_turnover must be <= 2.0\")\nsrc/trend_analysis/config/models.py:530:                    port[\"max_turnover\"] = mt\ntests/test_viz_charts_edgecases.py:17:def test_turnover_series_handles_missing_assets():\ntests/test_viz_charts_edgecases.py:26:    result = charts.turnover_series(weights_map)\ntests/test_viz_charts_edgecases.py:30:    assert list(df.columns) == [\"turnover\"]\ntests/test_viz_charts_edgecases.py:35:    expected = w_df.diff().abs().sum(axis=1).to_frame(\"turnover\")\nsrc/trend_analysis/typing.py:30:    turnover: float\nsrc/trend_analysis/config/model.py:350:    max_turnover: float\nsrc/trend_analysis/config/model.py:362:    turnover_cap: float | None = None\nsrc/trend_analysis/config/model.py:398:    @field_validator(\"max_turnover\", mode=\"before\")\nsrc/trend_analysis/config/model.py:400:    def _validate_turnover(cls, value: Any) -> float:\nsrc/trend_analysis/config/model.py:402:            turnover = float(value)\nsrc/trend_analysis/config/model.py:404:            raise ValueError(\"portfolio.max_turnover must be numeric.\") from exc\nsrc/trend_analysis/config/model.py:405:        if turnover < 0:\nsrc/trend_analysis/config/model.py:406:            raise ValueError(\"portfolio.max_turnover cannot be negative.\")\nsrc/trend_analysis/config/model.py:407:        if turnover > 1:\nsrc/trend_analysis/config/model.py:409:                \"portfolio.max_turnover must be between 0 and 1.0 inclusive to cap per-period turnover.\"\nsrc/trend_analysis/config/model.py:411:        return turnover\nsrc/trend_analysis/config/model.py:413:    @field_validator(\"turnover_cap\", mode=\"before\")\nsrc/trend_analysis/config/model.py:415:    def _validate_turnover_cap(cls, value: Any) -> float | None:\nsrc/trend_analysis/config/model.py:418:        return cls._validate_turnover(value)\ntests/test_config_load.py:36:            \"max_turnover\": 0.25,\nsrc/trend_analysis/config/bridge.py:21:    max_turnover: float,\nsrc/trend_analysis/config/bridge.py:47:            \"max_turnover\": max_turnover,\nscripts/generate_settings_evidence.py:351:            \"Cost setting. Ensure it is used in turnover/cost calculations \"\nsrc/trend_analysis/api.py:120:    turnover: pd.Series | None = None\nsrc/trend_analysis/api.py:186:    # Build combined turnover series from all periods\nsrc/trend_analysis/api.py:187:    turnover_series = _build_multi_period_turnover(period_results)\nsrc/trend_analysis/api.py:234:        turnover=turnover_series,\nsrc/trend_analysis/api.py:244:            if rr.turnover is None:\nsrc/trend_analysis/api.py:245:                rr.turnover = structured.turnover\nsrc/trend_analysis/api.py:254:def _build_multi_period_turnover(\nsrc/trend_analysis/api.py:257:    \"\"\"Build a combined turnover series from multi-period results.\"\"\"\nsrc/trend_analysis/api.py:258:    turnover_data: dict[str, float] = {}\nsrc/trend_analysis/api.py:269:        # Try to extract turnover from various possible locations\nsrc/trend_analysis/api.py:270:        turnover_val = res.get(\"turnover\")\nsrc/trend_analysis/api.py:271:        if turnover_val is None:\nsrc/trend_analysis/api.py:274:                turnover_val = risk_diag.get(\"turnover\")\nsrc/trend_analysis/api.py:276:        if isinstance(turnover_val, (int, float)):\nsrc/trend_analysis/api.py:277:            turnover_data[out_start] = float(turnover_val)\nsrc/trend_analysis/api.py:278:        elif isinstance(turnover_val, pd.Series) and not turnover_val.empty:\nsrc/trend_analysis/api.py:279:            # Take the last turnover value from this period\nsrc/trend_analysis/api.py:280:            turnover_data[out_start] = float(turnover_val.iloc[-1])\nsrc/trend_analysis/api.py:282:    if not turnover_data:\nsrc/trend_analysis/api.py:285:    return pd.Series(turnover_data, name=\"turnover\").sort_index()\nsrc/trend_analysis/api.py:465:        max_turnover=config.portfolio.get(\"max_turnover\"),\nsrc/trend_analysis/api.py:644:            rr.turnover = structured.turnover\ntests/test_multi_period_regime_wiring.py:47:                \"max_turnover\": 1.0,\nsrc/trend_analysis/pipeline.py:176:    turnover_cap: float | None\nsrc/trend_analysis/pipeline.py:752:    max_turnover: float | None,\nsrc/trend_analysis/pipeline.py:1020:    turnover_cap = None\nsrc/trend_analysis/pipeline.py:1021:    if max_turnover is not None:\nsrc/trend_analysis/pipeline.py:1023:            mt = float(max_turnover)\nsrc/trend_analysis/pipeline.py:1027:            turnover_cap = mt\nsrc/trend_analysis/pipeline.py:1056:            max_turnover=turnover_cap,\nsrc/trend_analysis/pipeline.py:1096:            turnover=pd.Series(dtype=float, name=\"turnover\"),\nsrc/trend_analysis/pipeline.py:1097:            turnover_value=float(\"nan\"),\nsrc/trend_analysis/pipeline.py:1228:        turnover_cap=turnover_cap,\nsrc/trend_analysis/pipeline.py:1353:            \"max_turnover\": computation.turnover_cap,\nsrc/trend_analysis/pipeline.py:1393:                \"turnover\": computation.risk_diagnostics.turnover,\nsrc/trend_analysis/pipeline.py:1394:                \"turnover_value\": computation.risk_diagnostics.turnover_value,\nsrc/trend_analysis/pipeline.py:2311:    max_turnover: float | None = None,\nsrc/trend_analysis/pipeline.py:2395:        max_turnover=max_turnover,\nsrc/trend_analysis/pipeline.py:2445:    max_turnover: float | None = None,\nsrc/trend_analysis/pipeline.py:2482:        max_turnover=max_turnover,\nsrc/trend_analysis/pipeline.py:2540:    max_turnover: float | None = None,\nsrc/trend_analysis/pipeline.py:2592:        max_turnover=max_turnover,\nsrc/trend_analysis/pipeline.py:2702:        max_turnover=_section_get(portfolio_cfg, \"max_turnover\"),\nsrc/trend_analysis/pipeline.py:2830:        max_turnover=_section_get(portfolio_cfg, \"max_turnover\"),\ntests/test_policy_engine_cov.py:68:def test_decide_hires_fires_diversification_and_turnover(monkeypatch):\ntests/test_policy_engine_cov.py:69:    \"\"\"Bucket caps and turnover limits should constrain hires/fires.\"\"\"\ntests/test_policy_engine_cov.py:79:        turnover_budget_max_changes=1,\ntests/test_policy_engine_cov.py:113:def test_decide_hires_fires_turnover_budget_prioritises(monkeypatch):\ntests/test_policy_engine_cov.py:121:        turnover_budget_max_changes=2,\ntests/test_policy_engine_cov.py:152:def test_decide_hires_fires_turnover_budget_mixed_moves(monkeypatch):\ntests/test_policy_engine_cov.py:162:        turnover_budget_max_changes=1,\ntests/test_policy_engine_cov.py:189:def test_decide_hires_fires_turnover_budget_trims_mixed(monkeypatch):\ntests/test_policy_engine_cov.py:196:        turnover_budget_max_changes=1,\ntests/test_policy_engine_cov.py:223:    # With a turnover budget of one move the higher scored hire should be kept\ntests/test_policy_engine_cov.py:228:def test_decide_hires_fires_turnover_budget_mixes_hires_and_fires(monkeypatch):\ntests/test_policy_engine_cov.py:234:        turnover_budget_max_changes=1,\ntests/test_policy_engine_cov.py:274:        turnover_budget_max_changes=1,\ntests/test_policy_engine_cov.py:305:    # NaN priority for the fired manager should drop it from turnover-constrained moves\ntests/test_trend_analysis_config_model.py:321:        \"max_turnover\": \"0.5\",\ntests/test_trend_analysis_config_model.py:327:    assert result.max_turnover == pytest.approx(0.5)\ntests/test_trend_analysis_config_model.py:334:        \"max_turnover\": 0.5,\ntests/test_trend_analysis_config_model.py:347:                \"max_turnover\": 0.5,\ntests/test_trend_analysis_config_model.py:358:            \"max_turnover\": 0.5,\ntests/test_trend_analysis_config_model.py:372:                \"max_turnover\": 0.5,\ntests/test_trend_analysis_config_model.py:380:    with pytest.raises(ValueError, match=\"turnover cannot be negative\"):\ntests/test_trend_analysis_config_model.py:384:                \"max_turnover\": -0.1,\ntests/test_trend_analysis_config_model.py:392:                \"max_turnover\": 1.5,\ntests/test_trend_analysis_config_model.py:400:                \"max_turnover\": 0.4,\ntests/test_trend_analysis_config_model.py:411:                \"max_turnover\": 0.1,\ntests/test_trend_analysis_config_model.py:503:            \"max_turnover\": 0.5,\ntests/test_trend_analysis_config_model.py:520:    assert cfg.portfolio.max_turnover == pytest.approx(0.5)\nscripts/test_settings_wiring.py:97:        expected_metric=\"turnover\",\nscripts/test_settings_wiring.py:99:        description=\"Buy and hold should produce lower turnover\",\nscripts/test_settings_wiring.py:315:        name=\"max_turnover\",\nscripts/test_settings_wiring.py:319:        expected_metric=\"actual_turnover\",\nscripts/test_settings_wiring.py:321:        description=\"Lower max turnover should constrain actual turnover\",\nscripts/test_settings_wiring.py:451:        \"max_turnover\": 1.0,\nscripts/test_settings_wiring.py:636:    max_turnover = _coerce_positive_float(state.get(\"max_turnover\"), default=1.0)\nscripts/test_settings_wiring.py:694:        \"max_turnover\": max_turnover,\nscripts/test_settings_wiring.py:713:        portfolio_cfg[\"turnover_budget_max_changes\"] = max_changes_per_period\nscripts/test_settings_wiring.py:997:    if metric_name == \"turnover\":\nscripts/test_settings_wiring.py:998:        if result.turnover is not None:\nscripts/test_settings_wiring.py:999:            return float(result.turnover.mean()) if len(result.turnover) > 0 else 0.0\nscripts/test_settings_wiring.py:1001:            turnovers = [\nscripts/test_settings_wiring.py:1002:                p.get(\"turnover\", 0.0) for p in result.period_results if \"turnover\" in p\nscripts/test_settings_wiring.py:1004:            return np.mean(turnovers) if turnovers else 0.0\nscripts/test_settings_wiring.py:1007:    if metric_name == \"actual_turnover\":\nscripts/test_settings_wiring.py:1008:        # Extract turnover from period results\nscripts/test_settings_wiring.py:1010:            turnovers = [\nscripts/test_settings_wiring.py:1011:                p.get(\"turnover\", 0.0) for p in result.period_results if \"turnover\" in p\nscripts/test_settings_wiring.py:1013:            return float(sum(turnovers)) if turnovers else 0.0\nscripts/test_settings_wiring.py:1014:        if result.turnover is not None:\nscripts/test_settings_wiring.py:1015:            return float(result.turnover.sum()) if len(result.turnover) > 0 else 0.0\nscripts/test_settings_wiring.py:1142:        \"actual_turnover\",\nscripts/test_settings_wiring.py:1256:    # max_turnover needs a churny setup so the cap becomes binding.\nscripts/test_settings_wiring.py:1257:    if setting.name == \"max_turnover\":\nsrc/trend_analysis/rebalancing/strategies.py:5:turnover constraints and transaction cost modelling.  Strategies are\nsrc/trend_analysis/rebalancing/strategies.py:22:# Small epsilon value for turnover comparisons to handle numerical precision\nsrc/trend_analysis/rebalancing/strategies.py:26:@rebalancer_registry.register(\"turnover_cap\")\nsrc/trend_analysis/rebalancing/strategies.py:30:    Limits the total turnover (sum of absolute trades) per rebalancing\nsrc/trend_analysis/rebalancing/strategies.py:37:        self.max_turnover = float(self.params.get(\"max_turnover\", 0.2))\nsrc/trend_analysis/rebalancing/strategies.py:48:        \"\"\"Apply turnover cap with trade prioritization and cost modeling.\nsrc/trend_analysis/rebalancing/strategies.py:64:            New weights after turnover cap and total transaction cost\nsrc/trend_analysis/rebalancing/strategies.py:73:        total_desired_turnover = trades.abs().sum()\nsrc/trend_analysis/rebalancing/strategies.py:75:        # If within turnover limit, execute all trades\nsrc/trend_analysis/rebalancing/strategies.py:76:        if total_desired_turnover <= self.max_turnover:\nsrc/trend_analysis/rebalancing/strategies.py:77:            actual_turnover = total_desired_turnover\nsrc/trend_analysis/rebalancing/strategies.py:80:            # Need to scale trades to respect turnover cap\nsrc/trend_analysis/rebalancing/strategies.py:81:            new_weights, actual_turnover = self._apply_turnover_cap(\nsrc/trend_analysis/rebalancing/strategies.py:86:        cost = self._calculate_cost(actual_turnover)\nsrc/trend_analysis/rebalancing/strategies.py:90:    def _apply_turnover_cap(\nsrc/trend_analysis/rebalancing/strategies.py:97:        \"\"\"Apply turnover cap with prioritized trade allocation.\"\"\"\nsrc/trend_analysis/rebalancing/strategies.py:108:        cumsum_turnover = np.cumsum(abs_trades)\nsrc/trend_analysis/rebalancing/strategies.py:109:        full_mask = cumsum_turnover <= (self.max_turnover + TURNOVER_EPSILON)\nsrc/trend_analysis/rebalancing/strategies.py:114:        remaining_turnover = max(0.0, self.max_turnover - abs_trades[full_mask].sum())\nsrc/trend_analysis/rebalancing/strategies.py:116:        if remaining_turnover > TURNOVER_EPSILON:\nsrc/trend_analysis/rebalancing/strategies.py:122:                    abs_trades[idx], remaining_turnover\nsrc/trend_analysis/rebalancing/strategies.py:124:                remaining_turnover = 0.0\nsrc/trend_analysis/rebalancing/strategies.py:135:        actual_turnover = executed_trades.abs().sum()\nsrc/trend_analysis/rebalancing/strategies.py:137:        return new_weights, actual_turnover\nsrc/trend_analysis/rebalancing/strategies.py:171:    def _calculate_cost(self, turnover: float) -> float:\nsrc/trend_analysis/rebalancing/strategies.py:172:        \"\"\"Calculate transaction cost based on turnover and cost basis\nsrc/trend_analysis/rebalancing/strategies.py:174:        return turnover * (self.cost_bps / 10000.0)\nscripts/compare_perf.py:12:  - turnover_vectorization.python_mean_s\nscripts/compare_perf.py:13:  - turnover_vectorization.vectorized_mean_s\nscripts/compare_perf.py:14:  - turnover_cap_vectorization.largest_gap.python_mean_s\nscripts/compare_perf.py:15:  - turnover_cap_vectorization.largest_gap.vectorized_mean_s\nscripts/compare_perf.py:16:  - turnover_cap_vectorization.best_score_delta.python_mean_s\nscripts/compare_perf.py:17:  - turnover_cap_vectorization.best_score_delta.vectorized_mean_s\nscripts/compare_perf.py:58:    \"turnover_vectorization.python_mean_s\",\nscripts/compare_perf.py:59:    \"turnover_vectorization.vectorized_mean_s\",\nscripts/compare_perf.py:60:    \"turnover_cap_vectorization.largest_gap.python_mean_s\",\nscripts/compare_perf.py:61:    \"turnover_cap_vectorization.largest_gap.vectorized_mean_s\",\nscripts/compare_perf.py:62:    \"turnover_cap_vectorization.best_score_delta.python_mean_s\",\nscripts/compare_perf.py:63:    \"turnover_cap_vectorization.best_score_delta.vectorized_mean_s\",\ntests/test_cli_trend_presets.py:34:            \"max_turnover\": 1.0,\nsrc/trend_analysis/metrics/__init__.py:445:turnover = import_module(\".turnover\", __name__)\ntests/test_pipeline_branch_coverage.py:36:    turnover = pd.Series([], dtype=float, name=\"turnover\")\ntests/test_pipeline_branch_coverage.py:41:        turnover=turnover,\ntests/test_pipeline_branch_coverage.py:42:        turnover_value=0.0,\ntests/test_quick_summary_report.py:30:            \"turnover\": {\ntests/test_quick_summary_report.py:91:    assert \"No turnover data available.\" in html\nsrc/trend_analysis/metrics/summary.py:10:from .turnover import realized_turnover, turnover_cost\nsrc/trend_analysis/metrics/summary.py:33:        Linear transaction cost applied to turnover when adjusting returns.\nsrc/trend_analysis/metrics/summary.py:38:    turn_df = realized_turnover(weights)\nsrc/trend_analysis/metrics/summary.py:39:    cost_series = turnover_cost(weights, transaction_cost_bps)\nsrc/trend_analysis/metrics/summary.py:47:    turn = float(turn_df[\"turnover\"].mean())\nsrc/trend_analysis/metrics/summary.py:57:        \"turnover\": float(turn),\nsrc/trend_analysis/metrics/turnover.py:3:This module provides helper functions for computing realized turnover and\nsrc/trend_analysis/metrics/turnover.py:30:def realized_turnover(\nsrc/trend_analysis/metrics/turnover.py:33:    \"\"\"Return a DataFrame of realized turnover per period.\"\"\"\nsrc/trend_analysis/metrics/turnover.py:35:    to = w_df.diff().abs().sum(axis=1).to_frame(\"turnover\")\nsrc/trend_analysis/metrics/turnover.py:39:def turnover_cost(\nsrc/trend_analysis/metrics/turnover.py:48:        Weight history used to compute turnover.\nsrc/trend_analysis/metrics/turnover.py:50:        Linear transaction cost in basis points applied to turnover.\nsrc/trend_analysis/metrics/turnover.py:52:    turn_df = realized_turnover(weights)\nsrc/trend_analysis/metrics/turnover.py:53:    return turn_df[\"turnover\"] * (cost_bps / 10000.0)\nsrc/trend_analysis/metrics/turnover.py:56:__all__ = [\"realized_turnover\", \"turnover_cost\"]\nsrc/trend_model/spec.py:50:    max_turnover: float | None\nsrc/trend_model/spec.py:222:    turnover_cap = _coerce_float(_section_get(portfolio, \"turnover_cap\"))\nsrc/trend_model/spec.py:223:    max_turnover_cfg = _coerce_float(_section_get(portfolio, \"max_turnover\"))\nsrc/trend_model/spec.py:224:    max_turnover_value = turnover_cap if turnover_cap is not None else max_turnover_cfg\nsrc/trend_model/spec.py:233:        max_turnover=max_turnover_value,\nsrc/trend_analysis/viz/__init__.py:13:    turnover_series,\nsrc/trend_analysis/viz/__init__.py:22:    \"turnover_series\",\nsrc/trend_analysis/risk.py:4:turnover enforcement so that both the CLI pipeline and the Streamlit app\nsrc/trend_analysis/risk.py:45:    turnover: pd.Series\nsrc/trend_analysis/risk.py:46:    turnover_value: float\nsrc/trend_analysis/risk.py:144:def _apply_turnover_penalty(\nsrc/trend_analysis/risk.py:170:def _enforce_turnover_cap(\nsrc/trend_analysis/risk.py:173:    max_turnover: float | None,\nsrc/trend_analysis/risk.py:177:        turnover = float(target.abs().sum())\nsrc/trend_analysis/risk.py:178:        return target, turnover\nsrc/trend_analysis/risk.py:184:    turnover = float(trades.abs().sum())\nsrc/trend_analysis/risk.py:185:    if max_turnover is None or max_turnover <= 0 or turnover <= max_turnover:\nsrc/trend_analysis/risk.py:186:        return target_aligned, turnover\nsrc/trend_analysis/risk.py:187:    scale = max_turnover / turnover if turnover > 0 else 0.0\nsrc/trend_analysis/risk.py:251:    max_turnover: float | None = None,\nsrc/trend_analysis/risk.py:286:    constrained = _apply_turnover_penalty(constrained, prev_series, lambda_tc)\nsrc/trend_analysis/risk.py:287:    constrained, turnover_value = _enforce_turnover_cap(\nsrc/trend_analysis/risk.py:288:        constrained, prev_series, max_turnover\nsrc/trend_analysis/risk.py:302:    turnover_index: Iterable[pd.Timestamp]\nsrc/trend_analysis/risk.py:304:        turnover_index = [returns.index[-1]]\nsrc/trend_analysis/risk.py:306:        turnover_index = [pd.Timestamp(\"1970-01-01\")]  # dummy timestamp\nsrc/trend_analysis/risk.py:308:    turnover_series = pd.Series(\nsrc/trend_analysis/risk.py:309:        [turnover_value],\nsrc/trend_analysis/risk.py:310:        index=pd.Index(turnover_index, name=\"rebalance\"),\nsrc/trend_analysis/risk.py:311:        name=\"turnover\",\nsrc/trend_analysis/risk.py:317:        turnover=turnover_series,\nsrc/trend_analysis/risk.py:318:        turnover_value=float(turnover_value),\nsrc/trend_analysis/walk_forward.py:232:def _compute_turnover(prev: pd.Series, new: pd.Series) -> float:\nsrc/trend_analysis/walk_forward.py:261:            turnover = _compute_turnover(prev_weights, weights)\nsrc/trend_analysis/walk_forward.py:287:                \"turnover\": turnover,\nsrc/trend_analysis/walk_forward.py:301:        \"turnover\": \"mean\",\nsrc/trend_analysis/walk_forward.py:315:            \"turnover\": \"mean_turnover\",\ntests/test_pipeline_run_analysis_helpers.py:278:        max_turnover=None,\ntests/test_pipeline_run_analysis_helpers.py:377:        max_turnover=None,\ntests/test_pipeline_run_analysis_helpers.py:465:            max_turnover=None,\ntests/test_pipeline_run_analysis_helpers.py:534:        max_turnover=None,\nsrc/trend_analysis/export/__init__.py:926:        turnover_value = risk_diag.get(\"turnover_value\")\nsrc/trend_analysis/export/__init__.py:927:        if isinstance(turnover_value, (float, int)) and not math.isnan(\nsrc/trend_analysis/export/__init__.py:928:            float(turnover_value)\nsrc/trend_analysis/export/__init__.py:930:            header.append(f\"Turnover applied: {float(turnover_value):.2%}\")\nsrc/trend_analysis/export/__init__.py:1192:      - Transaction Cost: linear cost computed from turnover and cost bps (if available)\nsrc/trend_analysis/export/__init__.py:1203:        turnover = cast(float | None, res.get(\"turnover\"))\nsrc/trend_analysis/export/__init__.py:1208:                \"Turnover\": float(turnover) if turnover is not None else float(\"nan\"),\nsrc/trend_analysis/export/__init__.py:1227:    \"\"\"Export execution metrics (turnover and transaction cost) separately.\ntests/test_multi_period_engine_branch_completion.py:77:            \"max_turnover\": 1.0,\ntests/test_multi_period_engine_branch_completion.py:108:def test_run_schedule_debug_turnover_validation(\ntests/test_multi_period_engine_branch_completion.py:111:    \"\"\"Enabling the debug env flag should execute the turnover validation block.\"\"\"\ntests/test_multi_period_engine_branch_completion.py:143:    assert portfolio.turnover[\"2020-01-31\"] >= 0.0\ntests/test_multi_period_engine_branch_completion.py:144:    assert portfolio.turnover[\"2020-02-29\"] >= 0.0\ntests/test_multi_period_engine_branch_completion.py:145:    assert calls, \"debug turnover validator should compare expected vs actual\"\ntests/test_multi_period_engine_branch_completion.py:200:    cfg.portfolio.setdefault(\"max_turnover\", 1.0)\nsrc/trend_analysis/export/bundle.py:145:                    turnover=pd.Series(dtype=float),\nsrc/trend_analysis/export/bundle.py:146:                    per_period_turnover=pd.Series(dtype=float),\ntests/test_trend_cli_diagnostics.py:18:def test_turnover_csv_diagnostic_for_missing_payload(tmp_path: Path):\ntests/test_trend_cli_diagnostics.py:19:    result = cli._maybe_write_turnover_csv(tmp_path, {})\ntests/test_trend_cli_diagnostics.py:26:def test_turnover_ledger_diagnostic_for_missing_payload():\ntests/test_trend_cli_diagnostics.py:27:    result = cli._persist_turnover_ledger(\"rid\", {})\ntests/test_config_models_additional.py:30:            \"max_turnover\": 1.0,\ntests/test_config_models_additional.py:221:    cfg[\"portfolio\"] = {\"rebalance_calendar\": \"NYSE\", \"max_turnover\": 0.5}\ntests/test_config_models_additional.py:232:        return DummyModel({\"portfolio\": {\"max_turnover\": 0.9}})\ntests/test_config_models_additional.py:239:    assert loaded.portfolio[\"max_turnover\"] == pytest.approx(0.9)\ntests/test_config_models_additional.py:275:            \"max_turnover\": 1.0,\ntests/test_viz_charts.py:161:        charts.turnover_series(empty_weights)\ntests/test_viz_charts.py:204:        to_result = charts.turnover_series(cast(dict, weights))\ntests/test_viz_charts.py:208:        assert list(to_result.columns) == [\"turnover\"]\nsrc/trend/cli.py:464:    ledger_result = _persist_turnover_ledger(run_id, getattr(result, \"details\", {}))\nsrc/trend/cli.py:591:    turnover_csv_result = _maybe_write_turnover_csv(\nsrc/trend/cli.py:594:    if turnover_csv_result.diagnostic:\nsrc/trend/cli.py:595:        logger.info(turnover_csv_result.diagnostic.message)\nsrc/trend/cli.py:639:def _maybe_write_turnover_csv(directory: Path, details: Any) -> DiagnosticResult[Path]:\nsrc/trend/cli.py:653:    turnover_obj = diag.get(\"turnover\")\nsrc/trend/cli.py:654:    if isinstance(turnover_obj, pd.Series):\nsrc/trend/cli.py:655:        series = turnover_obj.copy()\nsrc/trend/cli.py:656:    elif isinstance(turnover_obj, Mapping):\nsrc/trend/cli.py:657:        series = pd.Series(turnover_obj)\nsrc/trend/cli.py:658:    elif isinstance(turnover_obj, (list, tuple)):\nsrc/trend/cli.py:659:        series = pd.Series(turnover_obj)\nsrc/trend/cli.py:664:            context={\"turnover_type\": type(turnover_obj).__name__},\nsrc/trend/cli.py:672:            context={\"turnover_type\": type(turnover_obj).__name__},\nsrc/trend/cli.py:678:            context={\"turnover_type\": type(turnover_obj).__name__},\nsrc/trend/cli.py:681:    frame = series.rename(\"turnover\").to_frame()\nsrc/trend/cli.py:683:    path = directory / \"turnover.csv\"\nsrc/trend/cli.py:730:def _persist_turnover_ledger(run_id: str, details: Any) -> DiagnosticResult[Path]:\nsrc/trend/cli.py:734:            message=\"No turnover diagnostics captured for ledger persistence.\",\nsrc/trend/cli.py:741:            message=\"No turnover diagnostics captured for ledger persistence.\",\nsrc/trend/cli.py:744:    turnover_obj = diag.get(\"turnover\")\nsrc/trend/cli.py:745:    if turnover_obj is None:\nsrc/trend/cli.py:748:            message=\"No turnover diagnostics captured for ledger persistence.\",\nsrc/trend/cli.py:749:            context={\"turnover_type\": None},\nsrc/trend/cli.py:751:    if isinstance(turnover_obj, pd.Series):\nsrc/trend/cli.py:752:        if turnover_obj.empty:\nsrc/trend/cli.py:755:                message=\"No turnover diagnostics captured for ledger persistence.\",\nsrc/trend/cli.py:756:                context={\"turnover_type\": \"Series\"},\nsrc/trend/cli.py:758:    elif isinstance(turnover_obj, Mapping):\nsrc/trend/cli.py:759:        if not turnover_obj:\nsrc/trend/cli.py:762:                message=\"No turnover diagnostics captured for ledger persistence.\",\nsrc/trend/cli.py:763:                context={\"turnover_type\": \"Mapping\"},\nsrc/trend/cli.py:765:    elif isinstance(turnover_obj, (list, tuple)):\nsrc/trend/cli.py:766:        if not turnover_obj:\nsrc/trend/cli.py:769:                message=\"No turnover diagnostics captured for ledger persistence.\",\nsrc/trend/cli.py:770:                context={\"turnover_type\": \"Sequence\"},\nsrc/trend/cli.py:775:            message=\"No turnover diagnostics captured for ledger persistence.\",\nsrc/trend/cli.py:776:            context={\"turnover_type\": type(turnover_obj).__name__},\nsrc/trend/cli.py:780:    path_result = _maybe_write_turnover_csv(target_dir, details)\nsrc/trend_analysis/viz/charts.py:86:def turnover_series(\nsrc/trend_analysis/viz/charts.py:89:    \"\"\"Compute turnover figure and DataFrame from weights history.\"\"\"\nsrc/trend_analysis/viz/charts.py:92:    to = w_df.diff().abs().sum(axis=1).to_frame(\"turnover\")\nsrc/trend/reporting/quick_summary.py:80:def _extract_turnover(details: Mapping[str, Any]) -> pd.Series:\nsrc/trend/reporting/quick_summary.py:83:        series = _coerce_series(diagnostics.get(\"turnover\"))\nsrc/trend/reporting/quick_summary.py:166:def _turnover_chart(turnover: pd.Series) -> DiagnosticResult[str]:\nsrc/trend/reporting/quick_summary.py:167:    if turnover.empty:\nsrc/trend/reporting/quick_summary.py:171:            context={\"length\": len(turnover)},\nsrc/trend/reporting/quick_summary.py:174:    ax.plot(turnover.index, turnover.values, color=\"#ff7f0e\", linewidth=1.5)\nsrc/trend/reporting/quick_summary.py:248:    turnover_chart: str | None,\nsrc/trend/reporting/quick_summary.py:250:    turnover_diagnostic: DiagnosticPayload | None,\nsrc/trend/reporting/quick_summary.py:275:    turnover_block = (\nsrc/trend/reporting/quick_summary.py:276:        f'<img alt=\"Turnover\" src=\"data:image/png;base64,{turnover_chart}\">'\nsrc/trend/reporting/quick_summary.py:277:        if turnover_chart\nsrc/trend/reporting/quick_summary.py:278:        else '<p class=\"placeholder\">No turnover data available.</p>'\nsrc/trend/reporting/quick_summary.py:280:    if turnover_diagnostic is not None:\nsrc/trend/reporting/quick_summary.py:281:        turnover_block = f'{turnover_block}<p class=\"placeholder\">{html_escape(turnover_diagnostic.message)}</p>'\nsrc/trend/reporting/quick_summary.py:327:      {turnover_block}\nsrc/trend/reporting/quick_summary.py:364:    turnover = _extract_turnover(details)\nsrc/trend/reporting/quick_summary.py:375:    turnover_chart_result = _turnover_chart(turnover)\nsrc/trend/reporting/quick_summary.py:384:        turnover_chart=turnover_chart_result.value,\nsrc/trend/reporting/quick_summary.py:386:        turnover_diagnostic=turnover_chart_result.diagnostic,\nsrc/trend/reporting/unified.py:207:    turnover_series = None\nsrc/trend/reporting/unified.py:213:        turnover_series = _maybe_series(risk_diag.get(\"turnover\"))\nsrc/trend/reporting/unified.py:215:    if turnover_series is None:\nsrc/trend/reporting/unified.py:216:        turnover_series = pd.Series(dtype=float)\nsrc/trend/reporting/unified.py:247:        \"turnover_mean\": (\nsrc/trend/reporting/unified.py:248:            float(turnover_series.mean()) if not turnover_series.empty else 0.0\nsrc/trend/reporting/unified.py:269:            turnover=turnover_series.sort_index(),\nsrc/trend/reporting/unified.py:270:            per_period_turnover=pd.Series(dtype=float),\nsrc/trend/reporting/unified.py:486:    max_turnover = _get(portfolio, \"max_turnover\")\nsrc/trend/reporting/unified.py:487:    if isinstance(max_turnover, (int, float)):\nsrc/trend/reporting/unified.py:488:        params.append((\"Turnover cap\", _format_percent(float(max_turnover))))\nsrc/trend/reporting/unified.py:552:def _turnover_chart(backtest: BacktestResult | None) -> DiagnosticResult[str]:\nsrc/trend/reporting/unified.py:553:    if backtest is None or backtest.turnover.empty:\nsrc/trend/reporting/unified.py:559:    series = backtest.turnover.sort_index()\nsrc/trend/reporting/unified.py:688:    turnover = metrics.get(\"turnover_mean\", 0.0)\nsrc/trend/reporting/unified.py:695:        f\"{_format_percent(metrics.get('max_drawdown'))}, and mean turnover per rebalance was \"\nsrc/trend/reporting/unified.py:696:        f\"{_format_percent(turnover)}.{alloc_sentence}\"\nsrc/trend/reporting/unified.py:732:    turnover_img = (\nsrc/trend/reporting/unified.py:733:        f'<img src=\"data:image/png;base64,{context[\"turnover_chart\"]}\" alt=\"Turnover chart\" />'\nsrc/trend/reporting/unified.py:734:        if context[\"turnover_chart\"]\nsrc/trend/reporting/unified.py:735:        else \"<p>No turnover history available.</p>\"\nsrc/trend/reporting/unified.py:784:        f\"    {turnover_img}</div>\\n\"\nsrc/trend/reporting/unified.py:947:    if context[\"turnover_chart\"]:\nsrc/trend/reporting/unified.py:951:        chart_bytes = base64.b64decode(context[\"turnover_chart\"])\nsrc/trend/reporting/unified.py:954:        if not context[\"turnover_chart\"]:\nsrc/trend/reporting/unified.py:1021:    turnover_chart_result = _turnover_chart(backtest)\nsrc/trend/reporting/unified.py:1022:    if turnover_chart_result.diagnostic:\nsrc/trend/reporting/unified.py:1023:        diagnostics.append(turnover_chart_result.diagnostic)\nsrc/trend/reporting/unified.py:1024:        caveats.append(turnover_chart_result.diagnostic.message)\nsrc/trend/reporting/unified.py:1025:    turnover_chart = turnover_chart_result.value\nsrc/trend/reporting/unified.py:1044:        \"turnover_chart\": turnover_chart,\ntests/test_pipeline_run_cache_fallbacks.py:465:            turnover=pd.Series(dtype=float, name=\"turnover\"),\ntests/test_pipeline_run_cache_fallbacks.py:466:            turnover_value=0.0,\ntests/test_pipeline_run_cache_fallbacks.py:492:        max_turnover=\"oops\",\ntests/test_pipeline_run_cache_fallbacks.py:538:            turnover=pd.Series(dtype=float, name=\"turnover\"),\ntests/test_pipeline_run_cache_fallbacks.py:539:            turnover_value=0.0,\ntests/test_trend_config_model_negative_paths.py:206:            \"max_turnover\": 0.5,\ntests/test_trend_config_model_negative_paths.py:218:def test_portfolio_settings_enforce_turnover_bounds():\ntests/test_trend_config_model_negative_paths.py:223:                \"max_turnover\": 1.5,\ntests/test_trend_config_model_negative_paths.py:228:    assert \"portfolio.max_turnover\" in str(exc.value)\ntests/test_trend_config_model_negative_paths.py:236:                \"max_turnover\": 0.5,\ntests/test_trend_config_model_negative_paths.py:264:            \"max_turnover\": 0.2,\ntests/test_spec_loader.py:174:            \"max_turnover\": 0.5,\ntests/test_multi_period_engine_helpers_additional.py:36:def test_compute_turnover_state_handles_fresh_and_existing_weights() -> None:\ntests/test_multi_period_engine_helpers_additional.py:41:    first_turnover, idx, vals = mp_engine._compute_turnover_state(\ntests/test_multi_period_engine_helpers_additional.py:47:    assert first_turnover == pytest.approx(float(np.abs(first_series).sum()))\ntests/test_multi_period_engine_helpers_additional.py:53:    turnover, next_idx, next_vals = mp_engine._compute_turnover_state(\ntests/test_multi_period_engine_helpers_additional.py:70:    assert turnover == pytest.approx(expected)\ntests/test_multi_period_engine_helpers_additional.py:81:    pf.rebalance(\"2021-01-31\", weights_df, turnover=0.1, cost=0.05)\ntests/test_multi_period_engine_helpers_additional.py:86:    assert pf.turnover[\"2021-01-31\"] == pytest.approx(0.1)\ntests/test_multi_period_engine_helpers_additional.py:98:    pf.rebalance(\"2021-03-31\", march_series, turnover=0.12)\ntests/test_multi_period_engine_helpers_additional.py:102:    assert pf.turnover[march_key] == pytest.approx(0.12)\ntests/test_multi_period_engine_helpers_additional.py:108:def test_run_schedule_invokes_update_and_fast_turnover(\ntests/test_multi_period_engine_helpers_additional.py:111:    \"\"\"``run_schedule`` should exercise fast turnover and update hooks.\"\"\"\ntests/test_cli.py:60:            \"max_turnover\": 0.25,\ntests/test_config_validation.py:46:            \"max_turnover\": 1.0,\ntests/test_multi_period_rebalancer.py:173:                \"max_turnover\": 0.5,\ntests/backtesting/test_harness.py:47:    \"\"\"Alternate between two assets to force turnover for testing costs.\"\"\"\ntests/backtesting/test_harness.py:134:    assert \"turnover\" in summary\ntests/backtesting/test_harness.py:152:def test_transaction_costs_applied_to_turnover() -> None:\ntests/backtesting/test_harness.py:176:    assert result.turnover.iloc[0] == 1.0\ntests/backtesting/test_harness.py:177:    assert result.turnover.iloc[1] == 2.0\ntests/backtesting/test_harness.py:229:    expected_costs = taxed.turnover * (\ntests/backtesting/test_harness.py:243:def test_turnover_cap_and_cost_drag_series() -> None:\ntests/backtesting/test_harness.py:266:        turnover_cap=0.8,\ntests/backtesting/test_harness.py:274:    expected_turnover = pd.Series([0.8, 0.4], index=result.turnover.index)\ntests/backtesting/test_harness.py:275:    pdt.assert_series_equal(result.turnover, expected_turnover)\ntests/backtesting/test_harness.py:277:    expected_costs = expected_turnover * cost_multiplier\ntests/backtesting/test_harness.py:358:    assert len(baseline.turnover) >= 2\ntests/backtesting/test_harness.py:359:    second_rebalance = baseline.turnover.index[1]\ntests/backtesting/test_harness.py:360:    assert baseline.turnover.loc[second_rebalance] > 0\ntests/backtesting/test_harness.py:361:    assert banded.turnover.loc[second_rebalance] == pytest.approx(0.0)\ntests/backtesting/test_harness.py:370:    non_zero_periods = banded.per_period_turnover[banded.per_period_turnover > 0]\ntests/backtesting/test_harness.py:373:        banded.turnover.loc[baseline.turnover.index[0]]\ntests/backtesting/test_harness.py:740:        turnover=pd.Series([0.0, 1.0], index=idx),\ntests/backtesting/test_harness.py:741:        per_period_turnover=pd.Series([0.0, 1.0], index=idx),\ntests/backtesting/test_harness.py:765:        # Keep the strategy deterministic so the turnover expectation is stable.\ntests/backtesting/test_harness.py:795:    # Initial weights of 100% in ``A`` should result in an initial turnover of 0.8\ntests/backtesting/test_harness.py:797:    first_turnover = result.turnover.iloc[0]\ntests/backtesting/test_harness.py:798:    assert math.isclose(first_turnover, 0.8, rel_tol=1e-9)\ntests/test_constants.py:72:                \"portfolio: {selection_mode: all, rebalance_calendar: NYSE, max_turnover: 0.5, transaction_cost_bps: 10}\",\ntests/test_config_model_branch_coverage.py:24:        \"max_turnover\": 0.5,\ntests/test_config_model_branch_coverage.py:83:def test_portfolio_turnover_bounds() -> None:\ntests/test_config_model_branch_coverage.py:85:    base[\"max_turnover\"] = -0.1\ntests/test_config_model_branch_coverage.py:88:    base[\"max_turnover\"] = 1.5\ntests/backtesting/test_bootstrap.py:20:        turnover=pd.Series(dtype=float),\ntests/backtesting/test_bootstrap.py:21:        per_period_turnover=pd.Series(dtype=float),\ntests/test_multi_period_engine_branch_coverage.py:39:            \"max_turnover\": 1.0,\ntests/test_multi_period_engine_branch_coverage.py:126:            \"max_turnover\": 1.0,\ntests/test_multi_period_exits_cooldown.py:108:def test_threshold_hold_exit_drop_not_blocked_by_turnover_budget(\ntests/test_multi_period_exits_cooldown.py:112:    cfg.portfolio[\"turnover_budget_max_changes\"] = 1\ntests/test_multi_period_exits_cooldown.py:146:        ev.get(\"reason\") == \"turnover_budget\" and ev.get(\"action\") == \"skipped\"\ntests/test_multi_period_exits_cooldown.py:158:    cfg.portfolio[\"turnover_budget_max_changes\"] = 10\ntests/test_multi_period_exits_cooldown.py:229:        cfg.portfolio[\"turnover_budget_max_changes\"] = 10\ntests/test_multi_period_exits_cooldown.py:245:    cfg.portfolio[\"turnover_budget_max_changes\"] = 10\ntests/test_multi_period_exits_cooldown.py:347:def test_min_funds_can_exceed_turnover_budget(monkeypatch: pytest.MonkeyPatch) -> None:\ntests/test_multi_period_exits_cooldown.py:349:    cfg.portfolio[\"turnover_budget_max_changes\"] = 1\ntests/test_trend_cli_entrypoints.py:25:        turnover_idx = pd.date_range(\"2020-01-31\", periods=2, freq=\"ME\")\ntests/test_trend_cli_entrypoints.py:27:            \"turnover\": pd.Series([0.1, 0.2], index=turnover_idx),\ntests/test_trend_cli_entrypoints.py:28:            \"turnover_value\": 0.3,\ntests/test_trend_cli_entrypoints.py:229:    ledger = Path(\"perf\") / run_id / \"turnover.csv\"\ntests/test_trend_cli_entrypoints.py:232:    assert df[\"turnover\"].sum() == pytest.approx(0.3)\ntests/test_trend_cli_entrypoints.py:320:    turnover_path = tmp_path / \"turnover.csv\"\ntests/test_trend_cli_entrypoints.py:322:    assert turnover_path.exists()\ntests/test_trend_cli_entrypoints.py:325:    turnover_df = pd.read_csv(turnover_path)\ntests/test_trend_cli_entrypoints.py:326:    assert turnover_df[\"turnover\"].sum() == pytest.approx(0.3)\ntests/test_run_analysis_cli_default.py:18:                \"portfolio: {selection_mode: all, rebalance_calendar: NYSE, max_turnover: 0.5, transaction_cost_bps: 10}\",\ntests/test_rebalancing_strategies.py:22:rebalancer_registry.register(\"turnover_cap\")(strat_mod.TurnoverCapStrategy)\ntests/test_rebalancing_strategies.py:30:def test_turnover_cap_executes_within_limit():\ntests/test_rebalancing_strategies.py:33:    strat = TurnoverCapStrategy({\"max_turnover\": 0.3, \"cost_bps\": 0})\ntests/test_rebalancing_strategies.py:39:def test_turnover_cap_respects_limit_and_cost():\ntests/test_rebalancing_strategies.py:43:        {\"max_turnover\": 0.2, \"cost_bps\": 10, \"priority\": \"largest_gap\"}\ntests/test_rebalancing_strategies.py:51:def test_turnover_cap_best_score_priority():\ntests/test_rebalancing_strategies.py:78:    assert mapping[\"turnover_cap\"] is strat_mod.TurnoverCapStrategy\ntests/test_config_models_coverage_extensions.py:141:    with pytest.raises(ValueError, match=\"max_turnover must be >= 0\"):\ntests/test_config_models_coverage_extensions.py:142:        validator(models._PydanticConfigImpl, {\"max_turnover\": \"-0.5\"})  # type: ignore[attr-defined]\ntests/test_config_models_coverage_extensions.py:144:    with pytest.raises(ValueError, match=\"max_turnover must be <= 2.0\"):\ntests/test_config_models_coverage_extensions.py:145:        validator(models._PydanticConfigImpl, {\"max_turnover\": \"2.5\"})  # type: ignore[attr-defined]\ntests/test_config_models_coverage_extensions.py:155:        {\"transaction_cost_bps\": \"15\", \"max_turnover\": \"1.5\", \"lambda_tc\": \"0.3\"},\ntests/test_config_models_coverage_extensions.py:158:    assert validated[\"max_turnover\"] == pytest.approx(1.5)\ntests/test_config_models_coverage_extensions.py:288:    with pytest.raises(ValueError, match=\"max_turnover must be <= 2.0\"):\ntests/test_config_models_coverage_extensions.py:289:        fallback.Config(**{**kwargs, \"portfolio\": {\"max_turnover\": 3}})\ntests/test_pipeline_integration_direct.py:58:            \"max_turnover\": 0.5,\ntests/test_multi_period_sticky_periods.py:35:            \"max_turnover\": 1.0,\ntests/test_multi_period_sticky_periods.py:142:def _turnover_after_seed(results: list[dict[str, Any]]) -> float:\ntests/test_multi_period_sticky_periods.py:143:    return float(sum(float(r.get(\"turnover\", 0.0)) for r in results[1:]))\ntests/test_multi_period_sticky_periods.py:160:    assert _turnover_after_seed(delayed) < _turnover_after_seed(baseline)\ntests/test_multi_period_sticky_periods.py:177:    assert _turnover_after_seed(delayed) < _turnover_after_seed(baseline)\ntests/test_multi_period_sticky_periods.py:180:def test_sticky_add_and_drop_reduce_turnover(\ntests/test_multi_period_sticky_periods.py:194:    assert _turnover_after_seed(delayed) < _turnover_after_seed(baseline)\ntests/test_multi_period_engine_extended.py:76:def test_compute_turnover_state_tracks_union_alignment() -> None:\ntests/test_multi_period_engine_extended.py:81:    turnover, next_idx, next_vals = engine._compute_turnover_state(\ntests/test_multi_period_engine_extended.py:86:    expected_turnover = float(\ntests/test_multi_period_engine_extended.py:93:    assert turnover == pytest.approx(expected_turnover)\ntests/test_multi_period_engine_extended.py:154:    pf.rebalance(\"2021-03-31\", weights, turnover=0.12, cost=0.005)\ntests/test_multi_period_engine_extended.py:160:    assert pf.turnover[\"2021-03-31\"] == pytest.approx(0.12)\ntests/test_multi_period_engine_turnover_extended.py:1:\"\"\"Additional coverage tests for the multi-period engine turnover helpers.\"\"\"\ntests/test_multi_period_engine_turnover_extended.py:16:def test_compute_turnover_state_union_alignment() -> None:\ntests/test_multi_period_engine_turnover_extended.py:17:    \"\"\"Ensure the vectorised turnover helper handles index unions correctly.\"\"\"\ntests/test_multi_period_engine_turnover_extended.py:23:    turnover, next_idx, next_vals = mp_engine._compute_turnover_state(\ntests/test_multi_period_engine_turnover_extended.py:33:    expected_turnover = abs(0.6) + abs(0.4 - 0.8) + abs(0.2)\ntests/test_multi_period_engine_turnover_extended.py:34:    assert turnover == pytest.approx(expected_turnover)\ntests/test_multi_period_engine_turnover_extended.py:58:def test_run_schedule_fast_turnover_tracks_union(\ntests/test_multi_period_engine_turnover_extended.py:61:    \"\"\"Exercise the specialised fast-turnover path when holdings change.\"\"\"\ntests/test_multi_period_engine_turnover_extended.py:110:    # First period turnover equals the sum of absolute weights.\ntests/test_multi_period_engine_turnover_extended.py:111:    assert portfolio.turnover[\"2020-01-31\"] == pytest.approx(\ntests/test_multi_period_engine_turnover_extended.py:124:    assert portfolio.turnover[\"2020-02-29\"] == pytest.approx(expected)\ntests/test_config_models_fallback_loader.py:83:        \"portfolio\": {\"max_turnover\": 0.1},\ntests/test_config_turnover_validation.py:40:def test_max_turnover_valid(cap):\ntests/test_config_turnover_validation.py:41:    cfg_dict = make_cfg({\"portfolio\": {\"max_turnover\": cap}})\ntests/test_config_turnover_validation.py:43:    assert float(cfg.portfolio.get(\"max_turnover\")) == float(cap)\ntests/test_config_turnover_validation.py:47:def test_max_turnover_invalid(cap):\ntests/test_config_turnover_validation.py:48:    cfg_dict = make_cfg({\"portfolio\": {\"max_turnover\": cap}})\ntests/test_config_turnover_validation.py:55:        {\"portfolio\": {\"transaction_cost_bps\": \"15\", \"max_turnover\": \"0.75\"}}\ntests/test_config_turnover_validation.py:59:    assert cfg.portfolio[\"max_turnover\"] == 0.75\ntests/test_analysis_results_module.py:115:            \"turnover\": pd.Series([0.1, 0.2]),\ntests/test_analysis_results_module.py:116:            \"turnover_value\": 0.15,\ntests/test_analysis_results_module.py:128:    assert not results.turnover.empty\ntests/trend_analysis/test_backtesting_harness.py:38:    turnover = pd.Series([0.1, 0.05, 0.0, np.nan], index=sample_calendar)\ntests/trend_analysis/test_backtesting_harness.py:63:        turnover=turnover,\ntests/trend_analysis/test_backtesting_harness.py:64:        per_period_turnover=turnover,\ntests/trend_analysis/test_backtesting_harness.py:87:    assert summary[\"turnover\"][\"2021-02-28T00:00:00\"] == pytest.approx(0.05)\ntests/trend_analysis/test_backtesting_harness.py:96:    assert parsed[\"turnover\"][\"2021-03-31T00:00:00\"] == pytest.approx(0.0)\ntests/trend_analysis/test_backtesting_harness.py:127:    assert not result.turnover.empty\ntests/trend_analysis/test_backtesting_harness.py:128:    assert result.transaction_costs.index.equals(result.turnover.index)\ntests/trend_analysis/test_backtesting_harness.py:591:    assert result.turnover.loc[pd.Timestamp(\"2021-01-01\")] == pytest.approx(1.0)\ntests/test_risk_free_default_alignment.py:235:                \"max_turnover\": 0.5,\ntests/test_pipeline.py:41:            \"max_turnover\": 0.5,\ntests/test_pipeline.py:290:    assert \"turnover_value\" in diag\ntests/test_config_models.py:29:            \"max_turnover\": 1.0,\ntests/test_default_export.py:18:                \"portfolio: {selection_mode: all, rebalance_calendar: NYSE, max_turnover: 0.5, transaction_cost_bps: 10}\",\ntests/test_timeseries_properties.py:119:    turnover = positions.diff().abs().fillna(0.0)\ntests/test_timeseries_properties.py:120:    transaction_costs = turnover * 0.0005\ntests/test_timeseries_properties.py:128:    assert np.isfinite(turnover.to_numpy()).all()\ntests/test_trend_cli.py:26:    turnover = pd.Series(\ntests/test_trend_cli.py:48:            \"turnover\": turnover,\ntests/scripts/test_evaluate_settings_effectiveness.py:82:                \"recommendation\": \"Ensure transaction costs are applied to turnover.\",\ntests/test_cli_no_structured_log.py:42:            \"max_turnover\": 0.25,\ntests/test_config_fallback_minimal.py:17:    \"max_turnover\": 1.0,\ntests/test_config_models_fallback_unit.py:101:                \"max_turnover\": \"1.25\",\ntests/test_config_models_fallback_unit.py:106:    assert cfg.portfolio[\"max_turnover\"] == pytest.approx(1.25)\ntests/test_config_models_fallback_unit.py:122:    with pytest.raises(ValueError, match=\"max_turnover must be <= 2.0\"):\ntests/test_config_models_fallback_unit.py:123:        Config(**_base_config_payload(portfolio={\"max_turnover\": 3.0}))\ntests/test_multi_period_engine_turnover_regression.py:1:\"\"\"Regression tests for turnover and weight bound handling.\ntests/test_multi_period_engine_turnover_regression.py:4:and turnover bookkeeping for the multi-period engine helpers.\ntests/test_multi_period_engine_turnover_regression.py:45:def test_turnover_penalty_respects_bounds_and_normalisation() -> None:\ntests/test_multi_period_engine_turnover_regression.py:46:    \"\"\"Penalised turnover must stay within bounds and remain normalised.\"\"\"\ntests/test_multi_period_engine_turnover_regression.py:51:    adjusted = mp_engine._apply_turnover_penalty(\ntests/test_multi_period_engine_turnover_regression.py:83:def test_fast_turnover_matches_recomputed_history(\ntests/test_multi_period_engine_turnover_regression.py:86:    \"\"\"Fast turnover path should agree with recomputed turnover history.\"\"\"\ntests/test_multi_period_engine_turnover_regression.py:152:        assert portfolio.turnover[date_key] == pytest.approx(expected)\ntests/test_multi_period_engine_turnover_regression.py:155:def _turnover_series(results: list[dict[str, object]]) -> list[float]:\ntests/test_multi_period_engine_turnover_regression.py:160:            turnover = risk_diag.get(\"turnover\")\ntests/test_multi_period_engine_turnover_regression.py:162:            turnover = None\ntests/test_multi_period_engine_turnover_regression.py:163:        if isinstance(turnover, pd.Series) and not turnover.empty:\ntests/test_multi_period_engine_turnover_regression.py:164:            values.append(float(turnover.iloc[-1]))\ntests/test_multi_period_engine_turnover_regression.py:165:        elif isinstance(turnover, (int, float)):\ntests/test_multi_period_engine_turnover_regression.py:166:            values.append(float(turnover))\ntests/test_multi_period_engine_turnover_regression.py:170:def _turnover_config(max_turnover: float) -> Config:\ntests/test_multi_period_engine_turnover_regression.py:189:    portfolio_cfg[\"max_turnover\"] = max_turnover\ntests/test_multi_period_engine_turnover_regression.py:193:def test_multi_period_turnover_cap_reduces_rebalance_size() -> None:\ntests/test_multi_period_engine_turnover_regression.py:203:    high_cap_cfg = _turnover_config(1.0)\ntests/test_multi_period_engine_turnover_regression.py:204:    low_cap_cfg = _turnover_config(0.3)\ntests/test_multi_period_engine_turnover_regression.py:209:    high_turnover = _turnover_series(high_results)\ntests/test_multi_period_engine_turnover_regression.py:210:    low_turnover = _turnover_series(low_results)\ntests/test_multi_period_engine_turnover_regression.py:212:    assert len(high_turnover) == len(low_turnover) >= 2\ntests/test_multi_period_engine_turnover_regression.py:213:    assert low_turnover[1] <= 0.3 + 1e-12\ntests/test_multi_period_engine_turnover_regression.py:214:    assert low_turnover[1] < high_turnover[1]\ntests/test_trend_reporting_unified_helpers.py:82:    turnover = pd.Series(\ntests/test_trend_reporting_unified_helpers.py:92:        \"risk_diagnostics\": {\"turnover\": turnover, \"final_weights\": final_weights},\ntests/test_trend_reporting_unified_helpers.py:115:            \"max_turnover\": 0.2,\ntests/test_trend_reporting_unified_helpers.py:175:    turnover_chart = unified._turnover_chart(backtest).value\ntests/test_trend_reporting_unified_helpers.py:177:    assert turnover_chart and exposure_chart\ntests/test_trend_reporting_unified_helpers.py:192:        \"turnover_chart\": turnover_chart,\ntests/test_trend_reporting_unified_helpers.py:284:        \"risk_diagnostics\": {\"turnover\": [0.05, 0.02]},\ntests/test_trend_reporting_unified_helpers.py:293:    assert backtest.turnover.empty is False\ntests/test_trend_reporting_unified_helpers.py:313:    assert backtest.turnover.empty\ntests/test_trend_reporting_unified_helpers.py:370:        portfolio={\"max_turnover\": 0.1, \"rebalance_calendar\": \"NYSE\"},\ntests/test_trend_reporting_unified_helpers.py:436:def _pdf_context(turnover: str = \"\", exposure: str = \"\") -> Mapping[str, Any]:\ntests/test_trend_reporting_unified_helpers.py:448:        \"turnover_chart\": turnover,\ntests/test_trend_reporting_unified_helpers.py:481:def test_render_pdf_handles_bytearray_and_missing_turnover(\ntests/test_trend_reporting_unified_helpers.py:575:    turnover_result = unified._turnover_chart(None)\ntests/test_trend_reporting_unified_helpers.py:576:    assert turnover_result.value is None\ntests/test_trend_reporting_unified_helpers.py:577:    assert turnover_result.diagnostic is not None\ntests/test_trend_reporting_unified_helpers.py:579:        turnover=pd.Series(dtype=float), weights=pd.DataFrame()\ntests/test_trend_reporting_unified_helpers.py:581:    empty_turnover = unified._turnover_chart(dummy_backtest)\ntests/test_trend_reporting_unified_helpers.py:582:    assert empty_turnover.value is None\ntests/test_trend_reporting_unified_helpers.py:583:    assert empty_turnover.diagnostic is not None\ntests/test_trend_reporting_unified_helpers.py:613:            \"turnover_mean\": 0.0,\ntests/test_trend_reporting_unified_helpers.py:635:                \"turnover_chart\": \"\",\ntests/test_rebalancing_integration.py:11:def test_run_schedule_with_turnover_cap():\ntests/test_rebalancing_integration.py:12:    \"\"\"Test run_schedule integration with turnover_cap strategy.\"\"\"\ntests/test_rebalancing_integration.py:28:    # Test with turnover cap strategy\ntests/test_rebalancing_integration.py:29:    rebalance_strategies = [\"turnover_cap\"]\ntests/test_rebalancing_integration.py:31:        \"turnover_cap\": {\"max_turnover\": 0.3, \"cost_bps\": 20, \"priority\": \"largest_gap\"}\ntests/test_rebalancing_integration.py:46:    # Check that turnover constraint was respected\ntests/test_rebalancing_integration.py:47:    # (detailed turnover checking would require more complex setup)\ntests/test_rebalancing_integration.py:71:    rebalance_strategies = [\"drift_band\", \"turnover_cap\"]\ntests/test_rebalancing_integration.py:74:        \"turnover_cap\": {\"max_turnover\": 0.5, \"cost_bps\": 10},\ntests/test_risk_additional.py:75:        max_turnover=0.4,\ntests/test_risk_additional.py:81:    assert diagnostics.turnover_value <= 0.4 + 1e-9\ntests/test_risk_additional.py:82:    assert diagnostics.turnover.index[-1] == returns.index[-1]\ntests/test_risk_additional.py:114:        max_turnover=None,\ntests/test_risk_additional.py:119:    assert diagnostics.turnover.index.tolist() == [pd.Timestamp(\"1970-01-01\")]\ntests/test_run_multi_analysis_cli.py:18:                \"portfolio: {selection_mode: all, rebalance_calendar: NYSE, max_turnover: 0.5, transaction_cost_bps: 10}\",\ntests/test_multi_period_engine_additional.py:13:def test_compute_turnover_state_with_previous_allocation() -> None:\ntests/test_multi_period_engine_additional.py:18:    turnover, next_idx, next_vals = engine._compute_turnover_state(\ntests/test_multi_period_engine_additional.py:22:    assert pytest.approx(turnover) == 0.8\ntests/test_multi_period_engine_additional.py:81:    assert pytest.approx(portfolio.turnover[\"2020-01-31\"]) == 1.0\ntests/test_multi_period_engine_additional.py:444:                    \"max_turnover\": 0.4,\ntests/test_multi_period_engine_additional.py:593:    turnover_cap = cfg.portfolio[\"max_turnover\"]\ntests/test_multi_period_engine_additional.py:594:    assert results[1][\"turnover\"] <= turnover_cap + 1e-9\ntests/test_multi_period_engine_additional.py:595:    expected_cost = results[1][\"turnover\"] * (\ntests/test_multi_period_engine_additional.py:620:                    \"max_turnover\": 1.0,\ntests/test_multi_period_engine_additional.py:746:                    \"max_turnover\": 1.0,\ntests/test_multi_period_engine_additional.py:883:                    \"max_turnover\": 1.0,\ntests/test_multi_period_engine_additional.py:1142:def test_threshold_hold_replacements_and_turnover_cap(\ntests/test_multi_period_engine_additional.py:1154:            \"max_turnover\": 0.1,\ntests/test_invariants.py:56:    max_turnover = 0.35\ntests/test_invariants.py:68:        max_turnover=max_turnover,\ntests/test_invariants.py:73:    assert diagnostics.turnover_value <= max_turnover + 1e-12\ntests/test_invariants.py:74:    assert diagnostics.turnover.index.name == \"rebalance\"\ntests/test_invariants.py:238:    for ts, turnover in costly.per_period_turnover[\ntests/test_invariants.py:239:        costly.per_period_turnover > 0\ntests/test_invariants.py:241:        expected.loc[ts] -= turnover * tx_per_unit\ntests/test_trend_config_model_additional.py:203:                \"max_turnover\": 0.5,\ntests/test_trend_config_model_additional.py:212:                \"max_turnover\": -0.1,\ntests/test_trend_config_model_additional.py:221:                \"max_turnover\": 1.5,\ntests/test_trend_config_model_additional.py:230:                \"max_turnover\": 0.5,\ntests/test_trend_config_model_additional.py:240:            \"max_turnover\": 1.0,\ntests/test_trend_config_model_additional.py:299:            \"max_turnover\": 0.5,\ntests/test_multi_period_engine_debug.py:54:def test_run_schedule_turnover_debug_validation(\ntests/test_multi_period_engine_debug.py:72:        # Ensure the debug validator populated history and turnover for each period.\ntests/test_multi_period_engine_debug.py:75:        assert set(portfolio.turnover) == {\"2020-01-31\", \"2020-02-29\"}\ntests/test_multi_period_engine_debug.py:76:        # The second period should see turnover from introducing FundC while FundA is removed.\ntests/test_multi_period_engine_debug.py:77:        assert portfolio.turnover[\"2020-02-29\"] > 0.0\ntests/test_walk_forward_helpers_additional.py:114:def test_compute_turnover_handles_union_of_indices() -> None:\ntests/test_walk_forward_helpers_additional.py:118:    turnover = wf._compute_turnover(prev, new)\ntests/test_walk_forward_helpers_additional.py:120:    assert turnover == pytest.approx(1.0)\ntests/test_run_analysis_cli_export.py:18:                \"portfolio: {selection_mode: all, rebalance_calendar: NYSE, max_turnover: 0.5, transaction_cost_bps: 10}\",\ntests/test_multi_period_engine_portfolio_unit.py:34:    portfolio.rebalance(\"2020-01-31\", series, turnover=0.3, cost=1.5)\ntests/test_multi_period_engine_portfolio_unit.py:37:    assert portfolio.turnover[\"2020-01-31\"] == 0.3\ntests/test_multi_period_engine_portfolio_unit.py:54:    portfolio.rebalance(\"2020-03-31\", weights, turnover=0.25, cost=2.0)\ntests/test_multi_period_engine_portfolio_unit.py:61:    assert portfolio.turnover[\"2020-03-31\"] == 0.25\ntests/test_multi_period_engine_portfolio_unit.py:79:    assert portfolio.turnover[\"2020-04-30\"] == 0.0\ntests/test_config_alignment.py:102:            \"max_turnover\": 0.25,\ntests/test_quick_summary_diagnostics.py:14:def test_turnover_chart_reports_diagnostic_for_empty_series():\ntests/test_quick_summary_diagnostics.py:15:    result = quick_summary._turnover_chart(pd.Series(dtype=float))\ntests/test_quick_summary_diagnostics.py:30:        turnover_chart=None,\ntests/test_quick_summary_diagnostics.py:35:        turnover_diagnostic=None,\ntests/test_multi_period_engine_threshold_bounds.py:35:            \"max_turnover\": 1.0,\ntests/test_config_model.py:432:                    \"max_turnover\": 0.2,\ntests/test_config_model.py:437:    def test_rejects_turnover_above_one(self) -> None:\ntests/test_config_model.py:442:                    \"max_turnover\": 1.1,\ntests/test_config_model.py:452:                    \"max_turnover\": 0.2,\ntests/test_config_model.py:457:    def test_rejects_negative_turnover(self) -> None:\ntests/test_config_model.py:462:                    \"max_turnover\": -0.1,\ntests/test_config_model.py:530:                \"max_turnover\": 0.4,\ntests/test_config_model.py:543:        assert cfg.portfolio.max_turnover == 0.4\ntests/test_config_model.py:554:                \"max_turnover\": 0.4,\ntests/test_config_model.py:578:                \"max_turnover\": 0.2,\ntests/test_config_model.py:604:                \"max_turnover\": 0.5,\ntests/test_config_model.py:629:              max_turnover: 0.5\ntests/test_multi_period_engine_branch_new.py:269:        rebalance_strategies=[\"turnover_cap\"],\ntests/test_multi_period_engine_branch_new.py:270:        rebalance_params={\"turnover_cap\": {\"limit\": 0.1}},\ntests/test_multi_period_engine_branch_new.py:273:    assert captured[\"names\"] == [\"turnover_cap\"]\ntests/test_unified_report.py:15:    turnover = pd.Series(\ntests/test_unified_report.py:44:            \"turnover\": turnover,\ntests/test_unified_report.py:112:        max_turnover=None,\ntests/test_multi_period_engine_threshold_edgecases.py:36:            \"max_turnover\": 1.0,\ntests/test_multi_period_engine_threshold_edgecases.py:332:def test_threshold_hold_scales_trades_to_respect_turnover_cap(\ntests/test_multi_period_engine_threshold_edgecases.py:340:    cfg.portfolio.update({\"transaction_cost_bps\": 15.0, \"max_turnover\": 0.4})\ntests/test_multi_period_engine_threshold_edgecases.py:405:    # The expected weights below are derived from the initial weights and the turnover cap.\ntests/test_multi_period_engine_threshold_edgecases.py:407:    # Turnover cap: 1.0 (from config), but the test scenario results in a turnover of 0.4.\ntests/test_multi_period_engine_threshold_edgecases.py:626:def test_run_schedule_applies_strategy_and_turnover_fast_path(\ntests/test_multi_period_engine_threshold_edgecases.py:692:    assert portfolio.turnover[\"2020-01-31\"] == pytest.approx(1.0)\ntests/test_multi_period_engine_threshold_edgecases.py:693:    assert portfolio.turnover[\"2020-02-29\"] == pytest.approx(0.7, rel=1e-9)\ntests/test_multi_period_engine_threshold_edgecases.py:727:            \"max_turnover\": 0.2,\ntests/test_multi_period_engine_threshold_edgecases.py:924:    assert results[1][\"turnover\"] > 0\ntests/test_multi_period_engine_threshold_edgecases.py:1031:def test_threshold_hold_turnover_cap_scales_then_bounds(monkeypatch):\ntests/test_multi_period_engine_threshold_edgecases.py:1044:    cfg.portfolio.update({\"transaction_cost_bps\": 25.0, \"max_turnover\": 0.2})\ntests/test_multi_period_engine_threshold_edgecases.py:1136:    assert results[0][\"turnover\"] == pytest.approx(0.6)\ntests/test_tearsheet.py:13:    turnover = pd.Series(0.0, index=dates)\ntests/test_tearsheet.py:18:        turnover=turnover,\ntests/test_tearsheet.py:19:        costs={\"turnover_applied\": 0.001},\ntests/test_trend_analysis_typing_contract.py:50:    assert hints[\"turnover\"] is float\ntests/test_trend_analysis_typing_contract.py:83:    result[\"turnover\"] = 0.75\ntests/test_trend_analysis_typing_contract.py:90:    assert result[\"turnover\"] == 0.75\ntests/test_exports.py:84:            \"turnover\": 0.15,\ntests/test_exports.py:88:            \"turnover\": None,\ntests/test_exports.py:115:            \"turnover\": 0.1,\ntests/test_cv.py:97:                \"  cost_per_turnover: 0.0\",\ntests/test_cv.py:126:    assert {\"oos_sharpe\", \"turnover\", \"cost_drag\"}.issubset(folds_df.columns)\ntests/test_multi_period_engine_run_schedule_extra.py:70:    pf.rebalance(\"2023-03-15\", weights, turnover=0.12, cost=7.5)\ntests/test_multi_period_engine_run_schedule_extra.py:75:    assert pf.turnover[key] == pytest.approx(0.12)\ntests/test_export_additional_coverage.py:844:            \"turnover_value\": 0.03,\ntests/test_multi_period_engine_lambda_tc.py:8:def test_lambda_tc_penalty_reduces_turnover() -> None:\ntests/test_multi_period_engine_lambda_tc.py:12:    base = mp_engine._apply_turnover_penalty(\ntests/test_multi_period_engine_lambda_tc.py:15:    medium = mp_engine._apply_turnover_penalty(\ntests/test_multi_period_engine_lambda_tc.py:18:    high = mp_engine._apply_turnover_penalty(\ntests/test_multi_period_engine_lambda_tc.py:22:    def _turnover(weights: pd.Series) -> float:\ntests/test_multi_period_engine_lambda_tc.py:25:    base_turnover = _turnover(base)\ntests/test_multi_period_engine_lambda_tc.py:26:    assert base_turnover > _turnover(medium) > _turnover(high)\ntests/test_multi_period_engine.py:14:    portfolio.rebalance(\"2024-01-31\", weights, turnover=0.15, cost=0.02)\ntests/test_multi_period_engine.py:18:    assert np.isclose(portfolio.turnover[key], 0.15)\ntests/test_multi_period_engine.py:24:def test_compute_turnover_state_aligns_weights():\ntests/test_multi_period_engine.py:26:    turnover, prev_idx, prev_vals = engine._compute_turnover_state(None, None, first)\ntests/test_multi_period_engine.py:28:    assert np.isclose(turnover, np.abs(first).sum())\ntests/test_multi_period_engine.py:32:    turnover2, _, _ = engine._compute_turnover_state(prev_idx, prev_vals, second)\ntests/test_multi_period_engine.py:37:    assert np.isclose(turnover2, expected)\ntests/test_multi_period_engine.py:123:def test_run_schedule_without_strategies_uses_turnover_state():\ntests/test_multi_period_engine.py:132:    expected_turnover = np.abs(aligned - first).sum()\ntests/test_multi_period_engine.py:133:    assert np.isclose(pf.turnover[\"2024-02-29\"], expected_turnover)\ntests/test_plugin_registry.py:23:    assert \"turnover_cap\" in rebalancer_registry.available()\ntests/test_plugin_registry.py:24:    rb = rebalancer_registry.create(\"turnover_cap\", {\"max_turnover\": 0.1})\ntests/test_config_bridge.py:18:        max_turnover=0.5,\ntests/test_config_bridge.py:35:        max_turnover=0.5,\ntests/test_config_bridge.py:54:        max_turnover=0.5,\ntests/test_config_bridge.py:74:        max_turnover=0.5,\ntests/test_config_bridge.py:92:        max_turnover=0.5,\ntests/test_config_bridge.py:112:        max_turnover=0.5,\ntests/test_config_bridge.py:132:        max_turnover=0.5,\ntests/test_config_bridge.py:147:        max_turnover=0.5,\ntests/test_api_run_simulation_branches.py:205:            \"turnover\": pd.Series([0.1, 0.2]),\ntests/test_trend_config_model.py:26:            \"max_turnover\": 0.5,\ntests/test_trend_config_model.py:92:            \"max_turnover\": 0.5,\ntests/test_trend_config_model.py:124:            \"max_turnover\": 0.5,\ntests/test_trend_config_model.py:144:            \"max_turnover\": 0.5,\ntests/test_trend_config_model.py:169:            \"max_turnover\": 0.5,\ntests/test_trend_config_model.py:195:            \"max_turnover\": 0.5,\ntests/test_trend_config_model.py:218:            \"max_turnover\": 0.5,\ntests/test_trend_config_model.py:241:            \"max_turnover\": 0.5,\ntests/test_trend_config_model.py:283:            \"max_turnover\": 0.5,\ntests/test_trend_config_model.py:308:            \"max_turnover\": 0.5,\ntests/test_trend_config_model.py:331:            \"max_turnover\": 2,\ntests/test_trend_config_model.py:340:    assert str(exc.value).startswith(\"portfolio.max_turnover\")\ntests/test_trend_config_model.py:363:                    \"max_turnover\": 0.5,\ntests/test_backtesting_harness_additional.py:29:        turnover=pd.Series([0.1, 0.2, 0.0], index=index),\ntests/test_backtesting_harness_additional.py:30:        per_period_turnover=pd.Series([0.1, 0.2, 0.0], index=index),\ntests/test_backtesting_harness_additional.py:98:    assert result.turnover.index.equals(result.transaction_costs.index)\ntests/test_multi_period_engine_threshold_events_extended.py:34:            \"max_turnover\": 0.05,\ntests/test_multi_period_engine_threshold_events_extended.py:87:            \"max_turnover\": 1.0,\ntests/test_multi_period_engine_threshold_events_extended.py:401:    assert second_period[\"turnover\"] == pytest.approx(0.25)\ntests/test_multi_period_engine_threshold_events_extended.py:402:    expected_cost = second_period[\"turnover\"] * (\ntests/test_multi_period_engine_threshold_events_extended.py:478:    assert results[0][\"turnover\"] == pytest.approx(0.7)\ntests/test_config_fallback_additional.py:103:        ({\"portfolio\": {\"max_turnover\": 3.5}}, \"max_turnover must be <= 2.0\"),\ntests/test_config_model_validation.py:123:                \"max_turnover\": 0.5,\ntests/test_config_model_validation.py:132:                \"max_turnover\": 1.5,\ntests/test_config_model_validation.py:141:                \"max_turnover\": 0.25,\ntests/test_config_model_validation.py:177:            \"max_turnover\": 0.5,\ntests/test_config_model_validation.py:292:def test_portfolio_settings_negative_turnover_rejected() -> None:\ntests/test_config_model_validation.py:297:                \"max_turnover\": -0.1,\ntests/test_config_model_validation.py:390:    data[\"portfolio\"][\"max_turnover\"] = 2  # type: ignore[index]\ntests/test_config_model_validation.py:391:    with pytest.raises(ValueError, match=\"portfolio.max_turnover\"):\ntests/test_turnover_cap_strategy.py:1:\"\"\"Tests for turnover_cap rebalancing strategy.\"\"\"\ntests/test_turnover_cap_strategy.py:14:    \"\"\"Test turnover cap rebalancing strategy.\"\"\"\ntests/test_turnover_cap_strategy.py:16:    def test_no_turnover_cap_needed(self):\ntests/test_turnover_cap_strategy.py:17:        \"\"\"Test when desired trades are within turnover limit.\"\"\"\ntests/test_turnover_cap_strategy.py:19:            {\"max_turnover\": 0.2, \"cost_bps\": 10, \"priority\": \"largest_gap\"}\ntests/test_turnover_cap_strategy.py:27:        # Should execute all trades since total turnover = 0.1 < 0.2\ntests/test_turnover_cap_strategy.py:29:        expected_cost = 0.1 * 0.001  # 0.1 turnover * 10 bps\ntests/test_turnover_cap_strategy.py:32:    def test_turnover_cap_enforcement(self):\ntests/test_turnover_cap_strategy.py:33:        \"\"\"Test turnover cap limits total trades.\"\"\"\ntests/test_turnover_cap_strategy.py:35:            {\"max_turnover\": 0.15, \"cost_bps\": 20, \"priority\": \"largest_gap\"}\ntests/test_turnover_cap_strategy.py:43:        # Total desired turnover = |0.2-0.5| + |0.6-0.3| + |0.2-0.2| = 0.6\ntests/test_turnover_cap_strategy.py:45:        actual_turnover = (new_weights - current).abs().sum()\ntests/test_turnover_cap_strategy.py:46:        assert abs(actual_turnover - 0.15) < 1e-6\ntests/test_turnover_cap_strategy.py:48:        # Cost should be based on actual turnover\ntests/test_turnover_cap_strategy.py:49:        expected_cost = 0.15 * 0.002  # 0.15 turnover * 20 bps\ntests/test_turnover_cap_strategy.py:55:            {\"max_turnover\": 0.1, \"cost_bps\": 0, \"priority\": \"largest_gap\"}\ntests/test_turnover_cap_strategy.py:65:        # With 0.1 turnover budget, should execute A fully (0.3 turnover used)\ntests/test_turnover_cap_strategy.py:82:            {\"max_turnover\": 0.2, \"cost_bps\": 0, \"priority\": \"best_score_delta\"}\ntests/test_turnover_cap_strategy.py:95:        # Total desired turnover = 0.4, budget = 0.2\ntests/test_turnover_cap_strategy.py:104:            {\"max_turnover\": 0.1, \"cost_bps\": 0, \"priority\": \"best_score_delta\"}\ntests/test_turnover_cap_strategy.py:114:        actual_turnover = (new_weights - current).abs().sum()\ntests/test_turnover_cap_strategy.py:115:        assert abs(actual_turnover - 0.1) < 1e-6\ntests/test_turnover_cap_strategy.py:117:    def test_zero_turnover_cap(self):\ntests/test_turnover_cap_strategy.py:118:        \"\"\"Test zero turnover cap prevents all trades.\"\"\"\ntests/test_turnover_cap_strategy.py:120:            {\"max_turnover\": 0.0, \"cost_bps\": 10, \"priority\": \"largest_gap\"}\ntests/test_turnover_cap_strategy.py:135:            {\"max_turnover\": 0.5, \"cost_bps\": 0, \"priority\": \"largest_gap\"}\ntests/test_turnover_cap_strategy.py:151:            {\"max_turnover\": 0.3, \"cost_bps\": 100, \"priority\": \"largest_gap\"}  # 1%\ntests/test_turnover_cap_strategy.py:159:        # Total desired turnover = 0.4, but cap is 0.3, so actual turnover = 0.3\ntests/test_turnover_cap_strategy.py:160:        expected_cost = 0.3 * 0.01  # 0.3 turnover * 100 bps\ntests/test_turnover_cap_strategy.py:166:            {\"max_turnover\": 0.25, \"cost_bps\": 0, \"priority\": \"largest_gap\"}\ntests/test_turnover_cap_strategy.py:179:        actual_turnover = (new_weights - current).abs().sum()\ntests/test_turnover_cap_strategy.py:180:        assert abs(actual_turnover - 0.25) < 1e-6\ntests/test_turnover_cap_strategy.py:193:            \"turnover_cap\", {\"max_turnover\": 0.1, \"cost_bps\": 15}\ntests/test_turnover_cap_strategy.py:197:        assert strategy.max_turnover == 0.1\ntests/test_turnover_cap_strategy.py:207:        strategies = [\"drift_band\", \"turnover_cap\"]\ntests/test_turnover_cap_strategy.py:210:            \"turnover_cap\": {\ntests/test_turnover_cap_strategy.py:211:                \"max_turnover\": 0.6,\ntests/test_turnover_cap_strategy.py:223:        # Should apply drift_band first, then turnover_cap\ntests/test_turnover_cap_strategy.py:227:        # Since turnover cap is high (0.6), drift_band changes should be preserved\ntests/test_turnover_cap_strategy.py:228:        actual_turnover = (final_weights - current).abs().sum()\ntests/test_turnover_cap_strategy.py:229:        assert actual_turnover <= 0.6 + 1e-6  # Allow small numerical error\ntests/test_risk.py:57:        max_turnover=None,\ntests/test_risk.py:62:    assert diagnostics.turnover_value >= 0\ntests/test_risk.py:67:def test_compute_constrained_weights_respects_turnover_cap() -> None:\ntests/test_risk.py:88:        max_turnover=0.2,\ntests/test_risk.py:91:    assert diagnostics.turnover_value <= 0.2000001\ntests/test_risk.py:95:def test_compute_constrained_weights_lambda_penalty_reduces_turnover() -> None:\ntests/test_risk.py:117:        max_turnover=None,\ntests/test_risk.py:130:        max_turnover=None,\ntests/test_risk.py:133:    assert diag_high.turnover_value < diag_zero.turnover_value\ntests/test_risk.py:160:def test_enforce_turnover_cap_with_previous() -> None:\ntests/test_risk.py:163:    adjusted, turnover = risk._enforce_turnover_cap(target, prev, max_turnover=0.4)\ntests/test_risk.py:165:    assert turnover <= 0.4 + 1e-6\ntests/test_risk.py:207:def test_compute_constrained_weights_uses_dummy_turnover_index() -> None:\ntests/test_risk.py:220:        max_turnover=None,\ntests/test_risk.py:222:    assert \"rebalance\" in diagnostics.turnover.index.name\ntests/test_config_legacy.py:23:            \"max_turnover\": 1.0,\ntests/test_turnover_vectorization.py:9:from trend_analysis.multi_period.engine import _compute_turnover_state\ntests/test_turnover_vectorization.py:13:def python_turnover_state(\ntests/test_turnover_vectorization.py:44:    turnover = float(np.abs(new_aligned - prev_aligned).sum())\ntests/test_turnover_vectorization.py:45:    return turnover, nidx, nvals\ntests/test_turnover_vectorization.py:49:def test_vectorised_turnover_matches_python(cols: int) -> None:\ntests/test_turnover_vectorization.py:64:        expected, prev_idx_py, prev_vals_py = python_turnover_state(\ntests/test_turnover_vectorization.py:67:        got, prev_idx_vec, prev_vals_vec = _compute_turnover_state(\ntests/test_turnover_vectorization.py:75:def python_turnover_cap(\ntests/test_turnover_vectorization.py:86:    if total_desired <= strategy.max_turnover:\ntests/test_turnover_vectorization.py:87:        actual_turnover = float(total_desired)\ntests/test_turnover_vectorization.py:101:        remaining = strategy.max_turnover\ntests/test_turnover_vectorization.py:115:        actual_turnover = float(executed.abs().sum())\ntests/test_turnover_vectorization.py:117:    cost = strategy._calculate_cost(actual_turnover)\ntests/test_turnover_vectorization.py:122:def test_turnover_cap_vectorisation_matches_python(priority: str) -> None:\ntests/test_turnover_vectorization.py:126:    params = {\"max_turnover\": 0.35, \"cost_bps\": 15, \"priority\": priority}\ntests/test_turnover_vectorization.py:146:        expected_weights, expected_cost = python_turnover_cap(\ntests/test_transaction_costs_and_turnover.py:31:    cfg_data[\"portfolio\"][\"max_turnover\"] = 0.5\ntests/test_transaction_costs_and_turnover.py:37:def test_period_results_include_turnover_and_cost():\ntests/test_transaction_costs_and_turnover.py:44:        assert \"turnover\" in res\ntests/test_transaction_costs_and_turnover.py:46:        assert isinstance(res[\"turnover\"], float)\ntests/test_metrics_summary.py:32:    assert out.loc[\"turnover\", \"value\"] == pytest.approx(0.2)\ntests/test_pipeline_helpers_additional.py:642:            max_turnover=\"bad\",\ntests/test_pipeline_helpers_additional.py:698:            turnover=pd.Series(dtype=float),\ntests/test_pipeline_helpers_additional.py:699:            turnover_value=0.0,\ntests/test_pipeline_helpers_additional.py:722:            max_turnover=0.5,\ntests/test_pipeline_helpers_additional.py:1047:            turnover=pd.Series(dtype=float),\ntests/test_pipeline_helpers_additional.py:1048:            turnover_value=0.0,\ntests/test_pipeline_helpers_additional.py:1094:            max_turnover=None,\ntests/test_pipeline_helpers_additional.py:1257:            turnover=pd.Series(dtype=float),\ntests/test_pipeline_helpers_additional.py:1258:            turnover_value=0.0,\ntests/test_pipeline_helpers_additional.py:1375:            turnover=pd.Series(dtype=float),\ntests/test_pipeline_helpers_additional.py:1376:            turnover_value=0.0,\ntests/test_pipeline_helpers_additional.py:1463:            turnover=pd.Series(dtype=float),\ntests/test_pipeline_helpers_additional.py:1464:            turnover_value=0.0,\ntests/test_pipeline_helpers_additional.py:1549:            turnover=pd.Series(dtype=float),\ntests/test_pipeline_helpers_additional.py:1550:            turnover_value=0.0,\ntests/test_pipeline_helpers_additional.py:1634:            turnover=pd.Series(dtype=float),\ntests/test_pipeline_helpers_additional.py:1635:            turnover_value=0.0,\ntests/app/test_turnover_budget_policy.py:11:def test_turnover_budget_limits_changes():\ntests/app/test_turnover_budget_policy.py:27:        turnover_budget_max_changes=2,  # cap total moves to 2\ntests/test_metrics_turnover_extra.py:6:from trend_analysis.metrics.turnover import realized_turnover, turnover_cost\ntests/test_metrics_turnover_extra.py:9:def test_realized_turnover_sorts_mapping_and_handles_missing():\ntests/test_metrics_turnover_extra.py:18:    turnover = realized_turnover(weights)\ntests/test_metrics_turnover_extra.py:21:    assert list(turnover.index) == [\ntests/test_metrics_turnover_extra.py:27:    feb_turnover = abs(0.55 - 0.5) + abs(0.0 - 0.5)\ntests/test_metrics_turnover_extra.py:28:    mar_turnover = abs(0.6 - 0.55) + abs(0.4 - 0.0)\ntests/test_metrics_turnover_extra.py:29:    assert turnover.loc[pd.Timestamp(\"2020-02-29\"), \"turnover\"] == pytest.approx(\ntests/test_metrics_turnover_extra.py:30:        feb_turnover\ntests/test_metrics_turnover_extra.py:32:    assert turnover.loc[pd.Timestamp(\"2020-03-31\"), \"turnover\"] == pytest.approx(\ntests/test_metrics_turnover_extra.py:33:        mar_turnover\ntests/test_metrics_turnover_extra.py:37:def test_turnover_cost_scales_basis_points():\ntests/test_metrics_turnover_extra.py:45:    costs = turnover_cost(df, cost_bps=25)\ntests/test_metrics_turnover_extra.py:47:    expected_turnover = realized_turnover(df)[\"turnover\"]\ntests/test_metrics_turnover_extra.py:48:    assert costs.equals(expected_turnover * 0.0025)\ntests/app/test_results_page.py:221:                    \"turnover\": pd.Series([0.1, 0.2], index=returns.index[:2]),\ntests/app/test_results_page.py:232:        \"turnover_chart\",\ntests/app/test_results_page.py:303:                    \"turnover\": pd.Series([0.1, 0.2], index=returns.index[:2]),\ntests/app/test_results_page.py:314:        \"turnover_chart\",\ntests/app/test_comparison_helpers.py:16:def _result_with_periods(turnover: float, txn_cost: float) -> SimpleNamespace:\ntests/app/test_comparison_helpers.py:24:                    \"turnover\": turnover,\ntests/app/test_comparison_helpers.py:72:    res_a = _result_with_periods(turnover=0.20, txn_cost=0.01)\ntests/app/test_comparison_helpers.py:73:    res_b = _result_with_periods(turnover=0.25, txn_cost=0.015)\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_8","type":"reasoning","text":"**Tracing max_turnover configuration**"}}
{"type":"item.started","item":{"id":"item_9","type":"command_execution","command":"/bin/bash -lc 'rg -n \"enforce_turnover_cap|_enforce_turnover_cap\" src/trend_analysis'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_9","type":"command_execution","command":"/bin/bash -lc 'rg -n \"enforce_turnover_cap|_enforce_turnover_cap\" src/trend_analysis'","aggregated_output":"src/trend_analysis/risk.py:170:def _enforce_turnover_cap(\nsrc/trend_analysis/risk.py:287:    constrained, turnover_value = _enforce_turnover_cap(\n","exit_code":0,"status":"completed"}}
{"type":"item.started","item":{"id":"item_10","type":"command_execution","command":"/bin/bash -lc \"sed -n '1,240p' src/trend_analysis/risk.py\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_10","type":"command_execution","command":"/bin/bash -lc \"sed -n '1,240p' src/trend_analysis/risk.py\"","aggregated_output":"\"\"\"Shared risk-control helpers for volatility targeting and constraints.\n\nThis module centralises volatility targeting, weight normalisation, and\nturnover enforcement so that both the CLI pipeline and the Streamlit app\noperate with the exact same risk logic.  Functions are intentionally\nstateless â€“ callers supply the relevant slices of returns and any\nprevious-period weights and receive normalised, constraint-aware weights\nalongside diagnostics that power reporting layers.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Iterable, Mapping, MutableMapping\n\nimport numpy as np\nimport pandas as pd\n\nfrom .engine import optimizer as optimizer_mod\n\nPERIODS_PER_YEAR: Mapping[str, float] = {\n    \"D\": 252.0,\n    \"W\": 52.0,\n    \"M\": 12.0,\n    \"Q\": 4.0,\n    \"Y\": 1.0,\n}\n\n\n@dataclass(slots=True)\nclass RiskWindow:\n    \"\"\"Specification for realised-volatility lookback windows.\"\"\"\n\n    length: int\n    decay: str = \"simple\"\n    ewma_lambda: float = 0.94\n\n\n@dataclass(slots=True)\nclass RiskDiagnostics:\n    \"\"\"Diagnostics emitted by :func:`compute_constrained_weights`.\"\"\"\n\n    asset_volatility: pd.DataFrame\n    portfolio_volatility: pd.Series\n    turnover: pd.Series\n    turnover_value: float\n    scale_factors: pd.Series\n\n\ndef periods_per_year_from_code(code: str | None) -> float:\n    \"\"\"Return periods-per-year scaling given a frequency code.\"\"\"\n\n    if not code:\n        return 12.0\n    return PERIODS_PER_YEAR.get(code.upper(), 12.0)\n\n\ndef _ensure_series(weights: Mapping[str, float] | pd.Series) -> pd.Series:\n    if isinstance(weights, pd.Series):\n        series = weights.astype(float).copy()\n    else:\n        series = pd.Series({str(k): float(v) for k, v in weights.items()}, dtype=float)\n    return series.sort_index()\n\n\ndef realised_volatility(\n    returns: pd.DataFrame,\n    window: RiskWindow,\n    *,\n    periods_per_year: float = 12.0,\n    min_periods: int | None = None,\n) -> pd.DataFrame:\n    \"\"\"Compute realised volatility per asset across ``returns``.\n\n    The helper supports both simple rolling windows and an EWMA variant\n    that mirrors common risk engines.\n    \"\"\"\n\n    if returns.empty:\n        raise ValueError(\"returns cannot be empty\")\n\n    min_periods_val = 1 if not min_periods else int(min_periods)\n    rolling_kwargs: MutableMapping[str, object] = {\n        \"min_periods\": min_periods_val,\n    }\n\n    if window.length <= 0:\n        raise ValueError(\"window length must be positive\")\n\n    returns = returns.astype(float)\n\n    if window.decay.lower() == \"ewma\":\n        lam = float(window.ewma_lambda)\n        alpha = 1.0 - lam\n        if not (0.0 < alpha < 1.0):\n            raise ValueError(\n                f\"ewma_lambda must be between 0 and 1 (got {lam}); \"\n                f\"computed alpha = 1 - ewma_lambda = {alpha:.4f} must be between 0 and 1\"\n            )\n\n        def _ewma_std(values: np.ndarray) -> float:\n            series = pd.Series(values, dtype=float)\n            return float(series.ewm(alpha=alpha, adjust=False).std(bias=False).iloc[-1])\n\n        vol = returns.rolling(window=window.length, min_periods=min_periods_val).apply(\n            _ewma_std, raw=True\n        )\n    else:\n        vol = returns.rolling(window=window.length, **rolling_kwargs).std(ddof=0)\n\n    return vol.mul(np.sqrt(periods_per_year))\n\n\ndef _scale_factors(\n    latest_vol: pd.Series,\n    target_vol: float | None,\n    *,\n    floor_vol: float | None = None,\n) -> pd.Series:\n    if target_vol is None:\n        return pd.Series(1.0, index=latest_vol.index, dtype=float)\n    try:\n        target = float(target_vol)\n    except (TypeError, ValueError):  # pragma: no cover - defensive\n        return pd.Series(1.0, index=latest_vol.index, dtype=float)\n    if target <= 0:\n        return pd.Series(1.0, index=latest_vol.index, dtype=float)\n\n    vol = latest_vol.astype(float).replace(0.0, np.nan)\n    if floor_vol is not None and floor_vol > 0:\n        vol = vol.clip(lower=float(floor_vol))\n    factors = pd.Series(target, index=vol.index, dtype=float).div(vol)\n    factors = factors.replace([np.inf, -np.inf], 0.0).fillna(0.0)\n    return factors\n\n\ndef _normalise(weights: pd.Series) -> pd.Series:\n    total = float(weights.sum())\n    if total == 0.0:\n        return weights.copy()\n    return weights / total\n\n\ndef _apply_turnover_penalty(\n    target: pd.Series,\n    prev: pd.Series | None,\n    lambda_tc: float | None,\n) -> pd.Series:\n    \"\"\"Blend ``target`` towards ``prev`` when ``lambda_tc`` is positive.\"\"\"\n\n    if prev is None:\n        return target\n    if lambda_tc is None:\n        return target\n    try:\n        lam = float(lambda_tc)\n    except (TypeError, ValueError):  # pragma: no cover - defensive\n        lam = 0.0\n    if lam <= 0.0:\n        return target\n    if lam >= 1.0:\n        return _normalise(prev.reindex(target.index.union(prev.index), fill_value=0.0))\n\n    aligned_index = target.index.union(prev.index)\n    prev_aligned = prev.reindex(aligned_index, fill_value=0.0).astype(float)\n    target_aligned = target.reindex(aligned_index, fill_value=0.0).astype(float)\n    return prev_aligned + (target_aligned - prev_aligned) * (1.0 - lam)\n\n\ndef _enforce_turnover_cap(\n    target: pd.Series,\n    prev: pd.Series | None,\n    max_turnover: float | None,\n) -> tuple[pd.Series, float]:\n    target = target.astype(float)\n    if prev is None:\n        turnover = float(target.abs().sum())\n        return target, turnover\n\n    aligned_index = target.index.union(prev.index)\n    prev_aligned = prev.reindex(aligned_index, fill_value=0.0)\n    target_aligned = target.reindex(aligned_index, fill_value=0.0)\n    trades = target_aligned - prev_aligned\n    turnover = float(trades.abs().sum())\n    if max_turnover is None or max_turnover <= 0 or turnover <= max_turnover:\n        return target_aligned, turnover\n    scale = max_turnover / turnover if turnover > 0 else 0.0\n    adjusted = prev_aligned + trades * scale\n    return adjusted, float((adjusted - prev_aligned).abs().sum())\n\n\ndef _enforce_max_active(\n    weights: pd.Series,\n    max_active: int | None,\n) -> pd.Series:\n    if max_active is None:\n        return weights\n    try:\n        max_active_val = int(max_active)\n    except (TypeError, ValueError):  # pragma: no cover - defensive\n        return weights\n    if max_active_val <= 0:\n        return weights\n    if weights.empty:\n        return weights\n\n    cash_label = \"CASH\"\n    has_cash = cash_label in weights.index\n    working = weights.drop(labels=[cash_label]) if has_cash else weights\n    active = working[working.abs() > 0]\n    if len(active) <= max_active_val:\n        return weights\n\n    keep = (\n        active.abs()\n        .sort_values(ascending=False, kind=\"mergesort\")\n        .head(max_active_val)\n        .index\n    )\n    trimmed = working.copy()\n    trimmed.loc[~trimmed.index.isin(keep)] = 0.0\n\n    if has_cash:\n        cash_weight = float(weights.loc[cash_label])\n        remaining = 1.0 - cash_weight\n        if remaining < 0:\n            remaining = 0.0\n        trimmed_sum = float(trimmed.sum())\n        if trimmed_sum != 0 and remaining > 0:\n            trimmed = trimmed * (remaining / trimmed_sum)\n        result = trimmed.copy()\n        result.loc[cash_label] = cash_weight\n        return result.reindex(weights.index)\n\n    return trimmed.reindex(weights.index)\n\n\ndef compute_constrained_weights(\n    base_weights: Mapping[str, float] | pd.Series,\n    returns: pd.DataFrame,\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_11","type":"reasoning","text":"**Tracing method calls to _enforce_turnover_cap**"}}
{"type":"item.started","item":{"id":"item_12","type":"command_execution","command":"/bin/bash -lc \"sed -n '240,420p' src/trend_analysis/risk.py\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_12","type":"command_execution","command":"/bin/bash -lc \"sed -n '240,420p' src/trend_analysis/risk.py\"","aggregated_output":"    returns: pd.DataFrame,\n    *,\n    window: RiskWindow,\n    target_vol: float | None,\n    periods_per_year: float,\n    floor_vol: float | None,\n    long_only: bool,\n    max_weight: float | None,\n    max_active_positions: int | None = None,\n    previous_weights: Mapping[str, float] | pd.Series | None = None,\n    lambda_tc: float | None = None,\n    max_turnover: float | None = None,\n    group_caps: Mapping[str, float] | None = None,\n    groups: Mapping[str, str] | None = None,\n) -> tuple[pd.Series, RiskDiagnostics]:\n    \"\"\"Apply risk controls and return final weights plus diagnostics.\"\"\"\n\n    if returns.empty:\n        raise ValueError(\"returns cannot be empty\")\n\n    base = _ensure_series(base_weights)\n    if base.empty:\n        raise ValueError(\"base_weights cannot be empty\")\n\n    base = _normalise(base)\n    realised = realised_volatility(returns, window, periods_per_year=periods_per_year)\n    latest = realised.iloc[-1].reindex(base.index).ffill().bfill()\n    latest = latest.reindex(base.index).fillna(realised.mean(axis=0))\n    positive = latest[latest > 0]\n    fallback = float(positive.min()) if not positive.empty else 1.0\n    latest = latest.fillna(fallback)\n\n    scale_factors = _scale_factors(latest, target_vol, floor_vol=floor_vol)\n    scaled = base.mul(scale_factors)\n\n    constraint_payload = {\n        \"long_only\": bool(long_only),\n        \"max_weight\": max_weight,\n        \"group_caps\": group_caps,\n        \"groups\": groups,\n    }\n    constrained = optimizer_mod.apply_constraints(scaled, constraint_payload)\n\n    prev_series = (\n        _ensure_series(previous_weights) if previous_weights is not None else None\n    )\n    constrained = _apply_turnover_penalty(constrained, prev_series, lambda_tc)\n    constrained, turnover_value = _enforce_turnover_cap(\n        constrained, prev_series, max_turnover\n    )\n    constrained = constrained.reindex(base.index, fill_value=0.0)\n    constrained = _enforce_max_active(constrained, max_active_positions)\n    constrained = _normalise(constrained)\n\n    aligned_returns = returns.reindex(columns=constrained.index, fill_value=0.0)\n    portfolio_returns = aligned_returns.mul(constrained, axis=1).sum(axis=1)\n    portfolio_vol = realised_volatility(\n        portfolio_returns.to_frame(\"portfolio\"),\n        window,\n        periods_per_year=periods_per_year,\n    )[\"portfolio\"]\n\n    turnover_index: Iterable[pd.Timestamp]\n    if isinstance(returns.index, pd.DatetimeIndex) and len(returns.index) > 0:\n        turnover_index = [returns.index[-1]]\n    else:\n        turnover_index = [pd.Timestamp(\"1970-01-01\")]  # dummy timestamp\n\n    turnover_series = pd.Series(\n        [turnover_value],\n        index=pd.Index(turnover_index, name=\"rebalance\"),\n        name=\"turnover\",\n    )\n\n    diagnostics = RiskDiagnostics(\n        asset_volatility=realised,\n        portfolio_volatility=portfolio_vol,\n        turnover=turnover_series,\n        turnover_value=float(turnover_value),\n        scale_factors=scale_factors.reindex(constrained.index).fillna(0.0),\n    )\n\n    return constrained, diagnostics\n\n\n__all__ = [\n    \"RiskDiagnostics\",\n    \"RiskWindow\",\n    \"compute_constrained_weights\",\n    \"periods_per_year_from_code\",\n    \"realised_volatility\",\n]\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_13","type":"reasoning","text":"**Searching for turnover cap config**"}}
{"type":"item.started","item":{"id":"item_14","type":"command_execution","command":"/bin/bash -lc 'rg -n \"compute_constrained_weights|max_turnover\" src/trend_analysis/pipeline.py'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_14","type":"command_execution","command":"/bin/bash -lc 'rg -n \"compute_constrained_weights|max_turnover\" src/trend_analysis/pipeline.py'","aggregated_output":"43:    compute_constrained_weights,\n752:    max_turnover: float | None,\n1021:    if max_turnover is not None:\n1023:            mt = float(max_turnover)\n1044:        weights_series, risk_diagnostics = compute_constrained_weights(\n1056:            max_turnover=turnover_cap,\n1353:            \"max_turnover\": computation.turnover_cap,\n2311:    max_turnover: float | None = None,\n2395:        max_turnover=max_turnover,\n2445:    max_turnover: float | None = None,\n2482:        max_turnover=max_turnover,\n2540:    max_turnover: float | None = None,\n2592:        max_turnover=max_turnover,\n2702:        max_turnover=_section_get(portfolio_cfg, \"max_turnover\"),\n2830:        max_turnover=_section_get(portfolio_cfg, \"max_turnover\"),\n","exit_code":0,"status":"completed"}}
{"type":"item.started","item":{"id":"item_15","type":"command_execution","command":"/bin/bash -lc \"sed -n '720,1120p' src/trend_analysis/pipeline.py\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_15","type":"command_execution","command":"/bin/bash -lc \"sed -n '720,1120p' src/trend_analysis/pipeline.py\"","aggregated_output":"        preprocess.df[[preprocess.date_col] + fund_cols],\n        in_label,\n        in_end_label,\n        stats_cfg=stats_cfg,\n        risk_free=risk_free_override,\n    )\n\n    return _SelectionStage(\n        fund_cols=fund_cols,\n        rf_col=rf_col,\n        rf_source=rf_source,\n        score_frame=score_frame,\n        risk_free_override=risk_free_override,\n        indices_list=valid_indices,\n        requested_indices=requested_indices,\n        missing_indices=missing_indices,\n    )\n\n\ndef _compute_weights_and_stats(\n    preprocess: _PreprocessStage,\n    window: _WindowStage,\n    selection: _SelectionStage,\n    *,\n    target_vol: float | None,\n    monthly_cost: float,\n    custom_weights: dict[str, float] | None,\n    weighting_scheme: str | None,\n    constraints: Mapping[str, Any] | None,\n    risk_window: Mapping[str, Any] | None,\n    previous_weights: Mapping[str, float] | None,\n    lambda_tc: float | None,\n    max_turnover: float | None,\n    signal_spec: TrendSpec | None,\n    weight_policy: Mapping[str, Any] | None,\n    warmup: int,\n    min_floor: float,\n    stats_cfg: RiskStatsConfig,\n    weight_engine_params: Mapping[str, Any] | None,\n) -> _ComputationStage:\n    fund_cols = selection.fund_cols\n\n    def _enforce_window_bounds(\n        frame: pd.DataFrame,\n        label: str,\n        allowed_start: pd.Timestamp,\n        allowed_end: pd.Timestamp,\n    ) -> None:\n        if frame.empty:\n            return\n        idx = frame.index\n        outside_mask = (idx < allowed_start) | (idx > allowed_end)\n        if bool(outside_mask.any()):\n            oob_index = idx[outside_mask]\n            first = pd.Timestamp(oob_index.min())\n            last = pd.Timestamp(oob_index.max())\n            msg = (\n                f\"{label} contain dates outside the active analysis window: \"\n                f\"[{first} â†’ {last}] not within [{allowed_start} â†’ {allowed_end}]\"\n            )\n            raise ValueError(msg)\n\n    _enforce_window_bounds(\n        window.in_df,\n        label=\"In-sample returns\",\n        allowed_start=window.in_start,\n        allowed_end=window.in_end,\n    )\n    _enforce_window_bounds(\n        window.out_df,\n        label=\"Out-of-sample returns\",\n        allowed_start=window.out_start,\n        allowed_end=window.out_end,\n    )\n\n    def _scoped_signal_inputs() -> pd.DataFrame:\n        if not fund_cols:\n            return pd.DataFrame(dtype=float)\n\n        allowed_start = window.in_start\n        allowed_end = window.out_end\n\n        def _filter_window(frame: pd.DataFrame, *, strict: bool) -> pd.DataFrame:\n            outside_mask = (frame.index < allowed_start) | (frame.index > allowed_end)\n            if strict and bool(outside_mask.any()):\n                first = pd.Timestamp(frame.index[outside_mask].min())\n                last = pd.Timestamp(frame.index[outside_mask].max())\n                msg = (\n                    \"Signal inputs contain dates outside the active analysis window: \"\n                    f\"[{first} â†’ {last}] not within [{allowed_start} â†’ {allowed_end}]\"\n                )\n                raise ValueError(msg)\n\n            scoped = frame.loc[\n                (frame.index >= allowed_start) & (frame.index <= allowed_end)\n            ]\n            scoped = scoped.loc[:, ~scoped.columns.duplicated()]\n            scoped = scoped.reindex(columns=fund_cols)\n            return scoped.astype(float)\n\n        signal_source: pd.DataFrame | None = None\n        strict_enforcement = True\n        try:\n            scoped_cols = [preprocess.date_col, *fund_cols]\n            # Copy before ``set_index`` so DataFrame subclasses (see\n            # ``tests.test_pipeline_helpers_additional.SignalFrame``) can signal\n            # that no usable signal data are available.\n            subset = preprocess.df[scoped_cols].copy()\n            signal_source = subset.set_index(preprocess.date_col)\n            strict_enforcement = False\n        except Exception:\n            signal_source = None\n\n        if signal_source is None:\n            signal_source = (\n                pd.concat([window.in_df, window.out_df])\n                .sort_index()\n                .reindex(columns=fund_cols)\n            )\n\n        return _filter_window(signal_source, strict=strict_enforcement)\n\n    custom_weights_input = custom_weights is not None\n    weight_engine_used = False\n    weight_engine_fallback: dict[str, Any] | None = None\n    weight_engine_diagnostics: dict[str, Any] | None = None\n    if (\n        custom_weights is None\n        and weighting_scheme\n        and weighting_scheme.lower() != \"equal\"\n    ):\n        try:\n            from .plugins import create_weight_engine\n\n            cov = window.in_df[fund_cols].cov()\n            engine = create_weight_engine(\n                weighting_scheme.lower(), **(weight_engine_params or {})\n            )\n            w_series = engine.weight(cov).reindex(fund_cols).fillna(0.0)\n            custom_weights = {c: float(w_series.get(c, 0.0) * 100.0) for c in fund_cols}\n            weight_engine_used = True\n            weight_engine_diagnostics = getattr(engine, \"diagnostics\", None)\n            if (\n                weight_engine_diagnostics\n                and isinstance(weight_engine_diagnostics, Mapping)\n                and weight_engine_diagnostics.get(\"used_safe_mode\")\n            ):\n                safe_mode = weight_engine_diagnostics.get(\"safe_mode\")\n                condition_number = weight_engine_diagnostics.get(\"condition_number\")\n                condition_threshold = weight_engine_diagnostics.get(\n                    \"condition_threshold\"\n                )\n                condition_source = weight_engine_diagnostics.get(\"condition_source\")\n                raw_condition_number = weight_engine_diagnostics.get(\n                    \"raw_condition_number\"\n                )\n                shrunk_condition_number = weight_engine_diagnostics.get(\n                    \"shrunk_condition_number\"\n                )\n                shrinkage_info = weight_engine_diagnostics.get(\"shrinkage\")\n                fallback_reason = weight_engine_diagnostics.get(\n                    \"fallback_reason\", \"safe_mode\"\n                )\n                weight_engine_fallback = {\n                    \"engine\": str(weighting_scheme),\n                    \"reason\": str(fallback_reason),\n                    \"safe_mode\": safe_mode,\n                    \"condition_number\": condition_number,\n                    \"condition_threshold\": condition_threshold,\n                    \"condition_source\": condition_source,\n                }\n                if raw_condition_number is not None:\n                    weight_engine_fallback[\"raw_condition_number\"] = (\n                        raw_condition_number\n                    )\n                if shrunk_condition_number is not None:\n                    weight_engine_fallback[\"shrunk_condition_number\"] = (\n                        shrunk_condition_number\n                    )\n                if isinstance(shrinkage_info, Mapping):\n                    weight_engine_fallback[\"shrinkage\"] = dict(shrinkage_info)\n                if isinstance(condition_number, (int, float)) and isinstance(\n                    condition_threshold, (int, float)\n                ):\n                    logger.warning(\n                        \"Weight engine '%s' switched to safe mode '%s' \"\n                        \"(%s condition number %.2e > threshold %.2e).\",\n                        weighting_scheme,\n                        safe_mode,\n                        condition_source or \"covariance\",\n                        condition_number,\n                        condition_threshold,\n                    )\n                else:\n                    logger.warning(\n                        \"Weight engine '%s' switched to safe mode '%s'.\",\n                        weighting_scheme,\n                        safe_mode,\n                    )\n            logger.debug(\n                \"Successfully created %s weight engine\",\n                weighting_scheme,\n                extra={\"weight_engine\": weighting_scheme},\n            )\n        except Exception as e:  # pragma: no cover - exercised via tests\n            msg = (\n                \"Weight engine '%s' failed (%s: %s); falling back to equal weights\"\n                % (weighting_scheme, type(e).__name__, e)\n            )\n            logger.warning(msg)\n            logger.debug(\n                \"Weight engine creation failed, falling back to equal weights: %s\", e\n            )\n            weight_engine_fallback = {\n                \"engine\": str(weighting_scheme),\n                \"error_type\": type(e).__name__,\n                \"error\": str(e),\n                \"logger_level\": logging.getLevelName(logger.getEffectiveLevel()),\n            }\n            custom_weights = None\n\n    if custom_weights is None:\n        custom_weights = {c: 100 / len(fund_cols) for c in fund_cols}\n\n    constraints_cfg = constraints or {}\n    if not isinstance(constraints_cfg, Mapping):\n        constraints_cfg = {}\n    long_only = bool(constraints_cfg.get(\"long_only\", True))\n    raw_max_weight = constraints_cfg.get(\"max_weight\")\n    try:\n        max_weight_val = float(raw_max_weight) if raw_max_weight is not None else None\n    except (TypeError, ValueError):\n        max_weight_val = None\n    raw_max_active = constraints_cfg.get(\"max_active_positions\")\n    if raw_max_active is None:\n        raw_max_active = constraints_cfg.get(\"max_active\")\n    try:\n        max_active_val = int(raw_max_active) if raw_max_active is not None else None\n    except (TypeError, ValueError):\n        max_active_val = None\n    if max_active_val is not None and max_active_val <= 0:\n        max_active_val = None\n    raw_group_caps = constraints_cfg.get(\"group_caps\")\n    group_caps_map = (\n        {str(k): float(v) for k, v in raw_group_caps.items()}\n        if isinstance(raw_group_caps, Mapping)\n        else None\n    )\n    raw_groups = constraints_cfg.get(\"groups\")\n    groups_map = (\n        {str(k): str(v) for k, v in raw_groups.items()}\n        if isinstance(raw_groups, Mapping)\n        else None\n    )\n\n    base_series = pd.Series(\n        {c: float(custom_weights.get(c, 0.0)) / 100.0 for c in fund_cols},\n        dtype=float,\n    )\n    if float(base_series.sum()) <= 0:\n        base_series = pd.Series(\n            np.repeat(1.0 / len(fund_cols), len(fund_cols)),\n            index=fund_cols,\n            dtype=float,\n        )\n\n    negative_assets = base_series[base_series < 0].index.tolist()\n    if negative_assets:\n        if weight_engine_used:\n            source = f\"weight engine '{weighting_scheme}'\"\n        elif custom_weights_input:\n            source = \"custom weights\"\n        else:\n            source = \"base weights\"\n        action = \"clip negatives to zero\" if long_only else \"allow short allocations\"\n        logger.info(\n            \"%s produced %d negative weights; long_only=%s so pipeline will %s.\",\n            source,\n            len(negative_assets),\n            long_only,\n            action,\n        )\n\n    window_cfg = dict(risk_window or {})\n    try:\n        window_length = int(window_cfg.get(\"length\", len(window.in_df)))\n    except (TypeError, ValueError):\n        window_length = len(window.in_df)\n    if window_length <= 0:\n        window_length = max(len(window.in_df), 1)\n    decay_mode = str(window_cfg.get(\"decay\", \"simple\"))\n    lambda_value = window_cfg.get(\"lambda\", window_cfg.get(\"ewma_lambda\", 0.94))\n    try:\n        ewma_lambda = float(lambda_value)\n    except (TypeError, ValueError):\n        ewma_lambda = 0.94\n    window_spec = RiskWindow(\n        length=window_length, decay=decay_mode, ewma_lambda=ewma_lambda\n    )\n\n    turnover_cap = None\n    if max_turnover is not None:\n        try:\n            mt = float(max_turnover)\n        except (TypeError, ValueError):\n            mt = None\n        if mt is not None and mt > 0:\n            turnover_cap = mt\n\n    effective_signal_spec = signal_spec or TrendSpec(\n        window=window_spec.length,\n        min_periods=None,\n        lag=1,\n        vol_adjust=False,\n        vol_target=None,\n        zscore=False,\n    )\n    signal_inputs = _scoped_signal_inputs()\n    if not signal_inputs.empty:\n        signal_frame = compute_trend_signals(signal_inputs, effective_signal_spec)\n    else:\n        signal_frame = pd.DataFrame(dtype=float)\n\n    try:\n        weights_series, risk_diagnostics = compute_constrained_weights(\n            base_series,\n            window.in_df[fund_cols],\n            window=window_spec,\n            target_vol=target_vol,\n            periods_per_year=window.periods_per_year,\n            floor_vol=min_floor if min_floor > 0 else None,\n            long_only=long_only,\n            max_weight=max_weight_val,\n            max_active_positions=max_active_val,\n            previous_weights=previous_weights,\n            lambda_tc=lambda_tc,\n            max_turnover=turnover_cap,\n            group_caps=group_caps_map,\n            groups=groups_map,\n        )\n    except Exception as exc:  # pragma: no cover - defensive fallback\n        logger.warning(\n            \"Risk controls failed; falling back to base weights: %s\", exc, exc_info=True\n        )\n        weights_series = base_series.copy()\n        asset_vol = realised_volatility(\n            window.in_df[fund_cols],\n            window_spec,\n            periods_per_year=window.periods_per_year,\n        )\n        latest_vol = asset_vol.iloc[-1].reindex(fund_cols)\n        latest_vol = latest_vol.ffill().bfill()\n        positive = latest_vol[latest_vol > 0]\n        fallback_vol = float(positive.min()) if not positive.empty else 1.0\n        latest_vol = latest_vol.fillna(fallback_vol)\n        if min_floor > 0:\n            latest_vol = latest_vol.clip(lower=min_floor)\n        if target_vol is None:\n            scale_factors = pd.Series(1.0, index=fund_cols, dtype=float)\n        else:\n            scale_factors = (\n                pd.Series(target_vol, index=fund_cols, dtype=float)\n                .div(latest_vol)\n                .replace([np.inf, -np.inf], 0.0)\n                .fillna(0.0)\n            )\n        scaled_returns = window.in_df[fund_cols].mul(scale_factors, axis=1)\n        portfolio_returns = scaled_returns.mul(weights_series, axis=1).sum(axis=1)\n        portfolio_vol = realised_volatility(\n            portfolio_returns.to_frame(\"portfolio\"),\n            window_spec,\n            periods_per_year=window.periods_per_year,\n        )[\"portfolio\"]\n        risk_diagnostics = RiskDiagnostics(\n            asset_volatility=asset_vol,\n            portfolio_volatility=portfolio_vol,\n            turnover=pd.Series(dtype=float, name=\"turnover\"),\n            turnover_value=float(\"nan\"),\n            scale_factors=scale_factors,\n        )\n\n    policy_cfg = dict(weight_policy or {})\n    policy_mode = str(policy_cfg.get(\"mode\", policy_cfg.get(\"policy\", \"drop\"))).lower()\n    min_assets_policy = int(policy_cfg.get(\"min_assets\", 1) or 0)\n\n    signal_snapshot: pd.Series | None = None\n    if not signal_frame.empty:\n        try:\n            target_index = (\n                window.out_df.index[0]\n                if len(window.out_df.index)\n                else signal_frame.index[-1]\n            )\n            aligned = signal_frame.reindex(columns=fund_cols)\n            if target_index in aligned.index:\n                signal_snapshot = aligned.loc[target_index]\n            elif not aligned.empty:\n                signal_snapshot = aligned.iloc[-1]\n        except Exception:  # pragma: no cover - defensive\n            signal_snapshot = None\n\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_16","type":"reasoning","text":"**Tracing max_turnover config flow**"}}
{"type":"item.started","item":{"id":"item_17","type":"command_execution","command":"/bin/bash -lc \"sed -n '2280,2605p' src/trend_analysis/pipeline.py\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_17","type":"command_execution","command":"/bin/bash -lc \"sed -n '2280,2605p' src/trend_analysis/pipeline.py\"","aggregated_output":"    return stats\n\n\ndef _run_analysis_with_diagnostics(\n    df: pd.DataFrame,\n    in_start: str,\n    in_end: str,\n    out_start: str,\n    out_end: str,\n    target_vol: float | None,\n    monthly_cost: float,\n    *,\n    floor_vol: float | None = None,\n    warmup_periods: int = 0,\n    selection_mode: str = \"all\",\n    random_n: int = 8,\n    custom_weights: dict[str, float] | None = None,\n    rank_kwargs: Mapping[str, Any] | None = None,\n    manual_funds: list[str] | None = None,\n    indices_list: list[str] | None = None,\n    benchmarks: dict[str, str] | None = None,\n    seed: int = 42,\n    stats_cfg: RiskStatsConfig | None = None,\n    weighting_scheme: str | None = None,\n    constraints: dict[str, Any] | None = None,\n    missing_policy: str | Mapping[str, str] | None = None,\n    missing_limit: int | Mapping[str, int | None] | None = None,\n    risk_window: Mapping[str, Any] | None = None,\n    periods_per_year_override: float | None = None,\n    previous_weights: Mapping[str, float] | None = None,\n    lambda_tc: float | None = None,\n    max_turnover: float | None = None,\n    signal_spec: TrendSpec | None = None,\n    regime_cfg: Mapping[str, Any] | None = None,\n    weight_policy: Mapping[str, Any] | None = None,\n    risk_free_column: str | None = None,\n    allow_risk_free_fallback: bool | None = None,\n    weight_engine_params: Mapping[str, Any] | None = None,\n) -> PipelineResult:\n    preprocess_stage = _prepare_preprocess_stage(\n        df,\n        floor_vol=floor_vol,\n        warmup_periods=warmup_periods,\n        missing_policy=missing_policy,\n        missing_limit=missing_limit,\n        stats_cfg=stats_cfg,\n        periods_per_year_override=periods_per_year_override,\n        allow_risk_free_fallback=allow_risk_free_fallback,\n    )\n    if isinstance(preprocess_stage, PipelineResult):\n        return preprocess_stage\n\n    window_stage = _build_sample_windows(\n        preprocess_stage,\n        in_start=in_start,\n        in_end=in_end,\n        out_start=out_start,\n        out_end=out_end,\n    )\n    if isinstance(window_stage, PipelineResult):\n        return window_stage\n\n    regime_label, regime_settings = _resolve_regime_label(\n        preprocess_stage,\n        window_stage,\n        regime_cfg,\n        benchmarks=benchmarks,\n    )\n    random_n, rank_kwargs = _apply_regime_overrides(\n        random_n=random_n,\n        rank_kwargs=rank_kwargs,\n        regime_label=regime_label,\n        settings=regime_settings,\n        regime_cfg=regime_cfg,\n    )\n    target_vol, constraints = _apply_regime_weight_overrides(\n        target_vol=target_vol,\n        constraints=constraints,\n        regime_label=regime_label,\n        settings=regime_settings,\n        regime_cfg=regime_cfg,\n    )\n\n    selection_stage = _select_universe(\n        preprocess_stage,\n        window_stage,\n        in_label=in_start,\n        in_end_label=in_end,\n        selection_mode=selection_mode,\n        random_n=random_n,\n        custom_weights=custom_weights,\n        rank_kwargs=rank_kwargs,\n        manual_funds=manual_funds,\n        indices_list=indices_list,\n        seed=seed,\n        stats_cfg=stats_cfg,\n        risk_free_column=risk_free_column,\n        allow_risk_free_fallback=allow_risk_free_fallback,\n    )\n    if isinstance(selection_stage, PipelineResult):\n        return selection_stage\n\n    stats_cfg_obj = stats_cfg or RiskStatsConfig(risk_free=0.0)\n    computation_stage = _compute_weights_and_stats(\n        preprocess_stage,\n        window_stage,\n        selection_stage,\n        target_vol=target_vol,\n        monthly_cost=monthly_cost,\n        custom_weights=custom_weights,\n        weighting_scheme=weighting_scheme,\n        constraints=constraints,\n        risk_window=risk_window,\n        previous_weights=previous_weights,\n        lambda_tc=lambda_tc,\n        max_turnover=max_turnover,\n        signal_spec=signal_spec,\n        weight_policy=weight_policy,\n        warmup=preprocess_stage.warmup,\n        min_floor=preprocess_stage.min_floor,\n        stats_cfg=stats_cfg_obj,\n        weight_engine_params=weight_engine_params,\n    )\n\n    return _assemble_analysis_output(\n        preprocess_stage,\n        window_stage,\n        selection_stage,\n        computation_stage,\n        benchmarks=benchmarks,\n        regime_cfg=regime_cfg,\n        target_vol=target_vol,\n        monthly_cost=monthly_cost,\n        min_floor=preprocess_stage.min_floor,\n    )\n\n\ndef _run_analysis(\n    df: pd.DataFrame,\n    in_start: str,\n    in_end: str,\n    out_start: str,\n    out_end: str,\n    target_vol: float | None,\n    monthly_cost: float,\n    *,\n    floor_vol: float | None = None,\n    warmup_periods: int = 0,\n    selection_mode: str = \"all\",\n    random_n: int = 8,\n    custom_weights: dict[str, float] | None = None,\n    rank_kwargs: Mapping[str, Any] | None = None,\n    manual_funds: list[str] | None = None,\n    indices_list: list[str] | None = None,\n    benchmarks: dict[str, str] | None = None,\n    seed: int = 42,\n    stats_cfg: RiskStatsConfig | None = None,\n    weighting_scheme: str | None = None,\n    constraints: dict[str, Any] | None = None,\n    missing_policy: str | Mapping[str, str] | None = None,\n    missing_limit: int | Mapping[str, int | None] | None = None,\n    risk_window: Mapping[str, Any] | None = None,\n    periods_per_year_override: float | None = None,\n    previous_weights: Mapping[str, float] | None = None,\n    lambda_tc: float | None = None,\n    max_turnover: float | None = None,\n    signal_spec: TrendSpec | None = None,\n    regime_cfg: Mapping[str, Any] | None = None,\n    weight_policy: Mapping[str, Any] | None = None,\n    risk_free_column: str | None = None,\n    allow_risk_free_fallback: bool | None = None,\n    weight_engine_params: Mapping[str, Any] | None = None,\n) -> AnalysisResult | None:\n    \"\"\"Backward-compatible wrapper returning raw payloads for tests.\"\"\"\n\n    result = _run_analysis_with_diagnostics(\n        df,\n        in_start,\n        in_end,\n        out_start,\n        out_end,\n        target_vol,\n        monthly_cost,\n        floor_vol=floor_vol,\n        warmup_periods=warmup_periods,\n        selection_mode=selection_mode,\n        random_n=random_n,\n        custom_weights=custom_weights,\n        rank_kwargs=rank_kwargs,\n        manual_funds=manual_funds,\n        indices_list=indices_list,\n        benchmarks=benchmarks,\n        seed=seed,\n        stats_cfg=stats_cfg,\n        weighting_scheme=weighting_scheme,\n        constraints=constraints,\n        missing_policy=missing_policy,\n        missing_limit=missing_limit,\n        risk_window=risk_window,\n        periods_per_year_override=periods_per_year_override,\n        previous_weights=previous_weights,\n        lambda_tc=lambda_tc,\n        max_turnover=max_turnover,\n        signal_spec=signal_spec,\n        regime_cfg=regime_cfg,\n        weight_policy=weight_policy,\n        risk_free_column=risk_free_column,\n        allow_risk_free_fallback=allow_risk_free_fallback,\n        weight_engine_params=weight_engine_params,\n    )\n    return result.unwrap()\n\n\n_DEFAULT_RUN_ANALYSIS = _run_analysis\n\n\ndef _invoke_analysis_with_diag(*args: Any, **kwargs: Any) -> PipelineResult:\n    \"\"\"Call the patched analysis hook and normalise into a PipelineResult.\"\"\"\n\n    if _run_analysis is _DEFAULT_RUN_ANALYSIS:\n        return _run_analysis_with_diagnostics(*args, **kwargs)\n    patched_result = _run_analysis(*args, **kwargs)\n    if isinstance(patched_result, PipelineResult):\n        return patched_result\n    if isinstance(patched_result, DiagnosticResult):\n        return PipelineResult(\n            value=patched_result.value,\n            diagnostic=patched_result.diagnostic,\n        )\n    return PipelineResult(value=patched_result, diagnostic=None)\n\n\ndef run_analysis(\n    df: pd.DataFrame,\n    in_start: str,\n    in_end: str,\n    out_start: str,\n    out_end: str,\n    target_vol: float | None,\n    monthly_cost: float,\n    *,\n    floor_vol: float | None = None,\n    warmup_periods: int = 0,\n    selection_mode: str = \"all\",\n    random_n: int = 8,\n    custom_weights: dict[str, float] | None = None,\n    rank_kwargs: Mapping[str, Any] | None = None,\n    manual_funds: list[str] | None = None,\n    indices_list: list[str] | None = None,\n    benchmarks: dict[str, str] | None = None,\n    seed: int = 42,\n    stats_cfg: RiskStatsConfig | None = None,\n    weighting_scheme: str | None = None,\n    constraints: dict[str, Any] | None = None,\n    missing_policy: str | Mapping[str, str] | None = None,\n    missing_limit: int | Mapping[str, int | None] | None = None,\n    risk_window: Mapping[str, Any] | None = None,\n    periods_per_year: float | None = None,\n    previous_weights: Mapping[str, float] | None = None,\n    lambda_tc: float | None = None,\n    max_turnover: float | None = None,\n    signal_spec: TrendSpec | None = None,\n    regime_cfg: Mapping[str, Any] | None = None,\n    calendar_frequency: str | None = None,\n    calendar_timezone: str | None = None,\n    holiday_calendar: str | None = None,\n    weight_policy: Mapping[str, Any] | None = None,\n    risk_free_column: str | None = None,\n    allow_risk_free_fallback: bool | None = None,\n    weight_engine_params: Mapping[str, Any] | None = None,\n) -> PipelineResult:\n    \"\"\"Diagnostics-aware wrapper mirroring ``_run_analysis``.\"\"\"\n    if any(\n        value is not None\n        for value in (calendar_frequency, calendar_timezone, holiday_calendar)\n    ):\n        df = df.copy()\n        calendar_settings = dict(getattr(df, \"attrs\", {}).get(\"calendar_settings\", {}))\n        if calendar_frequency is not None:\n            calendar_settings[\"frequency\"] = calendar_frequency\n        if calendar_timezone is not None:\n            calendar_settings[\"timezone\"] = calendar_timezone\n        if holiday_calendar is not None:\n            calendar_settings[\"holiday_calendar\"] = holiday_calendar\n        df.attrs[\"calendar_settings\"] = calendar_settings\n    return _invoke_analysis_with_diag(\n        df,\n        in_start,\n        in_end,\n        out_start,\n        out_end,\n        target_vol,\n        monthly_cost,\n        floor_vol=floor_vol,\n        warmup_periods=warmup_periods,\n        selection_mode=selection_mode,\n        random_n=random_n,\n        custom_weights=custom_weights,\n        rank_kwargs=rank_kwargs,\n        manual_funds=manual_funds,\n        indices_list=indices_list,\n        benchmarks=benchmarks,\n        seed=seed,\n        stats_cfg=stats_cfg,\n        weighting_scheme=weighting_scheme,\n        constraints=constraints,\n        missing_policy=missing_policy,\n        missing_limit=missing_limit,\n        risk_window=risk_window,\n        periods_per_year_override=periods_per_year,\n        previous_weights=previous_weights,\n        lambda_tc=lambda_tc,\n        max_turnover=max_turnover,\n        signal_spec=signal_spec,\n        regime_cfg=regime_cfg,\n        weight_policy=weight_policy,\n        risk_free_column=risk_free_column,\n        allow_risk_free_fallback=allow_risk_free_fallback,\n        weight_engine_params=weight_engine_params,\n    )\n\n\ndef _attach_calendar_settings(df: pd.DataFrame, cfg: Config) -> None:\n    preprocessing_section = _cfg_section(cfg, \"preprocessing\")\n    data_settings = _cfg_section(cfg, \"data\")\n    data_frequency = _section_get(data_settings, \"frequency\")\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_18","type":"reasoning","text":"**Tracing config usage**"}}
{"type":"item.started","item":{"id":"item_19","type":"command_execution","command":"/bin/bash -lc \"sed -n '2640,2875p' src/trend_analysis/pipeline.py\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_19","type":"command_execution","command":"/bin/bash -lc \"sed -n '2640,2875p' src/trend_analysis/pipeline.py\"","aggregated_output":"\n    split_cfg = _cfg_section(cfg, \"sample_split\")\n    resolved_split = _resolve_sample_split(df, split_cfg)\n    metrics_section = _cfg_section(cfg, \"metrics\")\n    metrics_list = _section_get(metrics_section, \"registry\")\n    stats_cfg = None\n    if metrics_list:\n        from .core.rank_selection import RiskStatsConfig, canonical_metric_list\n\n        stats_cfg = RiskStatsConfig(\n            metrics_to_run=canonical_metric_list(metrics_list),\n            risk_free=0.0,\n        )\n\n    missing_section = _section_get(preprocessing_section, \"missing_data\")\n    if not isinstance(missing_section, Mapping):\n        missing_section = None\n    policy_spec, limit_spec = _policy_from_config(\n        missing_section if isinstance(missing_section, Mapping) else None\n    )\n\n    vol_adjust = _cfg_section(cfg, \"vol_adjust\")\n    run_settings = _cfg_section(cfg, \"run\")\n    portfolio_cfg = _cfg_section(cfg, \"portfolio\")\n    weighting_scheme = _section_get(portfolio_cfg, \"weighting_scheme\", \"equal\")\n    robustness_cfg = _section_get(portfolio_cfg, \"robustness\")\n    if not isinstance(robustness_cfg, Mapping):\n        robustness_cfg = _cfg_section(cfg, \"robustness\")\n    weight_engine_params = weight_engine_params_from_robustness(\n        weighting_scheme, robustness_cfg\n    )\n    trend_spec = _build_trend_spec(cfg, vol_adjust)\n    lambda_tc_val = _section_get(portfolio_cfg, \"lambda_tc\", 0.0)\n    risk_free_column = _section_get(data_settings, \"risk_free_column\")\n    allow_risk_free_fallback = _section_get(data_settings, \"allow_risk_free_fallback\")\n\n    diag_res = _invoke_analysis_with_diag(\n        df,\n        resolved_split[\"in_start\"],\n        resolved_split[\"in_end\"],\n        resolved_split[\"out_start\"],\n        resolved_split[\"out_end\"],\n        _resolve_target_vol(vol_adjust),\n        _section_get(run_settings, \"monthly_cost\", 0.0),\n        floor_vol=_section_get(vol_adjust, \"floor_vol\"),\n        warmup_periods=int(_section_get(vol_adjust, \"warmup_periods\", 0) or 0),\n        selection_mode=_section_get(portfolio_cfg, \"selection_mode\", \"all\"),\n        random_n=_section_get(portfolio_cfg, \"random_n\", 8),\n        custom_weights=_section_get(portfolio_cfg, \"custom_weights\"),\n        rank_kwargs=_section_get(portfolio_cfg, \"rank\"),\n        manual_funds=_section_get(portfolio_cfg, \"manual_list\"),\n        indices_list=_section_get(portfolio_cfg, \"indices_list\"),\n        benchmarks=_cfg_value(cfg, \"benchmarks\"),\n        seed=_cfg_value(cfg, \"seed\", 42),\n        weighting_scheme=weighting_scheme,\n        constraints=_section_get(portfolio_cfg, \"constraints\"),\n        stats_cfg=stats_cfg,\n        missing_policy=policy_spec,\n        missing_limit=limit_spec,\n        risk_window=_section_get(vol_adjust, \"window\"),\n        previous_weights=_section_get(portfolio_cfg, \"previous_weights\"),\n        lambda_tc=lambda_tc_val,\n        max_turnover=_section_get(portfolio_cfg, \"max_turnover\"),\n        signal_spec=trend_spec,\n        regime_cfg=_cfg_section(cfg, \"regime\"),\n        weight_policy=_section_get(portfolio_cfg, \"weight_policy\"),\n        risk_free_column=risk_free_column,\n        allow_risk_free_fallback=allow_risk_free_fallback,\n        weight_engine_params=weight_engine_params,\n    )\n    diag = diag_res.diagnostic\n    if diag_res.value is None:\n        if diag:\n            logger.warning(\n                \"pipeline.run aborted (%s): %s\",\n                diag.reason_code,\n                diag.message,\n            )\n        empty = pd.DataFrame()\n        if diag:\n            empty.attrs[\"diagnostic\"] = diag\n        return empty\n\n    res = diag_res.value\n    stats = cast(dict[str, _Stats], res[\"out_sample_stats\"])\n    df = pd.DataFrame({k: vars(v) for k, v in stats.items()}).T\n    for label, ir_map in cast(\n        dict[str, dict[str, float]], res.get(\"benchmark_ir\", {})\n    ).items():\n        col = f\"ir_{label}\"\n        df[col] = pd.Series(\n            {\n                k: v\n                for k, v in ir_map.items()\n                if k not in {\"equal_weight\", \"user_weight\"}\n            }\n        )\n    if diag:\n        df.attrs[\"diagnostic\"] = diag\n    return df\n\n\ndef run_full(cfg: Config) -> PipelineResult:\n    \"\"\"Return the full analysis results (with diagnostics) based on ``cfg``.\"\"\"\n    cfg = _unwrap_cfg(cfg)\n    preprocessing_section = _cfg_section(cfg, \"preprocessing\")\n    data_settings = _cfg_section(cfg, \"data\")\n    csv_path = _section_get(data_settings, \"csv_path\")\n    if csv_path is None:\n        raise KeyError(\"cfg.data['csv_path'] must be provided\")\n\n    missing_policy_cfg = _section_get(data_settings, \"missing_policy\")\n    if missing_policy_cfg is None:\n        missing_policy_cfg = _section_get(data_settings, \"nan_policy\")\n    missing_limit_cfg = _section_get(data_settings, \"missing_limit\")\n    if missing_limit_cfg is None:\n        missing_limit_cfg = _section_get(data_settings, \"nan_limit\")\n\n    df = load_csv(\n        csv_path,\n        errors=\"raise\",\n        missing_policy=missing_policy_cfg,\n        missing_limit=missing_limit_cfg,\n    )\n    df = cast(pd.DataFrame, df)\n\n    _attach_calendar_settings(df, cfg)\n\n    split_cfg = _cfg_section(cfg, \"sample_split\")\n    resolved_split = _resolve_sample_split(df, split_cfg)\n    metrics_section = _cfg_section(cfg, \"metrics\")\n    metrics_list = _section_get(metrics_section, \"registry\")\n    stats_cfg = None\n    if metrics_list:\n        from .core.rank_selection import RiskStatsConfig, canonical_metric_list\n\n        stats_cfg = RiskStatsConfig(\n            metrics_to_run=canonical_metric_list(metrics_list),\n            risk_free=0.0,\n        )\n\n    missing_section = _section_get(preprocessing_section, \"missing_data\")\n    if not isinstance(missing_section, Mapping):\n        missing_section = None\n    policy_spec, limit_spec = _policy_from_config(\n        missing_section if isinstance(missing_section, Mapping) else None\n    )\n\n    vol_adjust = _cfg_section(cfg, \"vol_adjust\")\n    run_settings = _cfg_section(cfg, \"run\")\n    portfolio_cfg = _cfg_section(cfg, \"portfolio\")\n    weighting_scheme = _section_get(portfolio_cfg, \"weighting_scheme\", \"equal\")\n    robustness_cfg = _section_get(portfolio_cfg, \"robustness\")\n    if not isinstance(robustness_cfg, Mapping):\n        robustness_cfg = _cfg_section(cfg, \"robustness\")\n    weight_engine_params = weight_engine_params_from_robustness(\n        weighting_scheme, robustness_cfg\n    )\n    risk_free_column = _section_get(data_settings, \"risk_free_column\")\n    trend_spec = _build_trend_spec(cfg, vol_adjust)\n    lambda_tc_val = _section_get(portfolio_cfg, \"lambda_tc\", 0.0)\n    risk_free_column = _section_get(data_settings, \"risk_free_column\")\n    allow_risk_free_fallback = _section_get(data_settings, \"allow_risk_free_fallback\")\n\n    diag_res = _invoke_analysis_with_diag(\n        df,\n        resolved_split[\"in_start\"],\n        resolved_split[\"in_end\"],\n        resolved_split[\"out_start\"],\n        resolved_split[\"out_end\"],\n        _resolve_target_vol(vol_adjust),\n        _section_get(run_settings, \"monthly_cost\", 0.0),\n        floor_vol=_section_get(vol_adjust, \"floor_vol\"),\n        warmup_periods=int(_section_get(vol_adjust, \"warmup_periods\", 0) or 0),\n        selection_mode=_section_get(portfolio_cfg, \"selection_mode\", \"all\"),\n        random_n=_section_get(portfolio_cfg, \"random_n\", 8),\n        custom_weights=_section_get(portfolio_cfg, \"custom_weights\"),\n        rank_kwargs=_section_get(portfolio_cfg, \"rank\"),\n        manual_funds=_section_get(portfolio_cfg, \"manual_list\"),\n        indices_list=_section_get(portfolio_cfg, \"indices_list\"),\n        benchmarks=_cfg_value(cfg, \"benchmarks\"),\n        seed=_cfg_value(cfg, \"seed\", 42),\n        weighting_scheme=weighting_scheme,\n        constraints=_section_get(portfolio_cfg, \"constraints\"),\n        stats_cfg=stats_cfg,\n        missing_policy=policy_spec,\n        missing_limit=limit_spec,\n        risk_window=_section_get(vol_adjust, \"window\"),\n        previous_weights=_section_get(portfolio_cfg, \"previous_weights\"),\n        lambda_tc=lambda_tc_val,\n        max_turnover=_section_get(portfolio_cfg, \"max_turnover\"),\n        signal_spec=trend_spec,\n        regime_cfg=_cfg_section(cfg, \"regime\"),\n        weight_policy=_section_get(portfolio_cfg, \"weight_policy\"),\n        risk_free_column=risk_free_column,\n        allow_risk_free_fallback=allow_risk_free_fallback,\n        weight_engine_params=weight_engine_params,\n    )\n    diag = diag_res.diagnostic\n    if diag_res.value is None:\n        if diag:\n            logger.warning(\n                \"pipeline.run_full aborted (%s): %s\",\n                diag.reason_code,\n                diag.message,\n            )\n        else:\n            logger.warning(\"pipeline.run_full aborted with no diagnostic context\")\n    return diag_res\n\n\n# --- Shift-safe helpers ----------------------------------------------------\n\n\ndef compute_signal(\n    df: pd.DataFrame,\n    *,\n    column: str = \"returns\",\n    window: int = 3,\n    min_periods: int | None = None,\n) -> pd.Series:\n    \"\"\"Return a trailing rolling-mean signal using information strictly prior\n    to the current row.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing the data.\n        column (str, optional): Name of the column to compute the signal from.\n            Defaults to \"returns\".\n        window (int, optional): Size of the trailing window for the rolling mean.\n            Must be positive. Defaults to 3.\n        min_periods (int or None, optional): Minimum number of observations in window\n            required to have a value (otherwise result is NaN). If None, defaults to\n            the value of `window`.\n\n    Returns:\n        pd.Series: A float Series containing the strictly causal (lookâ€‘back only)\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_20","type":"reasoning","text":"**Tracing max_turnover through pipeline**\n\nIâ€™m focusing on verifying that the config max_turnover properly reaches the risk moduleâ€™s _enforce_turnover_cap by adding a test that monkeypatches it to capture the value during a pipeline run. I plan to use existing pipeline tests and run helpers to confirm the data flow end-to-end before tackling the turnover constraint bypass scenario."}}
{"type":"item.started","item":{"id":"item_21","type":"command_execution","command":"/bin/bash -lc 'rg -n \"run_full|run_analysis|_invoke_analysis_with_diag|pipeline.run\" tests'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_21","type":"command_execution","command":"/bin/bash -lc 'rg -n \"run_full|run_analysis|_invoke_analysis_with_diag|pipeline.run\" tests'","aggregated_output":"tests/test_constants.py:7:from trend_analysis import cli, run_analysis, run_multi_analysis\ntests/test_constants.py:34:def test_constants_are_used_in_run_analysis():\ntests/test_constants.py:35:    \"\"\"Test that run_analysis uses the constants.\"\"\"\ntests/test_constants.py:37:    assert hasattr(run_analysis, \"DEFAULT_OUTPUT_DIRECTORY\")\ntests/test_constants.py:38:    assert hasattr(run_analysis, \"DEFAULT_OUTPUT_FORMATS\")\ntests/test_constants.py:39:    assert run_analysis.DEFAULT_OUTPUT_DIRECTORY == DEFAULT_OUTPUT_DIRECTORY\ntests/test_constants.py:40:    assert run_analysis.DEFAULT_OUTPUT_FORMATS == DEFAULT_OUTPUT_FORMATS\ntests/test_constants.py:96:    rc = run_analysis.main([\"-c\", str(cfg)])\ntests/test_multi_period_engine_branch_coverage.py:174:def _stub_run_analysis(*_: Any, **__: Any) -> Dict[str, Any]:\ntests/test_multi_period_engine_branch_coverage.py:218:    monkeypatch.setattr(mp_engine, \"_run_analysis\", _stub_run_analysis)\ntests/test_multi_period_engine_branch_coverage.py:284:    def fake_run_analysis(*args: Any, **kwargs: Any) -> Dict[str, Any] | None:\ntests/test_multi_period_engine_branch_coverage.py:287:    monkeypatch.setattr(mp_engine, \"_run_analysis\", fake_run_analysis)\ntests/test_multi_period_engine_branch_coverage.py:351:    monkeypatch.setattr(mp_engine, \"_run_analysis\", _stub_run_analysis)\ntests/test_pipeline_entrypoints.py:25:        pipeline.run({})\ntests/test_pipeline_entrypoints.py:88:    def fake_run_analysis(*args, **kwargs):\ntests/test_pipeline_entrypoints.py:99:    monkeypatch.setattr(pipeline, \"_run_analysis\", fake_run_analysis)\ntests/test_pipeline_entrypoints.py:101:    result = pipeline.run(base_config)\ntests/test_pipeline_entrypoints.py:130:    monkeypatch.setattr(pipeline, \"_run_analysis\", lambda *_, **__: None)\ntests/test_pipeline_entrypoints.py:132:    result = pipeline.run(base_config)\ntests/test_pipeline_entrypoints.py:136:def test_run_full_propagates_analysis_payload(\ntests/test_pipeline_entrypoints.py:155:    monkeypatch.setattr(pipeline, \"_run_analysis\", lambda *_, **__: payload)\ntests/test_pipeline_entrypoints.py:157:    result = pipeline.run_full(base_config)\ntests/test_pipeline_entrypoints.py:161:def test_run_full_returns_empty_when_analysis_none(\ntests/test_pipeline_entrypoints.py:174:    monkeypatch.setattr(pipeline, \"_run_analysis\", lambda *_, **__: None)\ntests/test_pipeline_entrypoints.py:176:    result = pipeline.run_full(base_config)\ntests/test_pipeline_entrypoints.py:180:def test_empty_run_full_result_template() -> None:\ntests/test_pipeline_entrypoints.py:181:    payload = pipeline._empty_run_full_result()\ntests/test_vol_floor_and_warmup.py:7:from trend_analysis.pipeline import _run_analysis\ntests/test_vol_floor_and_warmup.py:33:    res = _run_analysis(\ntests/test_vol_floor_and_warmup.py:52:    res = _run_analysis(\ntests/test_vol_floor_and_warmup.py:79:    res = _run_analysis(\ntests/test_vol_floor_and_warmup.py:101:    baseline = _run_analysis(\ntests/test_vol_floor_and_warmup.py:113:    clamped = _run_analysis(\ntests/test_app_coverage.py:1373:        \"run_full\",\ntests/test_app_coverage.py:1495:        \"run_full\",\ntests/test_app_coverage.py:1496:        lambda cfg: pytest.fail(\"run_full should not execute\"),\ntests/test_app_coverage.py:1508:    assert run_calls, \"Expected pipeline.run to be invoked\"\ntests/test_app_coverage.py:1556:        \"run_full\",\ntests/test_app_coverage.py:1557:        lambda cfg: pytest.fail(\"run_full should not run\"),\ntests/test_api_run_simulation_extra.py:49:    monkeypatch.setattr(api, \"_run_analysis\", lambda *a, **k: None)\ntests/test_api_run_simulation_extra.py:67:    def _capture_run_analysis(*_args, **kwargs):\ntests/test_api_run_simulation_extra.py:71:    monkeypatch.setattr(api, \"_run_analysis\", _capture_run_analysis)\ntests/test_api_run_simulation_extra.py:112:    monkeypatch.setattr(api, \"_run_analysis\", lambda *a, **k: payload)\ntests/test_api_run_simulation_extra.py:136:    monkeypatch.setattr(api, \"_run_analysis\", lambda *a, **k: object())\ntests/test_api_run_simulation_extra.py:171:    monkeypatch.setattr(api, \"_run_analysis\", lambda *a, **k: payload)\ntests/test_default_export.py:5:from trend_analysis import run_analysis\ntests/test_default_export.py:38:    rc = run_analysis.main([\"-c\", str(cfg)])\ntests/test_pipeline_run_cache_fallbacks.py:55:    def fake_run_analysis(*args, **kwargs):\ntests/test_pipeline_run_cache_fallbacks.py:64:    monkeypatch.setattr(pipeline, \"_run_analysis\", fake_run_analysis)\ntests/test_pipeline_run_cache_fallbacks.py:85:    result = pipeline.run(cfg)\ntests/test_pipeline_run_cache_fallbacks.py:94:def test_run_full_uses_nan_fallbacks(monkeypatch: pytest.MonkeyPatch) -> None:\ntests/test_pipeline_run_cache_fallbacks.py:115:    monkeypatch.setattr(pipeline, \"_run_analysis\", lambda *a, **k: payload)\ntests/test_pipeline_run_cache_fallbacks.py:136:    result = pipeline.run_full(cfg)\ntests/test_pipeline_run_cache_fallbacks.py:146:        pipeline.run({\"data\": {}})\ntests/test_pipeline_run_cache_fallbacks.py:149:def test_run_full_requires_csv_path() -> None:\ntests/test_pipeline_run_cache_fallbacks.py:151:        pipeline.run_full({\"data\": {}})\ntests/test_pipeline_run_cache_fallbacks.py:170:        pipeline, \"_run_analysis\", lambda *a, **k: pipeline._empty_run_full_result()\ntests/test_pipeline_run_cache_fallbacks.py:187:    result = pipeline.run(cfg)\ntests/test_pipeline_run_cache_fallbacks.py:193:def test_run_full_handles_missing_data_section(monkeypatch: pytest.MonkeyPatch) -> None:\ntests/test_pipeline_run_cache_fallbacks.py:207:    def fake_run_analysis(*args, **kwargs):\ntests/test_pipeline_run_cache_fallbacks.py:214:    monkeypatch.setattr(pipeline, \"_run_analysis\", fake_run_analysis)\ntests/test_pipeline_run_cache_fallbacks.py:228:    payload = pipeline.run_full(cfg)\ntests/test_pipeline_run_cache_fallbacks.py:272:    empty = pipeline._empty_run_full_result()\ntests/test_pipeline_run_cache_fallbacks.py:417:def test_run_analysis_rank_branch_with_fallbacks(\ntests/test_pipeline_run_cache_fallbacks.py:480:    result = pipeline._run_analysis(\ntests/test_pipeline_run_cache_fallbacks.py:504:def test_run_analysis_risk_window_zero_length(monkeypatch: pytest.MonkeyPatch) -> None:\ntests/test_pipeline_run_cache_fallbacks.py:553:    result = pipeline._run_analysis(\ntests/test_pipeline_run_cache_fallbacks.py:594:def test_run_analysis_returns_none_when_no_value_columns(\ntests/test_pipeline_run_cache_fallbacks.py:616:    result = pipeline._run_analysis(\ntests/test_multi_period_engine_additional.py:136:    def fake_run_analysis(\ntests/test_multi_period_engine_additional.py:142:    monkeypatch.setattr(engine, \"_run_analysis\", fake_run_analysis)\ntests/test_multi_period_engine_additional.py:222:    monkeypatch.setattr(engine, \"_run_analysis\", lambda *a, **k: {\"ok\": True})\ntests/test_multi_period_engine_additional.py:264:    def fake_run_analysis(\ntests/test_multi_period_engine_additional.py:270:    monkeypatch.setattr(engine, \"_run_analysis\", fake_run_analysis)\ntests/test_multi_period_engine_additional.py:338:    def fake_run_analysis(\ntests/test_multi_period_engine_additional.py:343:    monkeypatch.setattr(engine, \"_run_analysis\", fake_run_analysis)\ntests/test_multi_period_engine_additional.py:562:    def fake_run_analysis(\ntests/test_multi_period_engine_additional.py:575:    monkeypatch.setattr(engine, \"_run_analysis\", fake_run_analysis)\ntests/test_multi_period_engine_additional.py:702:    def fake_run_analysis(\ntests/test_multi_period_engine_additional.py:711:    monkeypatch.setattr(engine, \"_run_analysis\", fake_run_analysis)\ntests/test_multi_period_engine_additional.py:814:    def fake_run_analysis(\ntests/test_multi_period_engine_additional.py:842:    monkeypatch.setattr(engine, \"_run_analysis\", fake_run_analysis)\ntests/test_multi_period_engine_additional.py:991:    def fake_run_analysis(\ntests/test_multi_period_engine_additional.py:1002:    monkeypatch.setattr(engine, \"_run_analysis\", fake_run_analysis)\ntests/test_multi_period_engine_additional.py:1061:    def fake_run_analysis(\ntests/test_multi_period_engine_additional.py:1077:    monkeypatch.setattr(engine, \"_run_analysis\", fake_run_analysis)\ntests/test_multi_period_engine_additional.py:1255:    def fake_run_analysis(\ntests/test_multi_period_engine_additional.py:1272:    monkeypatch.setattr(engine, \"_run_analysis\", fake_run_analysis)\ntests/test_weight_engine_logging.py:32:        result = pipeline._run_analysis(\ntests/test_weight_engine_logging.py:61:        result = pipeline._run_analysis(\ntests/test_weight_engine_logging.py:110:            result = pipeline._run_analysis(\ntests/test_weight_engine_logging.py:145:        result = pipeline._run_analysis(\ntests/test_weight_engine_logging.py:171:        result = pipeline._run_analysis(\ntests/test_weight_engine_logging.py:199:        result = pipeline._run_analysis(\ntests/test_multi_period_engine_incremental_extra.py:83:    monkeypatch.setattr(mp_engine, \"_run_analysis\", fake_run)\ntests/test_multi_period_engine_incremental_extra.py:149:    monkeypatch.setattr(mp_engine, \"_run_analysis\", fake_run)\ntests/test_multi_period_engine_incremental_extra.py:206:    monkeypatch.setattr(mp_engine, \"_run_analysis\", fake_run)\ntests/test_multi_period_engine_incremental_extra.py:258:    monkeypatch.setattr(mp_engine, \"_run_analysis\", fake_run)\ntests/test_gui_launch_run.py:25:    monkeypatch.setattr(app.pipeline, \"run_full\", lambda cfg: {\"result\": 1})\ntests/test_gui_launch_run.py:63:    monkeypatch.setattr(app.pipeline, \"run_full\", lambda cfg: {\"full\": \"results\"})\ntests/test_pipeline_optional_features.py:112:def test_run_analysis_na_tolerant_filtering_preserves_funds() -> None:\ntests/test_pipeline_optional_features.py:125:    result = pipeline._run_analysis(\ntests/test_pipeline_optional_features.py:143:def test_run_analysis_na_tolerant_filtering_drops_excessive_gaps() -> None:\ntests/test_pipeline_optional_features.py:156:    result = pipeline._run_analysis(\ntests/test_pipeline_optional_features.py:181:    base = pipeline._run_analysis(\ntests/test_pipeline_optional_features.py:195:    enabled = pipeline._run_analysis(\ntests/test_pipeline_optional_features.py:221:    spx = pipeline._run_analysis(\ntests/test_pipeline_optional_features.py:234:    acwi = pipeline._run_analysis(\ntests/test_pipeline_optional_features.py:265:    result = pipeline._run_analysis(\ntests/test_pipeline_optional_features.py:293:    baseline = pipeline._run_analysis(\ntests/test_pipeline_optional_features.py:307:    enabled = pipeline._run_analysis(\ntests/test_pipeline_optional_features.py:343:    base = pipeline._run_analysis(\ntests/test_pipeline_optional_features.py:356:    enabled = pipeline._run_analysis(\ntests/test_pipeline_optional_features.py:377:    base = pipeline._run_analysis(\ntests/test_pipeline_optional_features.py:390:    enabled = pipeline._run_analysis(\ntests/test_pipeline_optional_features.py:415:    base = pipeline._run_analysis(\ntests/test_pipeline_optional_features.py:428:    enabled = pipeline._run_analysis(\ntests/test_pipeline_optional_features.py:453:def test_run_analysis_avg_corr_metrics_populate_stats() -> None:\ntests/test_pipeline_optional_features.py:459:    result = pipeline._run_analysis(\ntests/test_pipeline_optional_features.py:483:def test_run_analysis_skips_avg_corr_for_single_fund() -> None:\ntests/test_pipeline_optional_features.py:489:    result = pipeline._run_analysis(\ntests/test_pipeline_optional_features.py:510:def test_run_analysis_does_not_duplicate_existing_avg_corr(\ntests/test_pipeline_optional_features.py:528:    result = pipeline._run_analysis(\ntests/test_pipeline_optional_features.py:548:def test_run_analysis_avg_corr_corr_failure(monkeypatch: pytest.MonkeyPatch) -> None:\ntests/test_pipeline_optional_features.py:564:    result = pipeline._run_analysis(\ntests/test_pipeline_optional_features.py:585:def test_run_analysis_constraint_failure_falls_back(\ntests/test_pipeline_optional_features.py:597:    result = pipeline._run_analysis(\ntests/test_pipeline_optional_features.py:619:def test_run_analysis_applies_constraints_on_success(\ntests/test_pipeline_optional_features.py:634:    result = pipeline._run_analysis(\ntests/test_pipeline_optional_features.py:657:def test_run_analysis_constraint_violation_fallback(\ntests/test_pipeline_optional_features.py:669:    result = pipeline._run_analysis(\ntests/test_pipeline_optional_features.py:691:def test_run_analysis_constraint_missing_groups_fallback(\ntests/test_pipeline_optional_features.py:703:    result = pipeline._run_analysis(\ntests/test_pipeline_optional_features.py:725:def test_run_analysis_benchmark_ir_best_effort(monkeypatch: pytest.MonkeyPatch) -> None:\ntests/test_pipeline_optional_features.py:749:    result = pipeline._run_analysis(\ntests/test_pipeline_optional_features.py:771:def test_run_analysis_benchmark_ir_handles_scalar_output(\ntests/test_pipeline_optional_features.py:782:    result = pipeline._run_analysis(\ntests/test_pipeline_optional_features.py:804:def test_run_analysis_benchmark_ir_populates_portfolio_entries() -> None:\ntests/test_pipeline_optional_features.py:808:    result = pipeline._run_analysis(\ntests/test_pipeline_optional_features.py:830:def test_run_analysis_benchmark_ir_handles_scalar_response(\ntests/test_pipeline_optional_features.py:848:    result = pipeline._run_analysis(\ntests/test_pipeline_optional_features.py:872:def test_run_analysis_constraints_missing_groups_fallbacks() -> None:\ntests/test_pipeline_optional_features.py:878:    result = pipeline._run_analysis(\ntests/test_pipeline_optional_features.py:901:def test_run_analysis_benchmark_ir_non_numeric_enrichment(\ntests/test_pipeline_optional_features.py:928:    result = pipeline._run_analysis(\ntests/test_run_analysis_cli_default.py:5:from trend_analysis import run_analysis\ntests/test_run_analysis_cli_default.py:37:    rc = run_analysis.main([\"-c\", str(cfg)])\ntests/test_multi_period_engine_incremental_fallback.py:111:    def fake_run_analysis(*args: Any, **kwargs: Any) -> Dict[str, Any]:\ntests/test_multi_period_engine_incremental_fallback.py:115:    monkeypatch.setattr(mp_engine, \"_run_analysis\", fake_run_analysis)\ntests/test_multi_period_engine_incremental_fallback.py:142:    # run_analysis invoked once per period and covariance recomputed twice\ntests/test_cli.py:146:    monkeypatch.setattr(cli.pipeline, \"run_full\", lambda cfg: {\"dummy\": 1})\ntests/test_cli.py:163:    monkeypatch.setattr(cli.pipeline, \"run_full\", lambda cfg: None)\ntests/test_cli.py:266:    monkeypatch.setattr(cli.pipeline, \"run_full\", lambda cfg: results_payload)\ntests/test_multi_period_sticky_periods.py:105:    monkeypatch.setattr(mp_engine, \"_run_analysis\", lambda *args, **kwargs: {})\ntests/test_api_run_simulation.py:103:    expected_details = pipeline.run_full(cfg).unwrap()\ntests/test_api_run_simulation.py:105:    expected_metrics = pipeline.run(cfg)\ntests/test_threshold_hold_alignment.py:106:    # Minimal stub for _run_analysis to avoid heavy pipeline work\ntests/test_threshold_hold_alignment.py:107:    def fake_run_analysis(*_args, **_kwargs):\ntests/test_threshold_hold_alignment.py:110:    monkeypatch_ctx.setattr(mp_engine, \"_run_analysis\", fake_run_analysis)\ntests/test_run_analysis_cli_coverage.py:1:\"\"\"Coverage-focused tests for ``trend_analysis.run_analysis`` CLI entry point.\"\"\"\ntests/test_run_analysis_cli_coverage.py:11:from trend_analysis import run_analysis\ntests/test_run_analysis_cli_coverage.py:69:    monkeypatch.setattr(run_analysis, \"load\", lambda _: cfg)\ntests/test_run_analysis_cli_coverage.py:70:    monkeypatch.setattr(run_analysis, \"load_csv\", _stub_load_csv)\ntests/test_run_analysis_cli_coverage.py:72:        run_analysis, \"api\", SimpleNamespace(run_simulation=_stub_run_simulation)\ntests/test_run_analysis_cli_coverage.py:85:        run_analysis.export, \"format_summary_text\", _format_summary_text\ntests/test_run_analysis_cli_coverage.py:87:    monkeypatch.setattr(run_analysis.export, \"export_data\", _export_data)\ntests/test_run_analysis_cli_coverage.py:89:        run_analysis.export, \"make_summary_formatter\", lambda *_, **__: lambda df: df\ntests/test_run_analysis_cli_coverage.py:92:    exit_code = run_analysis.main([\"-c\", \"config.yml\"])\ntests/test_run_analysis_cli_coverage.py:105:        run_analysis,\ntests/test_run_analysis_cli_coverage.py:110:        run_analysis.main([\"--config\", \"config.yml\"])\ntests/test_run_analysis_cli_coverage.py:117:    monkeypatch.setattr(run_analysis, \"load\", lambda _: cfg)\ntests/test_run_analysis_cli_coverage.py:118:    monkeypatch.setattr(run_analysis, \"load_csv\", lambda *_args, **_kwargs: None)\ntests/test_run_analysis_cli_coverage.py:121:        run_analysis.main([\"--config\", \"config.yml\"])\ntests/test_run_analysis_cli_coverage.py:150:    monkeypatch.setattr(run_analysis, \"load\", lambda _: cfg)\ntests/test_run_analysis_cli_coverage.py:151:    monkeypatch.setattr(run_analysis, \"load_csv\", _load_csv)\ntests/test_run_analysis_cli_coverage.py:153:        run_analysis, \"api\", SimpleNamespace(run_simulation=_stub_run_simulation)\ntests/test_run_analysis_cli_coverage.py:156:        run_analysis.export, \"format_summary_text\", lambda *_, **__: \"summary\"\ntests/test_run_analysis_cli_coverage.py:158:    monkeypatch.setattr(run_analysis.export, \"export_data\", lambda *_, **__: None)\ntests/test_run_analysis_cli_coverage.py:160:        run_analysis.export, \"make_summary_formatter\", lambda *_, **__: lambda df: df\ntests/test_run_analysis_cli_coverage.py:163:    exit_code = run_analysis.main([\"--config\", \"config.yml\"])\ntests/test_run_analysis_cli_coverage.py:184:    monkeypatch.setattr(run_analysis, \"load\", lambda _: cfg)\ntests/test_run_analysis_cli_coverage.py:185:    monkeypatch.setattr(run_analysis, \"load_csv\", _load_csv)\ntests/test_run_analysis_cli_coverage.py:187:        run_analysis, \"api\", SimpleNamespace(run_simulation=lambda *_: empty_result)\ntests/test_run_analysis_cli_coverage.py:190:    exit_code = run_analysis.main([\"--detailed\", \"--config\", \"config.yml\"])\ntests/test_run_analysis_cli_coverage.py:213:    monkeypatch.setattr(run_analysis, \"load\", lambda _: cfg)\ntests/test_run_analysis_cli_coverage.py:214:    monkeypatch.setattr(run_analysis, \"load_csv\", _stub_load_csv)\ntests/test_run_analysis_cli_coverage.py:226:        run_analysis, \"api\", SimpleNamespace(run_simulation=_stub_run_simulation)\ntests/test_run_analysis_cli_coverage.py:229:        run_analysis.export, \"format_summary_text\", lambda *_, **__: \"summary\"\ntests/test_run_analysis_cli_coverage.py:232:        run_analysis.export, \"summary_frame_from_result\", lambda _: pd.DataFrame()\ntests/test_run_analysis_cli_coverage.py:235:        run_analysis.export, \"make_summary_formatter\", lambda *_, **__: lambda df: df\ntests/test_run_analysis_cli_coverage.py:237:    monkeypatch.setattr(run_analysis.export, \"export_to_excel\", _export_to_excel)\ntests/test_run_analysis_cli_coverage.py:238:    monkeypatch.setattr(run_analysis.export, \"export_data\", lambda *_, **__: None)\ntests/test_run_analysis_cli_coverage.py:240:    exit_code = run_analysis.main([\"--config\", \"config.yml\"])\ntests/test_multi_period_engine_price_frames.py:124:    def fake_run_analysis(\ntests/test_multi_period_engine_price_frames.py:134:    monkeypatch.setattr(mp_engine, \"_run_analysis\", fake_run_analysis)\ntests/test_risk_free_default_alignment.py:143:    monkeypatch.setattr(api, \"_run_analysis\", fake_single_run)\ntests/test_risk_free_default_alignment.py:166:    monkeypatch.setattr(mp_engine, \"_run_analysis\", fake_multi_run)\ntests/test_risk_free_default_alignment.py:191:    monkeypatch.setattr(api, \"_run_analysis\", _run_with_risk_free_check)\ntests/test_risk_free_default_alignment.py:201:    monkeypatch.setattr(mp_engine, \"_run_analysis\", _run_with_risk_free_check)\ntests/test_multi_period_engine_extended.py:206:    def fake_run_analysis(df, *_args, **_kwargs):\ntests/test_multi_period_engine_extended.py:210:    monkeypatch.setattr(engine, \"_run_analysis\", fake_run_analysis)\ntests/test_multi_period_engine_extended.py:214:    assert captured, \"_run_analysis should be called\"\ntests/test_multi_period_engine_incremental_cov.py:91:    def fake_run_analysis(*args, **kwargs):\ntests/test_multi_period_engine_incremental_cov.py:95:    monkeypatch.setattr(mp_engine, \"_run_analysis\", fake_run_analysis)\ntests/test_multi_period_engine_incremental_cov.py:132:    def fake_run_analysis(*args, **kwargs):\ntests/test_multi_period_engine_incremental_cov.py:135:    monkeypatch.setattr(mp_engine, \"_run_analysis\", fake_run_analysis)\ntests/test_multi_period_engine_incremental_cov.py:176:    def fake_run_analysis(*args, **kwargs):\ntests/test_multi_period_engine_incremental_cov.py:180:    monkeypatch.setattr(mp_engine, \"_run_analysis\", fake_run_analysis)\ntests/test_multi_period_engine_incremental_cov.py:201:    def fake_run_analysis(*args, **kwargs):\ntests/test_multi_period_engine_incremental_cov.py:204:    monkeypatch.setattr(mp_engine, \"_run_analysis\", fake_run_analysis)\ntests/test_multi_period_engine_incremental_cov.py:239:    def fake_run_analysis(*args, **kwargs):\ntests/test_multi_period_engine_incremental_cov.py:242:    monkeypatch.setattr(mp_engine, \"_run_analysis\", fake_run_analysis)\ntests/test_multi_period_engine_incremental_cov.py:296:    def fake_run_analysis(*args, **kwargs):\ntests/test_multi_period_engine_incremental_cov.py:299:    monkeypatch.setattr(mp_engine, \"_run_analysis\", fake_run_analysis)\ntests/test_multi_period_engine_incremental_cov.py:348:    def fake_run_analysis(*args, **kwargs):\ntests/test_multi_period_engine_incremental_cov.py:351:    monkeypatch.setattr(mp_engine, \"_run_analysis\", fake_run_analysis)\ntests/test_multi_period_engine_incremental_cov.py:401:    def fake_run_analysis(*args, **kwargs):\ntests/test_multi_period_engine_incremental_cov.py:404:    monkeypatch.setattr(mp_engine, \"_run_analysis\", fake_run_analysis)\ntests/test_multi_period_engine_incremental_cov.py:432:    def fake_run_analysis(*args, **kwargs):\ntests/test_multi_period_engine_incremental_cov.py:435:    monkeypatch.setattr(mp_engine, \"_run_analysis\", fake_run_analysis)\ntests/test_multi_period_engine_incremental_cov.py:476:    def fake_run_analysis(*args, **kwargs):\ntests/test_multi_period_engine_incremental_cov.py:479:    monkeypatch.setattr(mp_engine, \"_run_analysis\", fake_run_analysis)\ntests/test_multi_period_engine_incremental_cov.py:537:    def fake_run_analysis(*args, **kwargs):\ntests/test_multi_period_engine_incremental_cov.py:540:    monkeypatch.setattr(mp_engine, \"_run_analysis\", fake_run_analysis)\ntests/test_multi_period_engine_incremental_cov.py:578:    def fake_run_analysis(*args, **kwargs):\ntests/test_multi_period_engine_incremental_cov.py:581:    monkeypatch.setattr(mp_engine, \"_run_analysis\", fake_run_analysis)\ntests/test_multi_period_engine_keepalive.py:167:        mp_engine, \"_run_analysis\", lambda *_args, **_kwargs: {\"summary\": \"ok\"}\ntests/test_multi_period_engine_keepalive.py:212:    def fake_run_analysis(\ntests/test_multi_period_engine_keepalive.py:223:    monkeypatch.setattr(mp_engine, \"_run_analysis\", fake_run_analysis)\ntests/test_multi_period_engine_keepalive.py:304:        mp_engine, \"_run_analysis\", lambda *_a, **_k: {\"payload\": \"unused\"}\ntests/test_run_analysis_cli_parameters.py:1:\"\"\"Regression tests covering argument adaptation in ``run_analysis.main``.\"\"\"\ntests/test_run_analysis_cli_parameters.py:11:from trend_analysis import run_analysis as run_analysis_mod\ntests/test_run_analysis_cli_parameters.py:24:            run_analysis_mod.export,\ntests/test_run_analysis_cli_parameters.py:29:            run_analysis_mod.export,\ntests/test_run_analysis_cli_parameters.py:34:            run_analysis_mod.export,\ntests/test_run_analysis_cli_parameters.py:38:        mp.setattr(run_analysis_mod.export, \"export_data\", lambda *args, **kwargs: None)\ntests/test_run_analysis_cli_parameters.py:40:            run_analysis_mod.export, \"export_to_excel\", lambda *args, **kwargs: None\ntests/test_run_analysis_cli_parameters.py:94:        mp.setattr(run_analysis_mod, \"load\", lambda path: cfg)\ntests/test_run_analysis_cli_parameters.py:95:        mp.setattr(run_analysis_mod, \"load_csv\", fake_load_csv)\ntests/test_run_analysis_cli_parameters.py:97:            run_analysis_mod.api, \"run_simulation\", lambda cfg, df: _make_result()\ntests/test_run_analysis_cli_parameters.py:100:        rc = run_analysis_mod.main([\"-c\", \"config.yml\"])\ntests/test_run_analysis_cli_parameters.py:126:        mp.setattr(run_analysis_mod, \"load\", lambda path: cfg)\ntests/test_run_analysis_cli_parameters.py:127:        mp.setattr(run_analysis_mod, \"load_csv\", fake_load_csv)\ntests/test_run_analysis_cli_parameters.py:129:            run_analysis_mod.api, \"run_simulation\", lambda cfg, df: _make_result()\ntests/test_run_analysis_cli_parameters.py:132:        rc = run_analysis_mod.main([\"-c\", \"config.yml\"])\ntests/test_pipeline_branch_coverage.py:89:def test_run_analysis_df_none_returns_none() -> None:\ntests/test_pipeline_branch_coverage.py:91:        pipeline._run_analysis(\ntests/test_pipeline_branch_coverage.py:105:def test_run_analysis_requires_date_column() -> None:\ntests/test_pipeline_branch_coverage.py:108:        pipeline._run_analysis(\ntests/test_pipeline_branch_coverage.py:120:def test_run_analysis_returns_none_when_windows_empty(\ntests/test_pipeline_branch_coverage.py:147:    result = pipeline._run_analysis(\ntests/test_pipeline_branch_coverage.py:160:def test_run_analysis_na_policy_branch(monkeypatch: pytest.MonkeyPatch) -> None:\ntests/test_pipeline_branch_coverage.py:221:    result = pipeline._run_analysis(\ntests/test_pipeline_branch_coverage.py:241:def test_run_analysis_information_ratio_fallback(\ntests/test_pipeline_branch_coverage.py:311:    result = pipeline._run_analysis(\ntests/test_trend_cli_soft_coverage.py:468:    monkeypatch.setattr(cli.pipeline, \"run_full\", lambda cfg_obj: {})\ntests/test_trend_cli_soft_coverage.py:696:        \"run_full\",\ntests/test_na_as_zero_policy.py:5:from trend_analysis.pipeline import _run_analysis\ntests/test_na_as_zero_policy.py:28:    res = _run_analysis(\ntests/test_na_as_zero_policy.py:54:    res = _run_analysis(\ntests/test_na_as_zero_policy.py:87:    res = _run_analysis(\ntests/test_run_analysis_cli_soft_round1.py:1:\"\"\"Focused regression coverage for ``trend_analysis.run_analysis.main``.\"\"\"\ntests/test_run_analysis_cli_soft_round1.py:12:from trend_analysis import run_analysis as run_analysis_mod\ntests/test_run_analysis_cli_soft_round1.py:16:    \"\"\"Container matching the attributes ``run_analysis.main`` expects.\"\"\"\ntests/test_run_analysis_cli_soft_round1.py:130:    monkeypatch.setattr(run_analysis_mod, \"load\", fake_load)\ntests/test_run_analysis_cli_soft_round1.py:132:        run_analysis_mod,\ntests/test_run_analysis_cli_soft_round1.py:137:        run_analysis_mod.api, \"run_simulation\", lambda cfg, df: factory()\ntests/test_run_analysis_cli_soft_round1.py:163:            run_analysis_mod.export, \"format_summary_text\", fake_format_summary\ntests/test_run_analysis_cli_soft_round1.py:165:        monkeypatch.setattr(run_analysis_mod.export, \"export_data\", fake_export_data)\ntests/test_run_analysis_cli_soft_round1.py:176:    rc = run_analysis_mod.main(argv)\ntests/test_run_analysis_cli_soft_round1.py:211:        run_analysis_mod,\ntests/test_run_analysis_cli_soft_round1.py:217:        run_analysis_mod.main([\"-c\", \"config.yml\"])\ntests/test_run_analysis_cli_soft_round1.py:230:    monkeypatch.setattr(run_analysis_mod, \"load\", lambda *_: cfg)\ntests/test_run_analysis_cli_soft_round1.py:231:    monkeypatch.setattr(run_analysis_mod, \"load_csv\", lambda *_, **__: None)\ntests/test_run_analysis_cli_soft_round1.py:234:        run_analysis_mod.main([\"-c\", \"config.yml\"])\ntests/test_run_analysis_additional.py:1:\"\"\"Additional coverage for ``trend_analysis.run_analysis`` CLI entry point.\"\"\"\ntests/test_run_analysis_additional.py:11:from trend_analysis import run_analysis\ntests/test_run_analysis_additional.py:61:    monkeypatch.setattr(run_analysis, \"load\", lambda path: cfg)\ntests/test_run_analysis_additional.py:82:    monkeypatch.setattr(run_analysis, \"load_csv\", fake_load_csv)\ntests/test_run_analysis_additional.py:83:    monkeypatch.setattr(run_analysis.api, \"run_simulation\", lambda *_: DummyResult())\ntests/test_run_analysis_additional.py:85:        run_analysis.export, \"format_summary_text\", lambda *args, **kwargs: \"Summary\"\ntests/test_run_analysis_additional.py:88:    result = run_analysis.main([\"-c\", \"config.yml\"])\ntests/test_run_analysis_additional.py:128:    monkeypatch.setattr(run_analysis, \"load\", fake_load)\ntests/test_run_analysis_additional.py:129:    monkeypatch.setattr(run_analysis, \"load_csv\", fake_load_csv)\ntests/test_run_analysis_additional.py:130:    monkeypatch.setattr(run_analysis.api, \"run_simulation\", lambda *_: DummyResult())\ntests/test_run_analysis_additional.py:132:        run_analysis.export, \"format_summary_text\", lambda *args, **kwargs: \"Summary\"\ntests/test_run_analysis_additional.py:135:    assert run_analysis.main([\"-c\", \"cfg.yml\"]) == 0\ntests/test_run_analysis_additional.py:197:    monkeypatch.setattr(run_analysis, \"load\", fake_load)\ntests/test_run_analysis_additional.py:198:    monkeypatch.setattr(run_analysis, \"load_csv\", fake_load_csv)\ntests/test_run_analysis_additional.py:199:    monkeypatch.setattr(run_analysis.api, \"run_simulation\", lambda *_: DetailedResult())\ntests/test_run_analysis_additional.py:200:    monkeypatch.setattr(run_analysis.export, \"format_summary_text\", fake_format_summary)\ntests/test_run_analysis_additional.py:201:    monkeypatch.setattr(run_analysis.export, \"make_summary_formatter\", fake_formatter)\ntests/test_run_analysis_additional.py:203:        run_analysis.export, \"summary_frame_from_result\", fake_summary_frame\ntests/test_run_analysis_additional.py:205:    monkeypatch.setattr(run_analysis.export, \"export_to_excel\", fake_export_to_excel)\ntests/test_run_analysis_additional.py:206:    monkeypatch.setattr(run_analysis.export, \"export_data\", fake_export_data)\ntests/test_run_analysis_additional.py:208:    result = run_analysis.main([\"-c\", \"config.yml\"])\ntests/test_run_analysis_additional.py:229:    monkeypatch.setattr(run_analysis, \"load\", lambda _path: cfg)\ntests/test_run_analysis_additional.py:231:        run_analysis.main([\"-c\", \"cfg.yml\"])\ntests/test_run_analysis_additional.py:238:    monkeypatch.setattr(run_analysis, \"load\", lambda _path: cfg)\ntests/test_run_analysis_additional.py:239:    monkeypatch.setattr(run_analysis, \"load_csv\", lambda *a, **k: None)\ntests/test_run_analysis_additional.py:241:        run_analysis.main([\"-c\", \"cfg.yml\"])\ntests/test_run_analysis_additional.py:257:    monkeypatch.setattr(run_analysis, \"load\", lambda _path: cfg)\ntests/test_run_analysis_additional.py:258:    monkeypatch.setattr(run_analysis, \"load_csv\", fake_load_csv)\ntests/test_run_analysis_additional.py:259:    monkeypatch.setattr(run_analysis.api, \"run_simulation\", lambda *_: EmptyResult())\ntests/test_run_analysis_additional.py:261:    exit_code = run_analysis.main([\"-c\", \"cfg.yml\", \"--detailed\"])\ntests/test_run_analysis_additional.py:292:    monkeypatch.setattr(run_analysis, \"load\", lambda _path: cfg)\ntests/test_run_analysis_additional.py:293:    monkeypatch.setattr(run_analysis, \"load_csv\", fake_load_csv)\ntests/test_run_analysis_additional.py:294:    monkeypatch.setattr(run_analysis.api, \"run_simulation\", lambda *_: EmptyResult())\ntests/test_run_analysis_additional.py:296:    exit_code = run_analysis.main([\"-c\", \"cfg.yml\", \"--detailed\"])\ntests/test_run_analysis_additional.py:338:    monkeypatch.setattr(run_analysis, \"load\", lambda _path: cfg)\ntests/test_run_analysis_additional.py:339:    monkeypatch.setattr(run_analysis, \"load_csv\", fake_load_csv)\ntests/test_run_analysis_additional.py:340:    monkeypatch.setattr(run_analysis.api, \"run_simulation\", lambda *_: EmptyResult())\ntests/test_run_analysis_additional.py:342:    exit_code = run_analysis.main([\"-c\", \"cfg.yml\", \"--detailed\"])\ntests/test_multi_period_engine_turnover_extended.py:209:    def fake_run_analysis(*args: Any, **kwargs: Any) -> Dict[str, Any]:\ntests/test_multi_period_engine_turnover_extended.py:213:    monkeypatch.setattr(mp_engine, \"_run_analysis\", fake_run_analysis)\ntests/test_multi_period_engine_turnover_extended.py:227:    # ``_run_analysis`` invoked once per period with expected arguments (7-month slices).\ntests/test_optional_notebook_deps.py:65:def test_pipeline_runs_without_notebook_dependencies(monkeypatch: pytest.MonkeyPatch):\ntests/test_optional_notebook_deps.py:71:    result = pipeline.run_analysis(\ntests/test_run_analysis_cli.py:7:from trend_analysis import run_analysis\ntests/test_run_analysis_cli.py:46:    monkeypatch.setattr(run_analysis, \"load\", fake_load)\ntests/test_run_analysis_cli.py:49:        run_analysis.main([\"--config\", \"config.yml\"])\ntests/test_run_analysis_cli.py:57:    monkeypatch.setattr(run_analysis, \"load\", lambda path: cfg)\ntests/test_run_analysis_cli.py:72:    monkeypatch.setattr(run_analysis, \"load_csv\", fake_load_csv)\ntests/test_run_analysis_cli.py:73:    monkeypatch.setattr(run_analysis.api, \"run_simulation\", pytest.fail)\ntests/test_run_analysis_cli.py:76:        run_analysis.main([\"--config\", \"config.yml\"])\ntests/test_run_analysis_cli.py:89:    monkeypatch.setattr(run_analysis, \"load\", lambda path: cfg)\ntests/test_run_analysis_cli.py:103:    monkeypatch.setattr(run_analysis, \"load_csv\", fake_load_csv)\ntests/test_run_analysis_cli.py:110:    monkeypatch.setattr(run_analysis.api, \"run_simulation\", fake_run_simulation)\ntests/test_run_analysis_cli.py:112:    assert run_analysis.main([\"--config\", \"config.yml\", \"--detailed\"]) == 0\ntests/test_run_analysis_cli.py:123:    monkeypatch.setattr(run_analysis, \"load\", lambda path: cfg)\ntests/test_run_analysis_cli.py:136:    monkeypatch.setattr(run_analysis, \"load_csv\", fake_load_csv)\ntests/test_run_analysis_cli.py:141:    monkeypatch.setattr(run_analysis.api, \"run_simulation\", fake_run_simulation)\ntests/test_run_analysis_cli.py:143:    run_analysis.main([\"--config\", \"config.yml\", \"--detailed\"])\ntests/test_run_analysis_cli.py:156:    monkeypatch.setattr(run_analysis, \"load\", lambda path: cfg)\ntests/test_run_analysis_cli.py:159:        run_analysis,\ntests/test_run_analysis_cli.py:165:    monkeypatch.setattr(run_analysis.api, \"run_simulation\", lambda *_: result)\ntests/test_run_analysis_cli.py:167:    assert run_analysis.main([\"--config\", \"config.yml\"]) == 0\ntests/test_run_analysis_cli.py:179:    monkeypatch.setattr(run_analysis, \"load\", lambda path: cfg)\ntests/test_run_analysis_cli.py:182:        run_analysis,\ntests/test_run_analysis_cli.py:190:        run_analysis.api,\ntests/test_run_analysis_cli.py:197:        run_analysis.export, \"format_summary_text\", lambda *a: formatted_summary\ntests/test_run_analysis_cli.py:202:        run_analysis.export, \"export_data\", lambda *a, **k: export_calls.append((a, k))\ntests/test_run_analysis_cli.py:205:    assert run_analysis.main([\"--config\", \"config.yml\"]) == 0\ntests/test_run_analysis_cli.py:223:    monkeypatch.setattr(run_analysis, \"load\", lambda path: cfg)\ntests/test_run_analysis_cli.py:226:        run_analysis,\ntests/test_run_analysis_cli.py:234:        run_analysis.api,\ntests/test_run_analysis_cli.py:240:        run_analysis.export, \"format_summary_text\", lambda *a: \"summary text\"\ntests/test_run_analysis_cli.py:245:        run_analysis.export, \"export_data\", lambda *a, **k: export_calls.append((a, k))\ntests/test_run_analysis_cli.py:248:    assert run_analysis.main([\"--config\", \"config.yml\"]) == 0\ntests/test_run_analysis_cli.py:261:    monkeypatch.setattr(run_analysis, \"load\", lambda path: cfg)\ntests/test_run_analysis_cli.py:264:        run_analysis,\ntests/test_run_analysis_cli.py:277:        run_analysis.api,\ntests/test_run_analysis_cli.py:283:        run_analysis.export, \"format_summary_text\", lambda *a: \"summary\"\ntests/test_run_analysis_cli.py:288:        run_analysis.export, \"summary_frame_from_result\", lambda details: summary_frame\ntests/test_run_analysis_cli.py:298:        run_analysis.export, \"make_summary_formatter\", fake_make_formatter\ntests/test_run_analysis_cli.py:303:        run_analysis.export,\ntests/test_run_analysis_cli.py:310:        run_analysis.export, \"export_data\", lambda *a, **k: data_calls.append((a, k))\ntests/test_run_analysis_cli.py:313:    assert run_analysis.main([\"--config\", \"config.yml\"]) == 0\ntests/test_run_analysis_cli.py:336:    monkeypatch.setattr(run_analysis, \"load\", lambda path: cfg)\ntests/test_run_analysis_cli.py:342:    monkeypatch.setattr(run_analysis, \"load_csv\", loader_without_errors)\ntests/test_run_analysis_cli.py:345:        run_analysis.api,\ntests/test_run_analysis_cli.py:350:    assert run_analysis.main([\"--config\", \"config.yml\", \"--detailed\"]) == 0\ntests/test_run_analysis_cli.py:358:    monkeypatch.setattr(run_analysis, \"load\", lambda path: cfg)\ntests/test_run_analysis_cli.py:367:    monkeypatch.setattr(run_analysis, \"load_csv\", loader_without_policy)\ntests/test_run_analysis_cli.py:370:        run_analysis.api,\ntests/test_run_analysis_cli.py:375:    assert run_analysis.main([\"--config\", \"config.yml\", \"--detailed\"]) == 0\ntests/test_run_analysis_cli.py:385:    monkeypatch.setattr(run_analysis, \"load\", lambda path: cfg)\ntests/test_run_analysis_cli.py:388:        run_analysis,\ntests/test_run_analysis_cli.py:399:        run_analysis.api,\ntests/test_run_analysis_cli.py:405:        run_analysis.export, \"format_summary_text\", lambda *a: \"summary\"\ntests/test_run_analysis_cli.py:408:        run_analysis.export,\ntests/test_run_analysis_cli.py:419:    monkeypatch.setattr(run_analysis.export, \"make_summary_formatter\", fake_formatter)\ntests/test_run_analysis_cli.py:423:        run_analysis.export,\ntests/test_run_analysis_cli.py:430:        run_analysis.export, \"export_data\", lambda *a, **k: data_calls.append((a, k))\ntests/test_run_analysis_cli.py:433:    assert run_analysis.main([\"--config\", \"config.yml\"]) == 0\ntests/test_run_analysis_cli.py:441:        Path(run_analysis.DEFAULT_OUTPUT_DIRECTORY) / \"analysis.xlsx\"\ntests/test_run_analysis_entrypoint_modern.py:10:import trend_analysis.run_analysis as run_analysis\ntests/test_run_analysis_entrypoint_modern.py:63:    monkeypatch.setattr(run_analysis, \"load\", lambda _: cfg)\ntests/test_run_analysis_entrypoint_modern.py:81:    monkeypatch.setattr(run_analysis, \"load_csv\", fake_load_csv)\ntests/test_run_analysis_entrypoint_modern.py:98:    monkeypatch.setattr(run_analysis.api, \"run_simulation\", fake_run_simulation)\ntests/test_run_analysis_entrypoint_modern.py:106:    monkeypatch.setattr(run_analysis.export, \"format_summary_text\", record_summary)\ntests/test_run_analysis_entrypoint_modern.py:108:        run_analysis.export,\ntests/test_run_analysis_entrypoint_modern.py:113:        run_analysis.export,\ntests/test_run_analysis_entrypoint_modern.py:135:    monkeypatch.setattr(run_analysis.export, \"export_to_excel\", record_export_to_excel)\ntests/test_run_analysis_entrypoint_modern.py:136:    monkeypatch.setattr(run_analysis.export, \"export_data\", record_export_data)\ntests/test_run_analysis_entrypoint_modern.py:138:    rc = run_analysis.main([\"--config\", \"config.yml\"])\ntests/test_run_analysis_entrypoint_modern.py:174:    monkeypatch.setattr(run_analysis, \"load\", lambda _: cfg)\ntests/test_run_analysis_entrypoint_modern.py:189:    monkeypatch.setattr(run_analysis, \"load_csv\", fake_loader)\ntests/test_run_analysis_entrypoint_modern.py:191:        run_analysis.api,\ntests/test_run_analysis_entrypoint_modern.py:196:    rc = run_analysis.main([\"--config\", \"fallback.yml\"])\ntests/test_run_analysis_entrypoint_modern.py:215:    monkeypatch.setattr(run_analysis, \"load\", lambda _: cfg)\ntests/test_run_analysis_entrypoint_modern.py:217:        run_analysis,\ntests/test_run_analysis_entrypoint_modern.py:223:        run_analysis.api,\ntests/test_run_analysis_entrypoint_modern.py:228:    rc = run_analysis.main([\"--config\", \"config.yml\", \"--detailed\"])\ntests/test_run_analysis_entrypoint_modern.py:239:    monkeypatch.setattr(run_analysis, \"load\", lambda _: cfg)\ntests/test_run_analysis_entrypoint_modern.py:241:        run_analysis.main([\"--config\", \"config.yml\"])\ntests/test_run_analysis_entrypoint_modern.py:248:    monkeypatch.setattr(run_analysis, \"load\", lambda _: cfg)\ntests/test_run_analysis_entrypoint_modern.py:249:    monkeypatch.setattr(run_analysis, \"load_csv\", lambda *args, **kwargs: None)\ntests/test_run_analysis_entrypoint_modern.py:251:        run_analysis.main([\"--config\", \"config.yml\"])\ntests/test_multi_period_engine_price_frames_extra.py:125:    def fake_run_analysis(*args, **kwargs):\ntests/test_multi_period_engine_price_frames_extra.py:130:    monkeypatch.setattr(mp_engine, \"_run_analysis\", fake_run_analysis)\ntests/test_multi_period_engine_price_frames_extra.py:181:    def fake_run_analysis(*args, **kwargs):\ntests/test_multi_period_engine_price_frames_extra.py:186:    monkeypatch.setattr(mp_engine, \"_run_analysis\", fake_run_analysis)\ntests/golden/test_demo.py:210:            [\"python\", \"-m\", \"trend_analysis.run_analysis\", \"-c\", \"config/demo.yml\"],\ntests/golden/test_demo.py:340:                    \"trend_analysis.run_analysis\",\ntests/test_multi_period_missing_policy_observability.py:65:    def fake_run_analysis(\ntests/test_multi_period_missing_policy_observability.py:74:    monkeypatch.setattr(mp_engine, \"_run_analysis\", fake_run_analysis)\ntests/test_multi_period_missing_policy_observability.py:101:    def fake_run_analysis(\ntests/test_multi_period_missing_policy_observability.py:108:    monkeypatch.setattr(mp_engine, \"_run_analysis\", fake_run_analysis)\ntests/test_run_analysis_cli_export.py:5:from trend_analysis import run_analysis\ntests/test_run_analysis_cli_export.py:39:    rc = run_analysis.main([\"-c\", str(cfg)])\ntests/test_rank_selection.py:4:from trend_analysis.pipeline import run_analysis\ntests/test_rank_selection.py:58:def test_run_analysis_rank_mode():\ntests/test_rank_selection.py:60:    res = run_analysis(\ntests/test_rank_selection.py:76:def test_run_analysis_rank_mode_excludes_bottom_k():\ntests/test_rank_selection.py:92:    res = run_analysis(\ntests/test_multi_period_engine_branch_completion.py:228:    monkeypatch.setattr(mp_engine, \"_run_analysis\", lambda *a, **k: {\"summary\": \"ok\"})\ntests/test_multi_period_engine_branch_completion.py:250:    monkeypatch.setattr(mp_engine, \"_run_analysis\", fake_run)\ntests/test_multi_period_engine_branch_completion.py:262:    assert calls, \"patched _run_analysis should be invoked\"\ntests/test_multi_period_engine_cov_cache.py:95:    def fake_run_analysis(df: pd.DataFrame, *_, **__):\ntests/test_multi_period_engine_cov_cache.py:99:    monkeypatch.setattr(engine, \"_run_analysis\", fake_run_analysis)\ntests/test_multi_period_engine_cov_cache.py:122:    def fake_run_analysis(df: pd.DataFrame, *_, **__):\ntests/test_multi_period_engine_cov_cache.py:125:    monkeypatch.setattr(engine, \"_run_analysis\", fake_run_analysis)\ntests/test_rf_handling.py:25:    res = pipeline._run_analysis(  # type: ignore[attr-defined]\ntests/test_rf_handling.py:59:    res = pipeline._run_analysis(  # type: ignore[attr-defined]\ntests/test_rf_handling.py:92:    res = pipeline._run_analysis(  # type: ignore[attr-defined]\ntests/test_rf_handling.py:124:        pipeline._run_analysis(  # type: ignore[attr-defined]\ntests/test_rf_handling.py:144:    res = pipeline._run_analysis(  # type: ignore[attr-defined]\ntests/test_rf_handling.py:173:        pipeline._run_analysis(  # type: ignore[attr-defined]\ntests/test_rf_handling.py:194:        pipeline._run_analysis(  # type: ignore[attr-defined]\ntests/test_rf_handling.py:226:    result = pipeline._run_analysis(**kwargs)  # type: ignore[attr-defined]\ntests/test_rf_handling.py:265:    res = pipeline._run_analysis(  # type: ignore[attr-defined]\ntests/test_run_analysis_cli_keepalive.py:1:\"\"\"Focused coverage for ``trend_analysis.run_analysis`` CLI helper.\"\"\"\ntests/test_run_analysis_cli_keepalive.py:13:from trend_analysis import run_analysis\ntests/test_run_analysis_cli_keepalive.py:61:        run_analysis.export,\ntests/test_run_analysis_cli_keepalive.py:69:    monkeypatch.setattr(run_analysis, \"load\", lambda _: DummyConfig(data={}))\ntests/test_run_analysis_cli_keepalive.py:71:        run_analysis.main([\"-c\", \"config.yml\"])\ntests/test_run_analysis_cli_keepalive.py:76:    monkeypatch.setattr(run_analysis, \"load\", lambda _: cfg)\ntests/test_run_analysis_cli_keepalive.py:77:    monkeypatch.setattr(run_analysis, \"load_csv\", lambda *_, **__: None)\ntests/test_run_analysis_cli_keepalive.py:79:        run_analysis.main([\"-c\", \"config.yml\"])\ntests/test_run_analysis_cli_keepalive.py:113:    monkeypatch.setattr(run_analysis, \"load\", lambda _: cfg)\ntests/test_run_analysis_cli_keepalive.py:114:    monkeypatch.setattr(run_analysis, \"load_csv\", fake_load_csv)\ntests/test_run_analysis_cli_keepalive.py:115:    monkeypatch.setattr(run_analysis.api, \"run_simulation\", lambda *_: default_result)\ntests/test_run_analysis_cli_keepalive.py:117:        run_analysis.export, \"format_summary_text\", lambda *_, **__: \"summary\"\ntests/test_run_analysis_cli_keepalive.py:120:    exit_code = run_analysis.main([\"-c\", \"config.yml\"])\ntests/test_run_analysis_cli_keepalive.py:180:    monkeypatch.setattr(run_analysis, \"load\", lambda _: cfg)\ntests/test_run_analysis_cli_keepalive.py:181:    monkeypatch.setattr(run_analysis, \"load_csv\", loader_with_nan)\ntests/test_run_analysis_cli_keepalive.py:182:    monkeypatch.setattr(run_analysis.api, \"run_simulation\", lambda *_: default_result)\ntests/test_run_analysis_cli_keepalive.py:184:        run_analysis.export, \"format_summary_text\", lambda *_, **__: \"summary\"\ntests/test_run_analysis_cli_keepalive.py:186:    monkeypatch.setattr(run_analysis.export, \"export_data\", lambda *_, **__: None)\ntests/test_run_analysis_cli_keepalive.py:188:    exit_code = run_analysis.main([\"-c\", \"config.yml\", \"--detailed\"])\ntests/test_pipeline_calendar_alignment.py:8:def test_pipeline_run_applies_calendar_alignment():\ntests/test_pipeline_calendar_alignment.py:20:    result = pipeline._run_analysis(\ntests/test_avgcorr_export.py:8:from trend_analysis.pipeline import _run_analysis  # type: ignore\ntests/test_avgcorr_export.py:53:    res = _run_analysis(\ntests/test_avgcorr_export.py:95:    res = _run_analysis(\ntests/app/test_results_page.py:239:    monkeypatch.setattr(page.analysis_runner, \"run_analysis\", fake_run)\ntests/app/test_results_page.py:321:    monkeypatch.setattr(page.analysis_runner, \"run_analysis\", fake_run)\ntests/app/test_results_page.py:349:    monkeypatch.setattr(page.analysis_runner, \"run_analysis\", raise_error)\ntests/app/test_comparison_helpers.py:129:    def _fake_run_analysis(df_for_analysis, model_state, benchmark, *, data_hash=None):\ntests/app/test_comparison_helpers.py:144:        results_page.analysis_runner, \"run_analysis\", _fake_run_analysis\ntests/test_multi_period_engine_threshold_edgecases.py:124:def _stub_run_analysis(call_log: list[Dict[str, Any]]):\ntests/test_multi_period_engine_threshold_edgecases.py:178:    monkeypatch.setattr(mp_engine, \"_run_analysis\", _stub_run_analysis(run_calls))\ntests/test_multi_period_engine_threshold_edgecases.py:308:    monkeypatch.setattr(mp_engine, \"_run_analysis\", _stub_run_analysis(run_calls))\ntests/test_multi_period_engine_threshold_edgecases.py:400:    monkeypatch.setattr(mp_engine, \"_run_analysis\", _stub_run_analysis(run_calls))\ntests/test_multi_period_engine_threshold_edgecases.py:597:    monkeypatch.setattr(mp_engine, \"_run_analysis\", _stub_run_analysis(run_calls))\ntests/test_multi_period_engine_threshold_edgecases.py:878:    def fake_run_analysis(\ntests/test_multi_period_engine_threshold_edgecases.py:911:    monkeypatch.setattr(mp_engine, \"_run_analysis\", fake_run_analysis)\ntests/test_multi_period_engine_threshold_edgecases.py:1020:    monkeypatch.setattr(mp_engine, \"_run_analysis\", _stub_run_analysis(call_log))\ntests/test_multi_period_engine_threshold_edgecases.py:1025:    assert call_log, \"_run_analysis should receive custom weights\"\ntests/test_multi_period_engine_threshold_edgecases.py:1129:    monkeypatch.setattr(mp_engine, \"_run_analysis\", _stub_run_analysis(call_log))\ntests/test_multi_period_engine_threshold_events_extended.py:344:    def fake_run_analysis(\ntests/test_multi_period_engine_threshold_events_extended.py:366:    monkeypatch.setattr(mp_engine, \"_run_analysis\", fake_run_analysis)\ntests/test_multi_period_engine_threshold_events_extended.py:447:    def fake_run_analysis(\ntests/test_multi_period_engine_threshold_events_extended.py:469:    monkeypatch.setattr(mp_engine, \"_run_analysis\", fake_run_analysis)\ntests/test_pipeline_warmup_autofix.py:9:from trend_analysis.pipeline import _run_analysis, run_analysis\ntests/test_pipeline_warmup_autofix.py:39:    result = run_analysis(\ntests/test_pipeline_warmup_autofix.py:58:def test_run_analysis_warmup_zeroes_leading_rows() -> None:\ntests/test_pipeline_warmup_autofix.py:76:    result = _run_analysis(\ntests/test_pipeline_warmup_autofix.py:97:def test_run_analysis_additional_metrics_coverages() -> None:\ntests/test_pipeline_warmup_autofix.py:100:    result: int = run_analysis(\ntests/test_gui_app_extended.py:326:    monkeypatch.setattr(app.pipeline, \"run_full\", lambda cfg: {\"extra\": 1})\ntests/smoke/test_pipeline_smoke.py:3:from trend_analysis.pipeline import run_analysis\ntests/smoke/test_pipeline_smoke.py:18:    res = run_analysis(\ntests/test_pipeline_constraints_integration.py:5:from trend_analysis.pipeline import _run_analysis\ntests/test_pipeline_constraints_integration.py:45:    res = _run_analysis(\ntests/test_pipeline_constraints_integration.py:76:    res_long_only = _run_analysis(\ntests/test_pipeline_constraints_integration.py:94:    res_short_ok = _run_analysis(\ntests/test_pipeline_constraints_integration.py:115:    res_long_only = _run_analysis(\ntests/test_pipeline_constraints_integration.py:133:    res_short_ok = _run_analysis(\ntests/test_run_analysis_cli_branches.py:1:\"\"\"Focused regression tests for the run_analysis CLI helper.\"\"\"\ntests/test_run_analysis_cli_branches.py:11:from trend_analysis import run_analysis\ntests/test_run_analysis_cli_branches.py:32:    monkeypatch.setattr(run_analysis, \"load\", lambda _: _make_config({}))\ntests/test_run_analysis_cli_branches.py:35:        run_analysis.main([\"-c\", \"config.yml\"])\ntests/test_run_analysis_cli_branches.py:79:    monkeypatch.setattr(run_analysis, \"load\", fake_load)\ntests/test_run_analysis_cli_branches.py:80:    monkeypatch.setattr(run_analysis, \"load_csv\", fake_load_csv)\ntests/test_run_analysis_cli_branches.py:81:    monkeypatch.setattr(run_analysis.api, \"run_simulation\", fake_run_simulation)\ntests/test_run_analysis_cli_branches.py:83:        run_analysis.export, \"format_summary_text\", lambda *_, **__: \"summary\"\ntests/test_run_analysis_cli_branches.py:86:    exit_code = run_analysis.main([\"-c\", \"config.yml\"])\ntests/test_run_analysis_cli_branches.py:131:    monkeypatch.setattr(run_analysis, \"load\", fake_load)\ntests/test_run_analysis_cli_branches.py:132:    monkeypatch.setattr(run_analysis, \"load_csv\", fake_load_csv)\ntests/test_run_analysis_cli_branches.py:134:        run_analysis.api,\ntests/test_run_analysis_cli_branches.py:141:        run_analysis.export, \"format_summary_text\", lambda *_, **__: \"summary\"\ntests/test_run_analysis_cli_branches.py:144:    exit_code = run_analysis.main([\"-c\", \"config.yml\"])\ntests/test_run_analysis_cli_branches.py:177:    monkeypatch.setattr(run_analysis, \"load\", fake_load)\ntests/test_run_analysis_cli_branches.py:178:    monkeypatch.setattr(run_analysis, \"load_csv\", fake_load_csv)\ntests/test_run_analysis_cli_branches.py:180:        run_analysis.api,\ntests/test_run_analysis_cli_branches.py:187:        run_analysis.export, \"format_summary_text\", lambda *_, **__: \"summary\"\ntests/test_run_analysis_cli_branches.py:190:    exit_code = run_analysis.main([\"-c\", \"config.yml\"])\ntests/test_run_analysis_cli_branches.py:201:        run_analysis,\ntests/test_run_analysis_cli_branches.py:205:    monkeypatch.setattr(run_analysis, \"load_csv\", lambda *_, **__: None)\ntests/test_run_analysis_cli_branches.py:208:        run_analysis.main([\"-c\", \"config.yml\"])\ntests/test_run_analysis_cli_branches.py:217:        run_analysis,\ntests/test_run_analysis_cli_branches.py:230:    monkeypatch.setattr(run_analysis, \"load_csv\", fake_load_csv)\ntests/test_run_analysis_cli_branches.py:232:        run_analysis.api,\ntests/test_run_analysis_cli_branches.py:237:    exit_code = run_analysis.main([\"-c\", \"config.yml\", \"--detailed\"])\ntests/test_run_analysis_cli_branches.py:257:    monkeypatch.setattr(run_analysis, \"DEFAULT_OUTPUT_DIRECTORY\", str(sentinel_dir))\ntests/test_run_analysis_cli_branches.py:258:    monkeypatch.setattr(run_analysis, \"DEFAULT_OUTPUT_FORMATS\", sentinel_formats)\ntests/test_run_analysis_cli_branches.py:259:    monkeypatch.setattr(run_analysis, \"load\", lambda _: cfg)\ntests/test_run_analysis_cli_branches.py:266:    monkeypatch.setattr(run_analysis, \"load_csv\", fake_load_csv)\ntests/test_run_analysis_cli_branches.py:268:        run_analysis.export,\ntests/test_run_analysis_cli_branches.py:273:        run_analysis.api,\ntests/test_run_analysis_cli_branches.py:285:        run_analysis.export,\ntests/test_run_analysis_cli_branches.py:290:    exit_code = run_analysis.main([\"-c\", \"config.yml\"])\ntests/test_pipeline_helpers_additional.py:18:    _empty_run_full_result,\ntests/test_pipeline_helpers_additional.py:23:    _run_analysis,\ntests/test_pipeline_helpers_additional.py:94:    empty = _empty_run_full_result()\ntests/test_pipeline_helpers_additional.py:489:def test_run_analysis_short_circuits(monkeypatch: pytest.MonkeyPatch) -> None:\ntests/test_pipeline_helpers_additional.py:510:        _run_analysis(\ntests/test_pipeline_helpers_additional.py:534:        _run_analysis(\ntests/test_pipeline_helpers_additional.py:548:def test_run_analysis_rank_selection_with_fallbacks(\ntests/test_pipeline_helpers_additional.py:618:        result = _run_analysis(\ntests/test_pipeline_helpers_additional.py:660:def test_run_analysis_zero_weight_custom(monkeypatch: pytest.MonkeyPatch) -> None:\ntests/test_pipeline_helpers_additional.py:710:        result = _run_analysis(\ntests/test_pipeline_helpers_additional.py:768:    def fake_run_analysis(*args, **kwargs):\ntests/test_pipeline_helpers_additional.py:801:        mp.setattr(pipeline, \"_run_analysis\", fake_run_analysis)\ntests/test_pipeline_helpers_additional.py:803:        result = pipeline.run(cfg)\ntests/test_pipeline_helpers_additional.py:811:def test_run_full_passes_through_results(monkeypatch: pytest.MonkeyPatch) -> None:\ntests/test_pipeline_helpers_additional.py:829:    def fake_run_analysis(*args, **kwargs):\ntests/test_pipeline_helpers_additional.py:861:        mp.setattr(pipeline, \"_run_analysis\", fake_run_analysis)\ntests/test_pipeline_helpers_additional.py:863:        result = pipeline.run_full(cfg)\ntests/test_pipeline_helpers_additional.py:1015:def test_run_analysis_random_selection(monkeypatch: pytest.MonkeyPatch) -> None:\ntests/test_pipeline_helpers_additional.py:1076:        result = _run_analysis(\ntests/test_pipeline_helpers_additional.py:1111:def test_run_analysis_returns_none_when_copy_becomes_empty(\ntests/test_pipeline_helpers_additional.py:1145:        result = _run_analysis(\ntests/test_pipeline_helpers_additional.py:1159:def test_run_analysis_returns_none_when_ret_cols_consumed(\ntests/test_pipeline_helpers_additional.py:1213:        result = _run_analysis(\ntests/test_pipeline_helpers_additional.py:1228:def test_run_analysis_weight_engine_success(monkeypatch: pytest.MonkeyPatch) -> None:\ntests/test_pipeline_helpers_additional.py:1299:        result = _run_analysis(\ntests/test_pipeline_helpers_additional.py:1316:def test_run_analysis_uses_empty_signal_frame(monkeypatch: pytest.MonkeyPatch) -> None:\ntests/test_pipeline_helpers_additional.py:1415:        result = _run_analysis(\ntests/test_pipeline_helpers_additional.py:1432:def test_run_analysis_warmup_zeroes_initial_rows(\ntests/test_pipeline_helpers_additional.py:1498:        result = _run_analysis(\ntests/test_pipeline_helpers_additional.py:1517:def test_run_analysis_adds_valid_indices_and_skips_missing_benchmarks(\ntests/test_pipeline_helpers_additional.py:1584:        result = _run_analysis(\ntests/test_pipeline_helpers_additional.py:1602:def test_run_analysis_handles_benchmark_overrides(\ntests/test_pipeline_helpers_additional.py:1669:        result = _run_analysis(\ntests/test_pipeline_helpers_additional.py:1705:    def fake_run_analysis(*args, **kwargs):\ntests/test_pipeline_helpers_additional.py:1734:        mp.setattr(pipeline, \"_run_analysis\", fake_run_analysis)\ntests/test_pipeline_helpers_additional.py:1736:        result = pipeline.run(cfg)\ntests/test_pipeline_helpers_additional.py:1758:    def fake_run_analysis(*args, **kwargs):\ntests/test_pipeline_helpers_additional.py:1787:        mp.setattr(pipeline, \"_run_analysis\", fake_run_analysis)\ntests/test_pipeline_helpers_additional.py:1789:        pipeline.run(cfg)\ntests/test_pipeline_helpers_additional.py:1795:def test_run_full_requires_csv_path() -> None:\ntests/test_pipeline_helpers_additional.py:1798:        pipeline.run_full(cfg)\ntests/test_pipeline_helpers_additional.py:1801:def test_run_full_uses_nan_policy_defaults(monkeypatch: pytest.MonkeyPatch) -> None:\ntests/test_pipeline_helpers_additional.py:1816:    def fake_run_analysis(*args, **kwargs):\ntests/test_pipeline_helpers_additional.py:1842:        mp.setattr(pipeline, \"_run_analysis\", fake_run_analysis)\ntests/test_pipeline_helpers_additional.py:1844:        result = pipeline.run_full(cfg)\ntests/test_pipeline_helpers_additional.py:1851:def test_run_full_respects_explicit_policy(monkeypatch: pytest.MonkeyPatch) -> None:\ntests/test_pipeline_helpers_additional.py:1865:    def fake_run_analysis(*args, **kwargs):\ntests/test_pipeline_helpers_additional.py:1891:        mp.setattr(pipeline, \"_run_analysis\", fake_run_analysis)\ntests/test_pipeline_helpers_additional.py:1893:        pipeline.run_full(cfg)\ntests/test_multi_period_engine_threshold_bounds.py:215:    def fake_run_analysis(\ntests/test_multi_period_engine_threshold_bounds.py:237:    monkeypatch.setattr(mp_engine, \"_run_analysis\", fake_run_analysis)\ntests/test_multi_period_engine_threshold_bounds.py:344:    def fake_run_analysis(\ntests/test_multi_period_engine_threshold_bounds.py:366:    monkeypatch.setattr(mp_engine, \"_run_analysis\", fake_run_analysis)\ntests/test_multi_period_exits_cooldown.py:93:    monkeypatch.setattr(engine, \"_run_analysis\", lambda *args, **kwargs: {})\ntests/test_logging_module.py:5:the pipeline run logging experience, so we focus on verifying that:\ntests/test_trend_analysis_run_analysis_keepalive.py:1:\"\"\"Focused coverage tests for :mod:`trend_analysis.run_analysis`.\"\"\"\ntests/test_trend_analysis_run_analysis_keepalive.py:11:from trend_analysis import run_analysis\ntests/test_trend_analysis_run_analysis_keepalive.py:70:    monkeypatch.setattr(run_analysis, \"load\", lambda path: sample_config)\ntests/test_trend_analysis_run_analysis_keepalive.py:71:    monkeypatch.setattr(run_analysis, \"load_csv\", detailed_load)\ntests/test_trend_analysis_run_analysis_keepalive.py:72:    monkeypatch.setattr(run_analysis.api, \"run_simulation\", lambda cfg, df: result)\ntests/test_trend_analysis_run_analysis_keepalive.py:74:    rc = run_analysis.main([\"-c\", \"config.yml\", \"--detailed\"])\ntests/test_trend_analysis_run_analysis_keepalive.py:101:    monkeypatch.setattr(run_analysis, \"load\", lambda path: sample_config)\ntests/test_trend_analysis_run_analysis_keepalive.py:103:        run_analysis, \"load_csv\", lambda path, **kwargs: pd.DataFrame({\"value\": [1.0]})\ntests/test_trend_analysis_run_analysis_keepalive.py:105:    monkeypatch.setattr(run_analysis.api, \"run_simulation\", lambda cfg, df: result)\ntests/test_trend_analysis_run_analysis_keepalive.py:106:    monkeypatch.setattr(run_analysis.export, \"format_summary_text\", fake_format)\ntests/test_trend_analysis_run_analysis_keepalive.py:108:        run_analysis.export, \"make_summary_formatter\", lambda *a, **k: \"formatter\"\ntests/test_trend_analysis_run_analysis_keepalive.py:111:        run_analysis.export,\ntests/test_trend_analysis_run_analysis_keepalive.py:115:    monkeypatch.setattr(run_analysis.export, \"export_to_excel\", lambda *a, **k: None)\ntests/test_trend_analysis_run_analysis_keepalive.py:116:    monkeypatch.setattr(run_analysis.export, \"export_data\", fake_export_data)\ntests/test_trend_analysis_run_analysis_keepalive.py:118:    rc = run_analysis.main([\"-c\", \"config.yml\"])\ntests/test_trend_analysis_run_analysis_keepalive.py:129:    monkeypatch.setattr(run_analysis, \"load\", lambda path: sample_config)\ntests/test_trend_analysis_run_analysis_keepalive.py:131:        run_analysis.main([\"-c\", \"config.yml\"])\ntests/test_trend_analysis_run_analysis_keepalive.py:137:    monkeypatch.setattr(run_analysis, \"load\", lambda path: sample_config)\ntests/test_trend_analysis_run_analysis_keepalive.py:138:    monkeypatch.setattr(run_analysis, \"load_csv\", lambda *a, **k: None)\ntests/test_trend_analysis_run_analysis_keepalive.py:140:        run_analysis.main([\"-c\", \"config.yml\"])\ntests/test_trend_analysis_run_analysis_keepalive.py:168:    monkeypatch.setattr(run_analysis, \"load\", lambda path: sample_config)\ntests/test_trend_analysis_run_analysis_keepalive.py:169:    monkeypatch.setattr(run_analysis, \"load_csv\", fake_load)\ntests/test_trend_analysis_run_analysis_keepalive.py:171:        run_analysis.api, \"run_simulation\", lambda cfg, df: _make_result()\ntests/test_trend_analysis_run_analysis_keepalive.py:174:        run_analysis.export, \"format_summary_text\", lambda *a, **k: \"summary\"\ntests/test_trend_analysis_run_analysis_keepalive.py:176:    monkeypatch.setattr(run_analysis.export, \"export_data\", lambda *a, **k: None)\ntests/test_trend_analysis_run_analysis_keepalive.py:178:        run_analysis.export, \"make_summary_formatter\", lambda *a, **k: None\ntests/test_trend_analysis_run_analysis_keepalive.py:181:        run_analysis.export, \"summary_frame_from_result\", lambda *a, **k: pd.DataFrame()\ntests/test_trend_analysis_run_analysis_keepalive.py:183:    monkeypatch.setattr(run_analysis.export, \"export_to_excel\", lambda *a, **k: None)\ntests/test_trend_analysis_run_analysis_keepalive.py:185:    run_analysis.main([\"-c\", \"config.yml\"])\ntests/test_cli_api_golden_master.py:89:            \"trend_analysis.run_analysis\",\ntests/test_multi_period_engine.py:210:    def fake_run_analysis(*args, **kwargs):\ntests/test_multi_period_engine.py:213:    monkeypatch.setattr(engine, \"_run_analysis\", fake_run_analysis)\ntests/test_joblib_import.py:20:    \"trend_analysis.run_analysis\",\ntests/test_trend_analysis_cli_main.py:589:    monkeypatch.setattr(cli.pipeline, \"run_full\", lambda cfg: {\"portfolio\": [1, 2, 3]})\ntests/test_run_analysis.py:12:from trend_analysis.pipeline import Stats, calc_portfolio_returns, run_analysis\ntests/test_run_analysis.py:65:def test_run_analysis_basic():\ntests/test_run_analysis.py:67:    res = run_analysis(\ntests/test_run_analysis.py:76:def test_run_analysis_random_selection():\ntests/test_run_analysis.py:78:    res = run_analysis(\ntests/test_run_analysis.py:94:def test_run_analysis_returns_score_frame():\ntests/test_run_analysis.py:96:    res = run_analysis(\ntests/test_run_analysis.py:125:def test_run_analysis_detects_daily_frequency():\ntests/test_run_analysis.py:127:    res = run_analysis(\ntests/test_run_analysis.py:145:def test_run_analysis_missing_policy_zero_keeps_asset():\ntests/test_run_analysis.py:149:    dropped = run_analysis(\ntests/test_run_analysis.py:162:    filled = run_analysis(\ntests/test_run_analysis.py:179:def test_run_analysis_includes_regime_breakdown() -> None:\ntests/test_run_analysis.py:182:    res = run_analysis(\ntests/test_api_run_simulation_branches.py:46:    monkeypatch.setattr(api, \"_run_analysis\", lambda *_, **__: None)\ntests/test_api_run_simulation_branches.py:94:    def fake_run_analysis(*args: object, **kwargs: object) -> dict[str, object]:\ntests/test_api_run_simulation_branches.py:95:        captured[\"run_analysis_args\"] = (args, kwargs)\ntests/test_api_run_simulation_branches.py:114:    monkeypatch.setattr(api, \"_run_analysis\", fake_run_analysis)\ntests/test_api_run_simulation_branches.py:121:    args, kwargs = captured[\"run_analysis_args\"]\ntests/test_api_run_simulation_branches.py:137:    # The details object is exactly the payload returned by ``_run_analysis``.\ntests/test_api_run_simulation_branches.py:161:    monkeypatch.setattr(api, \"_run_analysis\", lambda *_, **__: payload)\ntests/test_api_run_simulation_branches.py:210:    monkeypatch.setattr(api, \"_run_analysis\", lambda *_, **__: payload)\ntests/test_api_run_simulation_branches.py:235:    monkeypatch.setattr(api, \"_run_analysis\", lambda *_, **__: payload)\ntests/test_api_run_simulation_branches.py:255:    monkeypatch.setattr(api, \"_run_analysis\", lambda *_, **__: payload)\ntests/test_api_run_simulation_branches.py:280:    monkeypatch.setattr(api, \"_run_analysis\", lambda *_, **__: make_payload())\ntests/test_run_full.py:6:from trend_analysis.pipeline import run, run_full\ntests/test_run_full.py:41:def test_run_full_matches_run(tmp_path):\ntests/test_run_full.py:43:    detailed = run_full(cfg)\ntests/test_run_full.py:51:def test_run_full_missing_csv_key(tmp_path):\ntests/test_run_full.py:64:        run_full(cfg)\ntests/test_run_full.py:67:def test_run_full_missing_file(tmp_path):\ntests/test_run_full.py:80:        run_full(cfg)\ntests/test_run_full.py:83:def test_run_full_with_benchmarks(tmp_path):\ntests/test_run_full.py:88:    res = run_full(cfg)\ntests/test_run_full.py:94:def test_run_full_respects_metric_registry(tmp_path):\ntests/test_run_full.py:98:    res = run_full(cfg)\ntests/test_rank_widgets.py:121:    def fake_run_analysis(*_args, **_kwargs):\ntests/test_rank_widgets.py:136:    monkeypatch.setattr(rank_widgets.pipeline, \"run_analysis\", fake_run_analysis)\ntests/test_pipeline.py:67:    out = pipeline.run(cfg)\ntests/test_pipeline.py:90:    out = pipeline.run(cfg)\ntests/test_pipeline.py:99:        pipeline.run(cfg)\ntests/test_pipeline.py:107:    res = pipeline.run_analysis(\ntests/test_pipeline.py:130:    res = pipeline._run_analysis(\ntests/test_pipeline.py:151:    result = pipeline.run(cfg)\ntests/test_pipeline.py:162:        pipeline.run(cfg)\ntests/test_pipeline.py:176:def test_run_analysis_none():\ntests/test_pipeline.py:177:    res = pipeline.run_analysis(\ntests/test_pipeline.py:183:def test_run_analysis_missing_date():\ntests/test_pipeline.py:186:        pipeline.run_analysis(df, \"2020-01\", \"2020-03\", \"2020-04\", \"2020-06\", 1.0, 0.0)\ntests/test_pipeline.py:189:def test_run_analysis_string_dates():\ntests/test_pipeline.py:192:    res = pipeline.run_analysis(\ntests/test_pipeline.py:205:def test_run_analysis_no_funds():\ntests/test_pipeline.py:209:    res = pipeline.run_analysis(\ntests/test_pipeline.py:222:def test_run_analysis_returns_none_when_window_missing():\ntests/test_pipeline.py:226:    res_in_empty = pipeline.run_analysis(\ntests/test_pipeline.py:239:    res_out_empty = pipeline.run_analysis(\ntests/test_pipeline.py:265:        pipeline.run(cfg)\ntests/test_pipeline.py:268:def test_run_analysis_custom_weights():\ntests/test_pipeline.py:270:    res = pipeline.run_analysis(\ntests/test_pipeline.py:284:def test_run_full_includes_risk_diagnostics(tmp_path):\ntests/test_pipeline.py:286:    res = pipeline.run_full(cfg)\ntests/test_pipeline.py:296:def test_run_full_robustness_settings_affect_weights(tmp_path):\ntests/test_pipeline.py:309:    res_rp = pipeline.run_full(cfg)\ntests/test_pipeline.py:316:    res_diag = pipeline.run_full(cfg)\ntests/test_pipeline.py:327:def test_run_full_robustness_condition_threshold_uses_cov_condition(tmp_path):\ntests/test_pipeline.py:373:    res = pipeline.run_full(cfg)\ntests/test_pipeline.py:426:def test_run_analysis_applies_constraints(monkeypatch):\ntests/test_pipeline.py:449:    res = pipeline.run_analysis(\ntests/test_pipeline.py:467:def test_run_analysis_constraint_failure_falls_back(monkeypatch):\ntests/test_pipeline.py:482:    res = pipeline.run_analysis(\ntests/test_pipeline.py:500:def test_run_analysis_injects_avg_corr_metric():\ntests/test_pipeline.py:508:    res = pipeline.run_analysis(\ntests/test_pipeline.py:530:def test_run_analysis_benchmark_ir_fallback(monkeypatch):\ntests/test_pipeline.py:544:    # Patch the module-level binding in pipeline.py so run_analysis sees our stub\ntests/test_pipeline.py:547:    res = pipeline.run_analysis(\ntests/trend_analysis/test_run_analysis_main.py:9:from trend_analysis import run_analysis\ntests/trend_analysis/test_run_analysis_main.py:40:    monkeypatch.setattr(run_analysis, \"load\", lambda path: cfg)\ntests/trend_analysis/test_run_analysis_main.py:43:        run_analysis.main([\"--config\", \"config.yml\"])\ntests/trend_analysis/test_run_analysis_main.py:72:    monkeypatch.setattr(run_analysis, \"load\", lambda path: cfg)\ntests/trend_analysis/test_run_analysis_main.py:73:    monkeypatch.setattr(run_analysis, \"load_csv\", fake_load_csv)\ntests/trend_analysis/test_run_analysis_main.py:81:        run_analysis.api,\ntests/trend_analysis/test_run_analysis_main.py:92:        run_analysis.export, \"format_summary_text\", lambda *a, **k: \"summary\"\ntests/trend_analysis/test_run_analysis_main.py:98:    monkeypatch.setattr(run_analysis.export, \"make_summary_formatter\", fake_formatter)\ntests/trend_analysis/test_run_analysis_main.py:100:        run_analysis.export,\ntests/trend_analysis/test_run_analysis_main.py:113:    monkeypatch.setattr(run_analysis.export, \"export_to_excel\", fake_export_to_excel)\ntests/trend_analysis/test_run_analysis_main.py:114:    monkeypatch.setattr(run_analysis.export, \"export_data\", fake_export_data)\ntests/trend_analysis/test_run_analysis_main.py:116:    exit_code = run_analysis.main([\"--config\", \"trend.yml\"])\ntests/trend_analysis/test_run_analysis_main.py:159:    monkeypatch.setattr(run_analysis, \"load\", lambda path: cfg)\ntests/trend_analysis/test_run_analysis_main.py:160:    monkeypatch.setattr(run_analysis, \"load_csv\", fake_load_csv)\ntests/trend_analysis/test_run_analysis_main.py:162:        run_analysis.api, \"run_simulation\", lambda cfg, df: _make_result()\ntests/trend_analysis/test_run_analysis_main.py:165:        run_analysis.export, \"format_summary_text\", lambda *a, **k: \"summary\"\ntests/trend_analysis/test_run_analysis_main.py:168:        run_analysis.export, \"make_summary_formatter\", lambda *a, **k: \"formatter\"\ntests/trend_analysis/test_run_analysis_main.py:171:        run_analysis.export,\ntests/trend_analysis/test_run_analysis_main.py:181:    monkeypatch.setattr(run_analysis.export, \"export_to_excel\", fake_export_to_excel)\ntests/trend_analysis/test_run_analysis_main.py:182:    monkeypatch.setattr(run_analysis.export, \"export_data\", lambda *a, **k: None)\ntests/trend_analysis/test_run_analysis_main.py:184:    exit_code = run_analysis.main([\"--config\", \"alias.yml\"])\ntests/trend_analysis/test_run_analysis_main.py:205:    monkeypatch.setattr(run_analysis, \"load\", lambda path: cfg)\ntests/trend_analysis/test_run_analysis_main.py:206:    monkeypatch.setattr(run_analysis, \"load_csv\", fake_load_csv)\ntests/trend_analysis/test_run_analysis_main.py:208:        run_analysis.api, \"run_simulation\", lambda cfg, df: _make_result()\ntests/trend_analysis/test_run_analysis_main.py:211:        run_analysis.export, \"format_summary_text\", lambda *a, **k: \"summary\"\ntests/trend_analysis/test_run_analysis_main.py:214:        run_analysis.export, \"make_summary_formatter\", lambda *a, **k: \"formatter\"\ntests/trend_analysis/test_run_analysis_main.py:217:        run_analysis.export, \"summary_frame_from_result\", lambda *a, **k: pd.DataFrame()\ntests/trend_analysis/test_run_analysis_main.py:223:        run_analysis.export,\ntests/trend_analysis/test_run_analysis_main.py:235:    monkeypatch.setattr(run_analysis.export, \"export_data\", fake_export_data)\ntests/trend_analysis/test_run_analysis_main.py:237:    exit_code = run_analysis.main([\"--config\", \"custom.yml\"])\ntests/trend_analysis/test_run_analysis_main.py:246:    monkeypatch.setattr(run_analysis, \"load\", lambda path: cfg)\ntests/trend_analysis/test_run_analysis_main.py:248:        run_analysis, \"load_csv\", lambda *a, **k: pd.DataFrame({\"value\": [1]})\ntests/trend_analysis/test_run_analysis_main.py:253:        run_analysis.api, \"run_simulation\", lambda cfg, df: empty_result\ntests/trend_analysis/test_run_analysis_main.py:259:    exit_code = run_analysis.main([\"--config\", \"trend.yml\", \"--detailed\"])\ntests/trend_analysis/test_run_analysis_main.py:268:    monkeypatch.setattr(run_analysis, \"load\", lambda path: cfg)\ntests/trend_analysis/test_run_analysis_main.py:269:    monkeypatch.setattr(run_analysis, \"load_csv\", lambda *a, **k: None)\ntests/trend_analysis/test_run_analysis_main.py:272:        run_analysis.main([\"--config\", \"trend.yml\"])\ntests/test_pipeline_indices.py:3:from trend_analysis.pipeline import run_analysis\ntests/test_pipeline_indices.py:20:    res = run_analysis(\ntests/test_pipeline_indices.py:36:    res = run_analysis(\ntests/test_pipeline_indices.py:52:    res = run_analysis(\ntests/trend_analysis/test_run_analysis.py:1:\"\"\"Targeted tests for :mod:`trend_analysis.run_analysis`.\"\"\"\ntests/trend_analysis/test_run_analysis.py:12:from trend_analysis import run_analysis as run_analysis_mod\ntests/trend_analysis/test_run_analysis.py:48:    monkeypatch.setattr(run_analysis_mod, \"load\", lambda path: cfg)\ntests/trend_analysis/test_run_analysis.py:51:        run_analysis_mod.main([\"-c\", \"config.yml\"])\ntests/trend_analysis/test_run_analysis.py:82:    monkeypatch.setattr(run_analysis_mod, \"load\", lambda path: sample_config)\ntests/trend_analysis/test_run_analysis.py:83:    monkeypatch.setattr(run_analysis_mod, \"load_csv\", fake_load_csv)\ntests/trend_analysis/test_run_analysis.py:85:        run_analysis_mod.api, \"run_simulation\", lambda cfg, frame: empty_result\ntests/trend_analysis/test_run_analysis.py:88:    rc = run_analysis_mod.main([\"-c\", \"config.yml\", \"--detailed\"])\ntests/trend_analysis/test_run_analysis.py:138:    monkeypatch.setattr(run_analysis_mod, \"load\", lambda path: sample_config)\ntests/trend_analysis/test_run_analysis.py:139:    monkeypatch.setattr(run_analysis_mod, \"load_csv\", fake_load_csv)\ntests/trend_analysis/test_run_analysis.py:141:        run_analysis_mod.api, \"run_simulation\", lambda cfg, frame: result\ntests/trend_analysis/test_run_analysis.py:144:        run_analysis_mod.export, \"format_summary_text\", lambda *a, **k: \"summary text\"\ntests/trend_analysis/test_run_analysis.py:147:        run_analysis_mod.export,\ntests/trend_analysis/test_run_analysis.py:152:        run_analysis_mod.export,\ntests/trend_analysis/test_run_analysis.py:157:        run_analysis_mod.export,\ntests/trend_analysis/test_run_analysis.py:167:    monkeypatch.setattr(run_analysis_mod.export, \"export_data\", fake_export_data)\ntests/trend_analysis/test_run_analysis.py:169:    rc = run_analysis_mod.main([\"-c\", \"config.yml\"])\ntests/trend_analysis/test_run_analysis.py:232:    monkeypatch.setattr(run_analysis_mod, \"load\", lambda path: sample_config)\ntests/trend_analysis/test_run_analysis.py:233:    monkeypatch.setattr(run_analysis_mod, \"load_csv\", fake_load_csv)\ntests/trend_analysis/test_run_analysis.py:235:        run_analysis_mod.api, \"run_simulation\", lambda cfg, frame: result\ntests/trend_analysis/test_run_analysis.py:238:    rc = run_analysis_mod.main([\"-c\", \"config.yml\", \"--detailed\"])\ntests/trend_analysis/test_run_analysis.py:260:    monkeypatch.setattr(run_analysis_mod, \"load\", lambda path: sample_config)\ntests/trend_analysis/test_run_analysis.py:261:    monkeypatch.setattr(run_analysis_mod, \"load_csv\", fake_load_csv)\ntests/trend_analysis/test_run_analysis.py:264:        run_analysis_mod.main([\"-c\", \"config.yml\"])\ntests/trend_analysis/test_run_analysis.py:293:    monkeypatch.setattr(run_analysis_mod, \"load\", lambda path: sample_config)\ntests/trend_analysis/test_run_analysis.py:294:    monkeypatch.setattr(run_analysis_mod, \"load_csv\", fake_load_csv)\ntests/trend_analysis/test_run_analysis.py:296:        run_analysis_mod.api,\ntests/trend_analysis/test_run_analysis.py:301:    rc = run_analysis_mod.main([\"-c\", \"config.yml\", \"--detailed\"])\ntests/trend_analysis/test_run_analysis.py:340:    monkeypatch.setattr(run_analysis_mod, \"load\", lambda path: sample_config)\ntests/trend_analysis/test_run_analysis.py:341:    monkeypatch.setattr(run_analysis_mod, \"load_csv\", fake_load_csv)\ntests/trend_analysis/test_run_analysis.py:343:        run_analysis_mod.api,\ntests/trend_analysis/test_run_analysis.py:348:    rc = run_analysis_mod.main([\"-c\", \"config.yml\", \"--detailed\"])\ntests/trend_analysis/test_run_analysis.py:380:    monkeypatch.setattr(run_analysis_mod, \"DEFAULT_OUTPUT_DIRECTORY\", str(tmp_path))\ntests/trend_analysis/test_run_analysis.py:381:    monkeypatch.setattr(run_analysis_mod, \"DEFAULT_OUTPUT_FORMATS\", [\"json\"])\ntests/trend_analysis/test_run_analysis.py:382:    monkeypatch.setattr(run_analysis_mod, \"load\", lambda path: sample_config)\ntests/trend_analysis/test_run_analysis.py:384:        run_analysis_mod,\ntests/trend_analysis/test_run_analysis.py:389:        run_analysis_mod.api, \"run_simulation\", lambda cfg, frame: result\ntests/trend_analysis/test_run_analysis.py:392:        run_analysis_mod.export, \"format_summary_text\", lambda *a, **k: \"summary\"\ntests/trend_analysis/test_run_analysis.py:395:        run_analysis_mod.export, \"make_summary_formatter\", lambda *a, **k: object()\ntests/trend_analysis/test_run_analysis.py:398:        run_analysis_mod.export,\ntests/trend_analysis/test_run_analysis.py:403:        run_analysis_mod.export,\ntests/trend_analysis/test_run_analysis.py:409:    rc = run_analysis_mod.main([\"-c\", \"config.yml\"])\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_22","type":"reasoning","text":"**Deciding where to add turnover cap test**"}}
{"type":"item.started","item":{"id":"item_23","type":"command_execution","command":"/bin/bash -lc \"sed -n '1,220p' tests/test_pipeline.py\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_23","type":"command_execution","command":"/bin/bash -lc \"sed -n '1,220p' tests/test_pipeline.py\"","aggregated_output":"import logging\n\nimport numpy as np\nimport pandas as pd\nimport pytest\n\nfrom trend_analysis import config, pipeline\nfrom trend_analysis.config import Config\nfrom trend_analysis.core.rank_selection import RiskStatsConfig, canonical_metric_list\nfrom trend_analysis.diagnostics import PipelineReasonCode\n\npytestmark = pytest.mark.runtime\n\n\npytestmark = pytest.mark.runtime\n\n\ndef make_cfg(tmp_path, df):\n    csv = tmp_path / \"data.csv\"\n    df.to_csv(csv, index=False)\n    cfg_dict = {\n        \"version\": \"1\",\n        \"data\": {\n            \"csv_path\": str(csv),\n            \"date_column\": \"Date\",\n            \"frequency\": \"M\",\n            \"risk_free_column\": \"RF\",\n            \"allow_risk_free_fallback\": False,\n        },\n        \"preprocessing\": {},\n        \"vol_adjust\": {\"target_vol\": 1.0},\n        \"sample_split\": {\n            \"in_start\": \"2020-01\",\n            \"in_end\": \"2020-03\",\n            \"out_start\": \"2020-04\",\n            \"out_end\": \"2020-06\",\n        },\n        \"portfolio\": {\n            \"selection_mode\": \"all\",\n            \"rebalance_calendar\": \"NYSE\",\n            \"max_turnover\": 0.5,\n            \"transaction_cost_bps\": 10,\n        },\n        \"metrics\": {},\n        \"export\": {},\n        \"run\": {},\n    }\n    return Config(**cfg_dict)\n\n\ndef make_df():\n    dates = pd.date_range(\"2020-01-31\", periods=6, freq=\"ME\")\n    return pd.DataFrame(\n        {\n            \"Date\": dates,\n            \"RF\": 0.0,\n            \"A\": [0.02, 0.01, 0.03, 0.04, 0.02, 0.01],\n        }\n    )\n\n\nRUN_ANALYSIS_KWARGS = {\"risk_free_column\": \"RF\", \"allow_risk_free_fallback\": False}\n\n\ndef test_run_returns_dataframe(tmp_path):\n    cfg = make_cfg(tmp_path, make_df())\n    out = pipeline.run(cfg)\n    assert not out.empty\n    columns = set(out.columns)\n    expected = {\n        \"cagr\",\n        \"vol\",\n        \"sharpe\",\n        \"sortino\",\n        \"information_ratio\",\n        \"max_drawdown\",\n    }\n    assert expected.issubset(columns)\n    # Some pipeline configurations now emit in/out-of-sample average correlation\n    # diagnostics.  If present they should be the only additional columns.\n    extra = columns - expected\n    assert extra <= {\"is_avg_corr\", \"os_avg_corr\"}\n\n\ndef test_run_with_benchmarks(tmp_path):\n    df = make_df()\n    df[\"SPX\"] = 0.01\n    cfg = make_cfg(tmp_path, df)\n    cfg.benchmarks = {\"spx\": \"SPX\"}\n    out = pipeline.run(cfg)\n    assert \"ir_spx\" in out.columns\n\n\ndef test_run_errors_when_risk_free_column_missing(tmp_path):\n    cfg = make_cfg(tmp_path, make_df())\n    cfg.data[\"risk_free_column\"] = \"MissingRF\"\n    cfg.data[\"allow_risk_free_fallback\"] = False\n    with pytest.raises(ValueError, match=\"Configured risk-free column 'MissingRF'\"):\n        pipeline.run(cfg)\n\n\ndef test_risk_free_series_used_for_score_frame():\n    dates = pd.date_range(\"2020-01-31\", periods=4, freq=\"ME\")\n    rf = pd.Series([0.001, 0.001, 0.002, 0.002], index=dates)\n    a = pd.Series([0.011, 0.012, 0.01, 0.009], index=dates)\n    df = pd.DataFrame({\"Date\": dates, \"RF\": rf, \"A\": a, \"B\": 0.0})\n    res = pipeline.run_analysis(\n        df,\n        \"2020-01\",\n        \"2020-02\",\n        \"2020-03\",\n        \"2020-04\",\n        1.0,\n        0.0,\n        risk_free_column=\"RF\",\n        allow_risk_free_fallback=False,\n    )\n\n    assert res is not None\n    expected_sharpe = pipeline.sharpe_ratio(\n        a.iloc[:2], risk_free=rf.iloc[:2], periods_per_year=12\n    )\n    assert res[\"score_frame\"].loc[\"A\", \"Sharpe\"] == pytest.approx(expected_sharpe)\n\n\ndef test_risk_free_fallback_logs_choice(caplog):\n    caplog.set_level(logging.INFO, logger=\"trend_analysis.pipeline\")\n    df = _make_two_fund_df()\n\n    res = pipeline._run_analysis(\n        df,\n        \"2020-01\",\n        \"2020-03\",\n        \"2020-04\",\n        \"2020-06\",\n        1.0,\n        0.0,\n        allow_risk_free_fallback=True,\n        risk_free_column=None,\n    )\n\n    assert res is not None\n    assert \"fallback enabled\" in caplog.text\n\n\ndef test_run_returns_empty_when_no_funds(tmp_path, monkeypatch):\n    df = pd.DataFrame(\n        {\"Date\": pd.date_range(\"2020-01-31\", periods=2, freq=\"ME\"), \"RF\": 0.0}\n    )\n    cfg = make_cfg(tmp_path, df)\n    result = pipeline.run(cfg)\n    assert result.empty\n    diagnostic = result.attrs.get(\"diagnostic\")\n    assert diagnostic is not None\n    assert diagnostic.reason_code == PipelineReasonCode.SAMPLE_WINDOW_EMPTY.value\n\n\ndef test_run_file_missing(tmp_path, monkeypatch):\n    cfg = make_cfg(tmp_path, make_df())\n    cfg.data[\"csv_path\"] = str(tmp_path / \"missing.csv\")\n    with pytest.raises(FileNotFoundError):\n        pipeline.run(cfg)\n\n\ndef test_env_override(tmp_path, monkeypatch):\n    df = make_df()\n    cfg = make_cfg(tmp_path, df)\n    cfg_yaml = tmp_path / \"c.yml\"\n    cfg_yaml.write_text(cfg.model_dump_json())\n    monkeypatch.setenv(\"TREND_CFG\", str(cfg_yaml))\n    loaded_env = config.load()\n    assert loaded_env.version == cfg.version\n    monkeypatch.delenv(\"TREND_CFG\", raising=False)\n\n\ndef test_run_analysis_none():\n    res = pipeline.run_analysis(\n        None, \"2020-01\", \"2020-03\", \"2020-04\", \"2020-06\", 1.0, 0.0\n    )\n    assert res.unwrap() is None\n\n\ndef test_run_analysis_missing_date():\n    df = pd.DataFrame({\"A\": [1, 2]})\n    with pytest.raises(ValueError):\n        pipeline.run_analysis(df, \"2020-01\", \"2020-03\", \"2020-04\", \"2020-06\", 1.0, 0.0)\n\n\ndef test_run_analysis_string_dates():\n    df = make_df()\n    df[\"Date\"] = df[\"Date\"].astype(str)\n    res = pipeline.run_analysis(\n        df,\n        \"2020-01\",\n        \"2020-03\",\n        \"2020-04\",\n        \"2020-06\",\n        1.0,\n        0.0,\n        **RUN_ANALYSIS_KWARGS,\n    )\n    assert res is not None\n\n\ndef test_run_analysis_no_funds():\n    df = pd.DataFrame(\n        {\"Date\": pd.date_range(\"2020-01-31\", periods=3, freq=\"ME\"), \"RF\": 0.0}\n    )\n    res = pipeline.run_analysis(\n        df,\n        \"2020-01\",\n        \"2020-02\",\n        \"2020-03\",\n        \"2020-03\",\n        1.0,\n        0.0,\n        **RUN_ANALYSIS_KWARGS,\n    )\n    assert res.unwrap() is None\n\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_24","type":"reasoning","text":"**Designing and planning test for max_turnover propagation**"}}
{"type":"item.completed","item":{"id":"item_25","type":"file_change","changes":[{"path":"/home/runner/work/Trend_Model_Project/Trend_Model_Project/tests/test_pipeline.py","kind":"update"}],"status":"completed"}}
{"type":"item.completed","item":{"id":"item_26","type":"reasoning","text":"**Running targeted tests**"}}
{"type":"item.started","item":{"id":"item_27","type":"command_execution","command":"/bin/bash -lc 'pytest tests/test_pipeline.py -m \"not slow\"'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_27","type":"command_execution","command":"/bin/bash -lc 'pytest tests/test_pipeline.py -m \"not slow\"'","aggregated_output":"mkdir -p failed for path /home/runner/.config/matplotlib: [Errno 13] Permission denied: '/home/runner/.config/matplotlib'\nMatplotlib created a temporary cache directory at /tmp/matplotlib-ksvb82md because there was an issue with the default path (/home/runner/.config/matplotlib); it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\nINTERNALERROR> Traceback (most recent call last):\nINTERNALERROR>   File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/_pytest/main.py\", line 314, in wrap_session\nINTERNALERROR>     config._do_configure()\nINTERNALERROR>   File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/_pytest/config/__init__.py\", line 1159, in _do_configure\nINTERNALERROR>     self.hook.pytest_configure.call_historic(kwargs=dict(config=self))\nINTERNALERROR>   File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/pluggy/_hooks.py\", line 534, in call_historic\nINTERNALERROR>     res = self._hookexec(self.name, self._hookimpls.copy(), kwargs, False)\nINTERNALERROR>           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nINTERNALERROR>   File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/pluggy/_manager.py\", line 120, in _hookexec\nINTERNALERROR>     return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\nINTERNALERROR>            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nINTERNALERROR>   File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/pluggy/_callers.py\", line 167, in _multicall\nINTERNALERROR>     raise exception\nINTERNALERROR>   File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/pluggy/_callers.py\", line 121, in _multicall\nINTERNALERROR>     res = hook_impl.function(*args)\nINTERNALERROR>           ^^^^^^^^^^^^^^^^^^^^^^^^^\nINTERNALERROR>   File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/pytest_rerunfailures.py\", line 347, in pytest_configure\nINTERNALERROR>     config.failures_db = ServerStatusDB()\nINTERNALERROR>                          ^^^^^^^^^^^^^^^^\nINTERNALERROR>   File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/pytest_rerunfailures.py\", line 451, in __init__\nINTERNALERROR>     super().__init__()\nINTERNALERROR>   File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/site-packages/pytest_rerunfailures.py\", line 432, in __init__\nINTERNALERROR>     self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\nINTERNALERROR>                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nINTERNALERROR>   File \"/opt/hostedtoolcache/Python/3.11.14/x64/lib/python3.11/socket.py\", line 232, in __init__\nINTERNALERROR>     _socket.socket.__init__(self, family, type, proto, fileno)\nINTERNALERROR> PermissionError: [Errno 1] Operation not permitted\n","exit_code":3,"status":"failed"}}
{"type":"item.completed","item":{"id":"item_28","type":"reasoning","text":"**Troubleshooting pytest rerunfailures plugin**"}}
{"type":"item.started","item":{"id":"item_29","type":"command_execution","command":"/bin/bash -lc 'pytest tests/test_pipeline.py -m \"not slow\" -p no:rerunfailures'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_29","type":"command_execution","command":"/bin/bash -lc 'pytest tests/test_pipeline.py -m \"not slow\" -p no:rerunfailures'","aggregated_output":"============================= test session starts ==============================\nplatform linux -- Python 3.11.14, pytest-9.0.2, pluggy-1.6.0\nrootdir: /home/runner/work/Trend_Model_Project/Trend_Model_Project\nconfigfile: pytest.ini (WARNING: ignoring pytest config in pyproject.toml!)\nplugins: anyio-4.12.0, cov-7.0.0, xdist-3.8.0, hypothesis-6.148.9, typeguard-4.4.4, langsmith-0.6.0\ncollected 23 items\n\ntests/test_pipeline.py .......................                           [100%]\n\n============================== 23 passed in 1.82s ==============================\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_30","type":"reasoning","text":"**Preparing to commit changes**"}}
{"type":"item.started","item":{"id":"item_31","type":"command_execution","command":"/bin/bash -lc 'git status --short'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_31","type":"command_execution","command":"/bin/bash -lc 'git status --short'","aggregated_output":" M codex-session-4138.jsonl\n M tests/test_pipeline.py\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_32","type":"reasoning","text":"**Handling unexpected file modifications**\n\nI found an unexpected change in `codex-session-4138.jsonl` likely from an automated tool, which I didnâ€™t explicitly make. The instructions say to stop and ask the user how to proceed in such cases, so Iâ€™m planning to mention the change, confirm that the test file `tests/test_pipeline.py` was updated and tested, and ask for guidance on whether to keep, revert, or ignore the session file modification before committing. Iâ€™ll also recap completed and remaining tasks and note the test command used including the rerunfailures plugin issue."}}
{"type":"item.completed","item":{"id":"item_33","type":"agent_message","text":"Added a pipeline-level test that captures the `max_turnover` from config and asserts it reaches `_enforce_turnover_cap()` via the risk module, in `tests/test_pipeline.py`. This gives a concrete trace from config â†’ pipeline â†’ risk enforcement.\n\nTests: `pytest tests/test_pipeline.py -m \"not slow\" -p no:rerunfailures` (rerunfailures plugin hit a socket permission error without `-p no:rerunfailures`).\n\nI noticed an unexpected modified file: `codex-session-4138.jsonl`. I didnâ€™t edit it directly. How would you like me to handle it (leave uncommitted, revert, or include)?\n\nProgress recap:\n- Completed: Trace `max_turnover` through the pipeline to `_enforce_turnover_cap()` (via new test).\n- Remaining: Check bypass behavior when natural turnover is low; fix enforcement if needed; ensure period-level turnover values respect the cap.\n\nIf you want me to proceed, tell me how to handle `codex-session-4138.jsonl` and Iâ€™ll commit the test change."}}
{"type":"turn.completed","usage":{"input_tokens":461626,"cached_input_tokens":400512,"output_tokens":4582}}
