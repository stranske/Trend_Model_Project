# agents.md
"""
YOU ARE CODEX.  EXTEND THE VOL_ADJ_TREND_ANALYSIS PROJECT AS FOLLOWS
--------------------------------------------------------------------

High‚Äëlevel goal
~~~~~~~~~~~~~~~
Add a **performance‚Äëbased manager‚Äëselection mode** that works alongside the
existing 'all', 'random', and 'manual' modes.  Make the pipeline fully
config‚Äëdriven and keep everything vectorised.

Functional spec
~~~~~~~~~~~~~~~
1.  New selection mode keyword:  `rank`.
    ‚Ä¢ Works on the *in‚Äësample* window **after** the usual data‚Äëquality filters.  
    ‚Ä¢ Supported inclusion approaches:
         - `'top_n'`       ‚Äì keep the N best funds.
         - `'top_pct'`     ‚Äì keep the top P‚ÄØpercent.  
         - `'threshold'`   ‚Äì keep funds whose score ‚â• user threshold
           (this is the ‚Äúuseful extra‚Äù beyond N and percentile).

2.  Rank criteria (`score_by`):
    ‚Ä¢ Any single metric registered in `METRIC_REGISTRY`
      (e.g. 'Sharpe', 'AnnualReturn', 'MaxDrawdown', ‚Ä¶).  
    ‚Ä¢ Special value `'blended'` that combines up to three metrics with
      user‚Äësupplied *positive* weights (weights will be normalised to 1.0).

3.  Direction‚Äëof‚Äëmerit:
    ‚Ä¢ Metrics where ‚Äúlarger is better‚Äù  ‚Üí rank descending.
    ‚Ä¢ Metrics where ‚Äúsmaller is better‚Äù (currently **only** MaxDrawdown)  
      ‚Üí rank ascending.  Future metrics can extend `ASCENDING_METRICS`.

4.  Config file (YAML) drives everything ‚Äì sample below.

5.  UI flow (ipywidgets, no external deps):
    Step 1  ‚Äì Mode (‚Äòall‚Äô, ‚Äòrandom‚Äô, ‚Äòmanual‚Äô, **‚Äòrank‚Äô**),
               checkboxes for ‚Äúvol‚Äëadj‚Äù and ‚Äúuse ranking‚Äù.  
    Step 2  ‚Äì If mode == 'rank' **or** user ticked ‚Äúuse ranking‚Äù
               ‚Üí reveal controls for `inclusion_approach`,
               `score_by`, `N / Pct / Threshold`, and (if blended)
               three sliders for weights + metric pickers.  
    Step 3  ‚Äì If mode == 'manual'  
               ‚Üí display an interactive DataFrame of the IS scores so the
               user can override selection and set weights.
    Step 4  ‚Äì Output format picker (csv / xlsx / json) then fire
               `run_analysis()` and `export_to_*`.

6.  No broken changes:
    ‚Ä¢ Default behaviour (config absent) must be identical to current build.
    ‚Ä¢ All heavy computation stays in NumPy / pandas vector land.

7.  Unit‚Äëtest hooks:
    ‚Ä¢ New pure functions must be import‚Äësafe and testable without widgets.
      (e.g. `rank_select_funds()`).

Sample YAML
~~~~~~~~~~~
selection:
  mode: rank               # all | random | manual | rank
  random_n: 12             # only if mode == random
  use_vol_adjust: true
rank:
  inclusion_approach: top_n     # top_n | top_pct | threshold
  n: 8                          # for top_n
  pct: 0.10                     # for top_pct  (decimal, not %)
  threshold: 1.5                # ignored unless approach == threshold
  score_by: blended             # Sharpe | AnnualReturn | ‚Ä¶ | blended
  blended_weights:
    Sharpe: 0.5
    AnnualReturn: 0.3
    MaxDrawdown: 0.2
output:
  format: excel                 # csv | excel | json
"""

"""
üîÑ 2025‚Äë06‚Äë15 UPDATE ‚Äî PHASE‚Äë1 ENHANCEMENTS
------------------------------------------
‚Ä¢ Blended ranking **must** use *z‚Äëscores* (mean‚Äë0, stdev‚Äë1) before the
  weighted sum so metrics on different scales are commensurable.
‚Ä¢ MaxDrawdown is currently the only ‚Äúsmaller‚Äëis‚Äëbetter‚Äù metric; the
  ASCENDING_METRICS set remains {"MaxDrawdown"} until further notice.
‚Ä¢ Config format stays YAML.
"""# agents.md
## Mission
Converge the scattered modules into one fully‚Äëtest‚Äëcovered, vectorised pipeline that can be invoked from a single CLI entry‚Äëpoint.
Never touch notebooks living under any directory whose name ends in old/.

---

### 2025-06-27 UPDATE ‚Äî RISK-METRICS EXPORT (SERIOUSLY, LEAVE THIS IN)

Codex removed the pretty reporting layer once; it shall not happen again.  
Follow these guard-rails whenever you touch export logic.

1. **Call the canonical exporters**  
   After `pipeline.run()` completes, pipe the returned `Mapping[str, pd.DataFrame]`
   into **exactly one** of  
   `trend_analysis.export.export_to_excel | export_to_csv | export_to_json`.

2. **Sheet / file keys**  

3. **Excel format contract**  
* Generate the summary sheet formatter via
  `trend_analysis.export.make_summary_formatter(...)`.
* Register any other sheet formatter with `@register_formatter_excel`
  so `export_to_excel` auto-hooks it.
* Required cosmetics:  
  - bold title row,  
  - `0.00%` for CAGR & Vol,  
  - `0.00` for Sharpe & Sortino,  
  - red numerals for MaxDD,  
  - freeze panes on header,  
  - auto-filter,  
  - column width = `max(len(header)) + 2`.

4. **Column order = law**  
Tests must fail if this order mutates.

5. **Config switches**  
`output.format` = `excel | csv | json`  
`output.path`   = prefix used by exporter (Excel auto-appends `.xlsx`).

6. **Tests**  
* In-memory smoke test: write to `BytesIO`, assert two sheets and cell
  `A1 == "Vol-Adj Trend Analysis"`.
* Regression test: `assert list(df.columns) == EXPECTED_COLUMNS`.

7. **Back-compat**  
Silent config = drop the fully formatted Excel workbook into `outputs/`
exactly as v1.0 did. Breaking that throws `ExportError`.

> üõ°Ô∏è  If you rip out these formatters again, CI will chaperone you with a failing gate and a stern commit message.


## | Layer / concern                      | **Canonical location**                                                     | Everything else is **deprecated**                         |
| ------------------------------------ | -------------------------------------------------------------------------- | --------------------------------------------------------- |
| **Data ingest & cleaning**           | `trend_analysis/data.py` <br>‚ÄØ(alias exported as `trend_analysis.data`)    | `data_utils.py`, helper code in notebooks or `scripts/`   |
| **Portfolio logic‚ÄØ& metrics**        | `trend_analysis/metrics.py` (vectorised)                                   | loops inside `run_analysis.py`, ad‚Äëhoc calcs in notebooks |
| **Export / I/O**                     | `trend_analysis/export.py`                                                 | the root‚Äëlevel `exports.py`, snippets inside notebooks    |
| **Domain kernels (fast primitives)** | `trend_analysis/core/` package                                             | stand‚Äëalone modules under the top‚Äëlevel `core/` directory |
| **Pipeline orchestration**           | `trend_analysis/pipeline.py` (pure)                                        | any duplicated control flow elsewhere                     |
| **CLI entry‚Äëpoint**                  | `run_analysis.py` **only** (thin wrapper around `trend_analysis.cli:main`) | bespoke `scripts/*.py` entry points                       |
| **Config**                           | `config/defaults.yml` loaded through `trend_analysis.config.load()`        | hard‚Äëcoded constants, magic numbers in notebooks          |
| **Tests**                            | `tests/` (pytest; 100‚ÄØ% branch‚Äëaware coverage gate)                        |    ‚Äî                                                      |
One concern ‚Üí one module.
Replacements must delete or comment‚Äëout whatever they obsolete in the same PR.

Immediate Refactor Tasks
Flatten duplications

Rename data_utils.py ‚Üí trend_analysis/data.py, adjust imports, delete the original.

Migrate the contents of the top‚Äëlevel exports.py into trend_analysis/export.py; keep only a re‚Äëexport stub for one minor release.

Turn the stray core/ directory into an importable sub‚Äëpackage:
core/indicator.py ‚Üí trend_analysis/core/indicator.py, etc.

Single pipeline

Implement trend_analysis/pipeline.py exposing a pure function
run(config: Config) -> pd.DataFrame.

run_analysis.py should parse CLI args, build a Config, pass it to pipeline.run, then handle pretty printing / file output only.

Config resolution

# trend_analysis/config.py
from pydantic import BaseModel
class Config(BaseModel):
    defaults: str = Path(__file__).with_name("..").joinpath("config/defaults.yml")
    # ...other validated fields...
def load(path: str | None = None) -> Config: ...

Env‚Äëvar override: TREND_CFG=/path/to/override.yml run_analysis ...

Dependency hygiene

Heavy imports (numpy, pandas, scipy) at top of each module are fine.

No circular imports. pipeline.py orchestrates; nothing imports it.

Tests

NOTE: Test fixtures must be text-serialised (CSV/JSON); no binary formats in PRs.

Require 100‚ÄØ% branch coverage on trend_analysis/* via pytest‚Äëcov in CI.

Conventions & Guard‚Äërails
Vectorise first.
Falling back to for‚Äëloops requires a comment justifying why vectorisation is impossible or harmful.

Public API (exported in __all__) uses US‚ÄëEnglish snake‚Äëcase; private helpers are prefixed with _.

Notebook hygiene: any new exploratory notebook must start with the header
# üî¨ scratchpad ‚Äì may be deleted at any time.

CI (GitHub Actions) stages to add:

lint  (ruff + black ‚Äì‚Äëcheck)

type‚Äëcheck (mypy, strict)

test (pytest ‚Äë‚Äëcov trend_analysis ‚Äë‚Äëcov‚Äëbranch)

build‚Äëwheel (tags only)

##NEW

### ‚ú® Task: Integrate `information_ratio` end‚Äëto‚Äëend  (#metrics‚ÄëIR)

**Motivation**  
Phase‚Äë1 now includes a vectorised `information_ratio` metric.  
It is fully unit‚Äëtested but not yet surfaced in the CLI / Excel export or
multi‚Äëbenchmark workflows.

---

#### 1.  Pipeline / Statistics

* [x] Extend `_Stats` dataclass with `information_ratio: float`.
* [x] In `_compute_stats()` compute `information_ratio(df[col], rf_series)`.
* [x] Ensure `out_stats_df` includes the new field.

#### 2.  Multi‚Äëbenchmark support

* [x] Accept `benchmarks:` mapping in YAML cfg, e.g.

```yaml
benchmarks:
  spx: SPX
  tsx: TSX

### 2025‚Äë07‚Äë03 UPDATE¬†‚Äî STEP‚ÄØ4: surface a real `score_frame`

* **Add** `single_period_run()` to **`trend_analysis/pipeline.py`**  
  * **Signature**  
    ```python
    def single_period_run(
        df: pd.DataFrame,
        start: str,
        end: str,
        *,
        stats_cfg: "RiskStatsConfig" | None = None
    ) -> pd.DataFrame:
        ...
    ```
  * **Behaviour**
    1. Slice *df* to `[start,‚ÄØend]` (inclusive) and drop the Date column into the index.  
    2. For every metric listed in `stats_cfg.metrics_to_run` **call the public registry** (`core.rank_selection._compute_metric_series`) to obtain a vectorised series.  
    3. **Concatenate** those series column‚Äëwise into **`score_frame`** (`index = fund code`, `columns = metric names`, dtype¬†`float64`).  
    4. Attach metadata  
       ```python
       score_frame.attrs["insample_len"] = len(window)        # number of bars
       score_frame.attrs["period"] = (start, end)             # optional helper
       ```
    5. Return `score_frame` ‚Äì *no side effects, no I/O*.

* **Update callers**
  * `pipeline._run_analysis()` should call `single_period_run()` once, stash the resulting frame in the returned dict under key `"score_frame"`, but **must not** change existing outputs or CLI flags.
  * Existing metrics‚Äëexport logic stays exactly as is.

* **Tests**
  1. **Golden‚Äëmaster**: compare the new `score_frame` against a pre‚Äëgenerated CSV fixture for a small sample set.  
  2. **Metadata**: `assert sf.attrs["insample_len"] == expected_len`.  
  3. **Column order** equals the order of `stats_cfg.metrics_to_run`; failing this should raise.

* **Performance / style guard‚Äërails**
  * Remain fully vectorised‚Äîno per‚Äëfund Python loops.
  * Keep `single_period_run()` *pure* (no global writes, no prints).
  * Do **not** introduce extra dependencies; stick to `numpy`‚ÄØ+‚ÄØ`pandas`.

> Once the test suite passes with the new `score_frame`, proceed to steps‚ÄØ5‚Äë7 (Selector & Weighting classes).




```

### 2025-07-10 UPDATE ‚Äî multi-period exports

Phase¬†2 rolls periods sequentially. Each period should output the same metrics
table **and formatting** as Phase¬†1, but all periods share one workbook.
Excel exports must create a tab per period plus a final *summary* tab that
shows combined portfolio performance using the identical column layout.
CSV and JSON exports merge all sheets into a single file (with a ``sheet``
column or key) instead of multiple files.

