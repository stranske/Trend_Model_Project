{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad16ef1f-070a-4460-8256-66bb3eb75a13",
   "metadata": {},
   "source": [
    "1) trendlib/config.py\n",
    "Purpose: Centralize user-defined parameters (dates, frequency, toggles, file paths, etc.).\n",
    "\n",
    "<details> <summary><strong>Click to show code</strong></summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2a9b94-afdb-47f7-8e47-aa10b20c277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trendlib/config.py\n",
    "\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any\n",
    "\n",
    "DEFAULT_CONFIG: Dict[str, Any] = {\n",
    "    # Paths\n",
    "    \"data_file\": \"Data/YourData.csv\",  # NOTE: Adjust if different\n",
    "    \"date_column\": \"Date\",            # If your code uses \"Date\"\n",
    "    \"manager_column\": \"Ticker\",       # NOTE: If you have \"Ticker\" or \"Symbol\"\n",
    "\n",
    "    # Analysis window\n",
    "    \"analysis_start_date\": \"2000-01-01\",\n",
    "    \"analysis_end_date\":   \"2020-12-31\",\n",
    "\n",
    "    # Frequency / Rolling windows (Phase 2 usage)\n",
    "    \"in_sample_length\":  12,  # months? days? reference your code\n",
    "    \"out_sample_length\": 6,\n",
    "\n",
    "    # Volatility toggle\n",
    "    \"volatility_adjusted\": True,\n",
    "\n",
    "    # Demo / Test Mode\n",
    "    \"demo_mode\": False,\n",
    "    \"demo_start_date\": \"2019-01-01\",\n",
    "    \"demo_end_date\":   \"2019-06-30\",\n",
    "    \"demo_num_managers\": 5,\n",
    "\n",
    "    # Export Paths\n",
    "    \"excel_path\": \"output/Trend_Analysis.xlsx\",\n",
    "    \"csv_path\":   \"output/Trend_Analysis.csv\",\n",
    "    \"json_path\":  \"output/Trend_Analysis.json\"\n",
    "}\n",
    "\n",
    "def validate_config(cfg: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Light checks to ensure logic consistency.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check date ordering\n",
    "        start = datetime.fromisoformat(cfg[\"analysis_start_date\"])\n",
    "        end   = datetime.fromisoformat(cfg[\"analysis_end_date\"])\n",
    "        if start >= end:\n",
    "            raise ValueError(\"analysis_start_date must be before analysis_end_date.\")\n",
    "\n",
    "        if cfg[\"demo_mode\"]:\n",
    "            ds = datetime.fromisoformat(cfg[\"demo_start_date\"])\n",
    "            de = datetime.fromisoformat(cfg[\"demo_end_date\"])\n",
    "            if ds >= de:\n",
    "                raise ValueError(\"demo_start_date must be before demo_end_date.\")\n",
    "\n",
    "    except KeyError as ke:\n",
    "        raise ValueError(f\"Missing config parameter: {ke}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac4a4c4-7473-4332-8a29-8f5255fcac2e",
   "metadata": {},
   "source": [
    "2) trendlib/data_io.py\n",
    "Purpose: Load, subset, and clean the data. Integrates your logic from “Load Data” and “Prep Data” steps in the notebook.\n",
    "\n",
    "<details> <summary><strong>Click to show code</strong></summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a35db84-5bf7-441c-bf35-8e0f1f564459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trendlib/data_io.py\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "from typing import Dict\n",
    "from trendlib.config import DEFAULT_CONFIG\n",
    "\n",
    "def load_data(config: Dict = DEFAULT_CONFIG) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads the main dataset from CSV, sets index to date, \n",
    "    filters to [analysis_start_date, analysis_end_date].\n",
    "    If demo_mode=True, further subsets date & managers.\n",
    "    \"\"\"\n",
    "    # 1. Read CSV\n",
    "    df = pd.read_csv(\n",
    "        config[\"data_file\"],\n",
    "        parse_dates=[config[\"date_column\"]]\n",
    "    )\n",
    "    \n",
    "    df.set_index(config[\"date_column\"], inplace=True, drop=True)\n",
    "\n",
    "    # 2. Limit by analysis start/end\n",
    "    start_date = config[\"analysis_start_date\"]\n",
    "    end_date   = config[\"analysis_end_date\"]\n",
    "    df = df.loc[start_date:end_date]\n",
    "\n",
    "    # 3. Basic cleaning from the original notebook logic\n",
    "    # NOTE: Insert your original \"dropna\" or renaming code as needed.\n",
    "    # For instance:\n",
    "    # df.dropna(subset=[\"Close\"], inplace=True)\n",
    "    # or reference \"Return\" columns from your code.\n",
    "\n",
    "    # 4. Demo/test mode subsetting\n",
    "    if config.get(\"demo_mode\", False):\n",
    "        demo_start = config[\"demo_start_date\"]\n",
    "        demo_end   = config[\"demo_end_date\"]\n",
    "        df = df.loc[demo_start:demo_end]\n",
    "\n",
    "        manager_col = config[\"manager_column\"]\n",
    "        all_managers = df[manager_col].unique().tolist()\n",
    "        num_m = min(config[\"demo_num_managers\"], len(all_managers))\n",
    "        chosen = random.sample(all_managers, num_m)\n",
    "        df = df[df[manager_col].isin(chosen)]\n",
    "\n",
    "        print(f\"[Demo Mode] Data from {demo_start} to {demo_end}, \"\n",
    "              f\"{num_m} managers chosen out of {len(all_managers)} total.\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6771c378-8b68-4e77-8a70-df20162a66b2",
   "metadata": {},
   "source": [
    "3) trendlib/metrics.py\n",
    "Purpose: Move your performance calculations (e.g., volatility, returns, Sharpe, drawdown) here.\n",
    "\n",
    "<details> <summary><strong>Click to show code</strong></summary>\n",
    "python\n",
    "Copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7868e68-d546-4514-a7e8-ae26622c1256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trendlib/metrics.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "\n",
    "def calculate_returns(df: pd.DataFrame, price_col: str = \"Close\") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Example placeholder: compute daily returns from a 'Close' column.\n",
    "    Adjust to match your actual logic from the notebook.\n",
    "    \"\"\"\n",
    "    return df[price_col].pct_change().fillna(0)\n",
    "\n",
    "\n",
    "def annualized_volatility(returns: pd.Series, periods_per_year=252) -> float:\n",
    "    \"\"\"\n",
    "    Standard annualized volatility = std(returns) * sqrt(periods_per_year).\n",
    "    \"\"\"\n",
    "    if returns.empty:\n",
    "        return np.nan\n",
    "    return returns.std() * np.sqrt(periods_per_year)\n",
    "\n",
    "\n",
    "def sharpe_ratio(returns: pd.Series, risk_free_annual=0.0, periods_per_year=252) -> float:\n",
    "    if returns.empty:\n",
    "        return np.nan\n",
    "    # Convert annual risk-free to each period\n",
    "    rf_period = risk_free_annual / periods_per_year\n",
    "    excess = returns - rf_period\n",
    "    vol = annualized_volatility(excess, periods_per_year)\n",
    "    if vol == 0:\n",
    "        return np.nan\n",
    "    return (excess.mean() * periods_per_year) / vol\n",
    "\n",
    "\n",
    "def max_drawdown(returns: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    If you track cumulative returns, compute max drawdown.\n",
    "    Insert your code from notebook.\n",
    "    \"\"\"\n",
    "    if returns.empty:\n",
    "        return np.nan\n",
    "    cum = (1 + returns).cumprod()\n",
    "    peak = cum.cummax()\n",
    "    drawdown = (cum - peak) / peak\n",
    "    return drawdown.min()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32bfc88-fc0f-4b1b-a90a-8bdefc334d3b",
   "metadata": {},
   "source": [
    "(Customize these to match your real variable names and logic from the notebook.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e5d0b0-9bd5-4350-9718-91025b3201a5",
   "metadata": {},
   "source": [
    "4) trendlib/exports.py\n",
    "Purpose: Provide consistent, try/except-wrapped export to Excel/CSV/JSON.\n",
    "\n",
    "<details> <summary><strong>Click to show code</strong></summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0188c14-0234-47c5-bedc-9bb54315d0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trendlib/exports.py\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def to_excel(df: pd.DataFrame, path: str) -> None:\n",
    "    \"\"\"\n",
    "    Write the DataFrame to Excel with minimal error handling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with pd.ExcelWriter(path, engine='xlsxwriter') as writer:\n",
    "            df.to_excel(writer, sheet_name='TrendAnalysis', index=False)\n",
    "            # Optional formatting or freeze panes\n",
    "            ws = writer.sheets['TrendAnalysis']\n",
    "            ws.freeze_panes(1, 0)\n",
    "    except PermissionError:\n",
    "        print(f\"[Export] Permission denied for '{path}'. Close file if open.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Export] Error writing Excel: {e}\")\n",
    "\n",
    "\n",
    "def to_csv(df: pd.DataFrame, path: str) -> None:\n",
    "    try:\n",
    "        df.to_csv(path, index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"[Export] CSV write error: {e}\")\n",
    "\n",
    "\n",
    "def to_json(df: pd.DataFrame, path: str) -> None:\n",
    "    try:\n",
    "        df.to_json(path, orient=\"records\", indent=2)\n",
    "    except Exception as e:\n",
    "        print(f\"[Export] JSON write error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b078e365-1b7b-45d7-a076-3a21cc2a9c45",
   "metadata": {},
   "source": [
    "5) trendlib/selection.py (Optional)\n",
    "Purpose: If you have a rank-ordered or manual selection approach, place that logic here for modularity.\n",
    "\n",
    "<details> <summary><strong>Click to show code</strong></summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dac1848-4ddb-4117-97a7-56dee0c2f6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trendlib/selection.py\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def rank_ordered_selection(df: pd.DataFrame, metric_col: str, top_n: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Sort by 'metric_col' descending, pick top N rows. \n",
    "    Example logic only; adapt to your actual code.\n",
    "    \"\"\"\n",
    "    sorted_df = df.sort_values(metric_col, ascending=False)\n",
    "    return sorted_df.head(top_n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff298050-2662-41eb-8347-5ed6ed3e1f80",
   "metadata": {},
   "source": [
    "6) Your New, Simplified Notebook: Vol_Adj_Trend_Analysis1.2.TrEx.ipynb\n",
    "Below is an example skeleton of how the notebook might look after the refactor. You’ll likely have more analysis steps, but this shows how to import from trendlib/ and reuse the code.\n",
    "\n",
    "<details> <summary><strong>Click to show code</strong></summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc3a066-897e-4b6c-906e-c7124ba23685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from trendlib.config import DEFAULT_CONFIG, validate_config\n",
    "from trendlib.data_io import load_data\n",
    "from trendlib.metrics import calculate_returns, annualized_volatility, sharpe_ratio, max_drawdown\n",
    "from trendlib.exports import to_excel, to_csv, to_json\n",
    "# from trendlib.selection import rank_ordered_selection  # if needed\n",
    "\n",
    "# Cell 2: Setup / Config\n",
    "config = DEFAULT_CONFIG.copy()\n",
    "# Tweak any parameters you want:\n",
    "config[\"data_file\"] = \"Data/Vol_Adj_Trend_Sample.csv\"  # Example path\n",
    "config[\"demo_mode\"] = False\n",
    "validate_config(config)\n",
    "\n",
    "# Cell 3: Data Load\n",
    "df = load_data(config)\n",
    "print(\"Data shape:\", df.shape)\n",
    "print(df.head())\n",
    "\n",
    "# Cell 4: Basic Single-Window Analysis\n",
    "# Insert your original logic. For example, if you have in_sample vs out_sample splitting:\n",
    "in_sample_df = df.loc[\"2008-01-01\":\"2012-12-31\"]\n",
    "out_sample_df = df.loc[\"2013-01-01\":\"2015-12-31\"]\n",
    "\n",
    "# Suppose you have daily returns in a column named 'DailyReturn':\n",
    "in_ret = in_sample_df[\"DailyReturn\"]\n",
    "out_ret = out_sample_df[\"DailyReturn\"]\n",
    "\n",
    "in_sharpe = sharpe_ratio(in_ret, risk_free_annual=0.02)\n",
    "out_sharpe = sharpe_ratio(out_ret, risk_free_annual=0.02)\n",
    "\n",
    "results = {\n",
    "    \"InSampleSharpe\": [in_sharpe],\n",
    "    \"OutSampleSharpe\": [out_sharpe],\n",
    "}\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Results:\\n\", results_df)\n",
    "\n",
    "# Cell 5: Export\n",
    "to_excel(results_df, config[\"excel_path\"])\n",
    "to_csv(results_df, config[\"csv_path\"])\n",
    "to_json(results_df, config[\"json_path\"])\n",
    "\n",
    "print(\"Exports completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498fb032-5ab2-4c67-8095-98984ea49a21",
   "metadata": {},
   "source": [
    "</details>\n",
    "Note that Cells 4 and onward in your real code will have more logic around volatility adjustments, manager selection, etc. The key is that you no longer need to define your metrics or reading logic inline: that code is in trendlib/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdd66cf-cf92-4398-9bdf-295a1adde427",
   "metadata": {},
   "source": [
    "7) Validation Steps\n",
    "Create the trendlib/ folder in your repo alongside notebooks/.\n",
    "\n",
    "Add each .py file with the code above.\n",
    "\n",
    "Adjust config.py to match your actual file paths, column names, or date logic.\n",
    "\n",
    "In Vol_Adj_Trend_Analysis1.2.TrEx.ipynb, replace old inline code with imports from trendlib/.\n",
    "\n",
    "Run the notebook.\n",
    "\n",
    "Confirm you get the same results as before or make small fixes if variable names differ.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090423ef-0966-4dfc-805f-ddede0ff8349",
   "metadata": {},
   "source": [
    "Final Thoughts\n",
    "This refactoring approach should keep all the same variable names, column references, and core logic you had in Vol_Adj_Trend_Analysis1.2.TrEx.ipynb, just distributed among modules for maintainability.\n",
    "\n",
    "Where I wrote placeholders (like # Insert your original dropna or merges here), be sure to copy the matching lines from your notebook if you rely on them.\n",
    "\n",
    "The end result is a clean, modular codebase that you can easily extend for Phase 2 (multi-period simulation, manager thresholds, etc.) and Phase 3 (Monte Carlo).\n",
    "\n",
    "If you run into any issues—like references to variables that moved or missing columns—just let me know and I can suggest how to reconcile them. But this should be a solid template to get you started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fec6fcd-6d06-409f-9efe-998a4c88b50c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
